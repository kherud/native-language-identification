Collapsed Variational Bayesian Inference for PCFGs
Phil Blunsom
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, United Kingdom
Phil.Blunsom@cs.ox.ac.uk

Pengyu Wang
Department of Computer Science
University of Oxford
Oxford, OX1 3QD, United Kingdom
Pengyu.Wang@cs.ox.ac.uk

Abstract

true posterior and an approximate one in which
the strong dependencies between the parameters
and latent variables are broken, this deterministic algorithm efficiently converges to an inaccurate and only locally optimal solution like EM.
Alternatively, Johnson et al. (2007) proposed two
Markov Chain Monte Carlo algorithms for PCFGs
that can reach the true posterior after convergence.
However, it is often difficult to diagnose a sampler‚Äôs convergence, and mixing is notoriously slow
for distributions with tightly coupled hidden variables such as PCFGs, especially when the data sets
are large. Therefore, there remains a challenge for
more efficient, but also accurate and deterministic
inference algorithms for PCFGs.
In this paper, we present a collapsed variational
Bayesian inference (CVB) algorithm for PCFGs.
It has the same computational complexity as the
standard variational Bayesian inference, but offers
almost the same performance as the stochastic algorithms due to its weak assumptions. The idea of
operating VB in the collapsed space was proposed
by Teh et al. (2007) and Sung et al. (2008), and it
was successfully applied to ‚Äúbag-of-words‚Äù models such as latent Dirichlet allocation (LDA) (Teh
et al., 2007) and mixture of Gaussian (Sung et al.,
2008), where the latent variables are conditionally
independent given the parameters. By combining
the CVB idea and the dynamic programming techniques used in structurally dependent models, we
deliver a both efficient and accurate algorithm for
training PCFGs and other structured natural language models.
The rest of the paper is structured as follows.
We begin with the Bayesian models of PCFGs,
and relate the existing training algorithms. Section 3 introduces collapsed variational Bayesian
inference for ‚Äúbag-of-words‚Äù models (defined in
Section 3.1). We discuss the difficulty in applying such inference to structured models, followed
by an approximate CVB algorithm for PCFGs.

This paper presents a collapsed variational
Bayesian inference algorithm for PCFGs
that has the advantages of two dominant
Bayesian training algorithms for PCFGs,
namely variational Bayesian inference and
Markov chain Monte Carlo. In three kinds
of experiments, we illustrate that our algorithm achieves close performance to the
Hastings sampling algorithm while using
an order of magnitude less training time;
and outperforms the standard variational
Bayesian inference and the EM algorithms
with similar training time.

1

Introduction

Probabilistic context-free grammars (PCFGs) are
commonly used in parsing and grammar induction
systems (Johnson, 1998; Collins, 1999; Klein and
Manning, 2003; Matsuzaki et al., 2005). The traditional method for estimating the parameters of
PCFGs from terminal strings is the inside-outside
(IO) algorithm (Baker, 1979). As a special instance of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977), based on the principle of maximum-likelihood estimation (MLE),
the standard IO algorithm learns relatively uniform probability distributions for grammars, while
the true distributions can be highly skewed (Johnson et al., 2007). In order to encourage sparse
grammars and avoid overfitting, recent research
for training PCFGs has drifted away from MLE in
favor of Bayesian inference algorithms that make
either deterministic or stochastic approximations
(Kurihara and Sato, 2006; Johnson et al., 2006;
Johnson et al., 2007).
Variational Bayesian inference (VB) (Kurihara
and Sato, 2006) for PCFGs extends EM and places
no constraints when updating parameters in the M
step. By minimising the divergence between the
173

Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 173‚Äì182,
Sofia, Bulgaria, August 8-9 2013. c 2013 Association for Computational Linguistics

2.2

An alternative approach is also included in brief.
In Section 4, we validate our CVB algorithm in
three simple experiments. They are inferring a
sparse grammar that describes the morphology of
the Sotho language (Johnson et al., 2007), unsupervised dependency parsing (Klein and Manning,
2004) and supervised parsing with latent annotations (Matsuzaki et al., 2005). Section 5 concludes
with future work.

2

The standard inside-outside algorithm for PCFGs
belongs to the general EM class, which is further
a subclass of VB (Beal, 2003). VB maximises the
negative free energy ‚àíF(Q(t, Œ∏)), a lower bound
of the log marginal likelihood of the observation
log P (w|Œ±). This is equivalent to minimising the
Kullback-Leibler divergence.
log P (w|Œ±) ‚â• ‚àíF(Q(t, Œ∏))

Approximate inference for PCFGs

2.1

=EQ(t,Œ∏) [log P (w, t, Œ∏|Œ±)] ‚àí EQ(t,Œ∏) [log Q(t, Œ∏)]

Definitions

Q(t, Œ∏) is an approximate posterior, where the parameters and hidden variables are assumed to be
independent. Thus, it is factorised:

A PCFG is a tuple (T, N, S, R, Œ∏), where T , N ,
R and Œ∏ are the finite sets of terminals, nonterminals, rules and parameters respectively, and
S ‚àà N is the start symbol. We adopt a similar
notation to Johnson et al. (2007), and assume that
the context free grammar G = (T, N, S, R) is in
Chomsky normal form and the empty string  ‚àà
/ T.
Hence, each rule r ‚àà R takes either the form
A ‚Üí BC or A ‚Üí w, where A, B, C ‚àà N and
w ‚àà T . Let Œ∏A‚ÜíŒ≤ be the probability of derivation
rule A ‚Üí Œ≤, where Œ≤ ranges over (N √ó N ) ‚à™ T .
In the Bayesian setting, we place Dirichlet priors
with hyperparameters Œ±A = {Œ±A‚ÜíŒ≤ } on each Œ∏A
= {Œ∏A‚ÜíŒ≤ }.
Given a corpus of sentences w = (w1 , ..., wn )
and the corresponding hidden parse trees t =
(t1 , ..., tn ), the joint probability distribution of parameters and variables is1 :
n
Y
P (w, t, Œ∏|Œ±) =P (Œ∏|Œ±)
PG (wi , ti |Œ∏)

Q(t, Œ∏) ‚âà Q(t)Q(Œ∏)

E step: Q(t) ‚àù exp(EQ(Œ∏) [log P (w, t, Œ∏)])
M step: Œ∏? = argmax P (w, t, Œ∏)
Œ∏

In the E step, we update Q(t). For each tree t,
Q(t) ‚àù PG (w, t|Œ∏? )
Y
=
(Œ∏r? )fr (t)

(3)

r‚ààR

i=1

The distribution over parse tree Q(t) is intractable
to compute as its normalization requires summing
over all possible parse trees producing w. We use
dynamic programming to compute inside and outside probabilities recursively with the aim of accumulating the expected counts.
X
E[fA‚ÜíBC (t)|w] ‚àù
POUT (A, i, k)√ó

r‚ààR

(1)

Y

1
Œ∏rŒ±r ‚àí1
B(Œ±A )
r‚ààRA
Q
r‚ààR Œì(Œ±r )
B(Œ±A ) = P A
Œì( r‚ààRA Œ±r )

PD (Œ∏A |Œ±A ) =

(2)

This strong independence assumption allows for
the separate updates of Q(t) and Q(Œ∏) iteratively,
optimising the negative free energy ‚àíF(Q(z, Œ∏)).
For the traditional IO algorithm using maximum
likelihood estimation, Q(Œ∏) is further assumed to
be degenerate, i.e. Q(Œ∏) = Œ¥(Œ∏ = Œ∏? ).

 Y
Y
=
PD (Œ∏A |Œ±A )
Œ∏rfr (t)
A‚ààN

Variational Bayesian inference

0‚â§i<j<k‚â§|w|

where fr (t) is the frequency of product rule r in
all the parse trees t, and RA is the set of rules
with left-hand side A. For a Dirichlet distribution
PD (Œ∏A |Œ±A ), B(Œ±A ) is the normalization constant
that can be written in terms of the gamma function
Œì (i.e. the generalised factorial function).

Œ∏A‚ÜíBC PIN (B, i, j)PIN (C, j, k)
X
E[fA‚Üíw (t)|w] ‚àù
POUT (A, i)√ó
0‚â§i‚â§|w|

Œ∏A‚Üíwi Œ¥(wi = w)
where PIN (A, i, k) is the inside probability of observation wi,k = wi , ..., wk given A is the root of
the subtree, and POUT (A, i, k) is the probability of
A spanning (i, k), together with the rest of w.

1

Strictly speaking, for each (w, t) pair, if a hidden tree t
is arbitrary, we need to include two delta functions, namely
Œ¥(w = yield(t)) and Œ¥(G ‚áí? t). We assume that both delta
functions are true, otherwise the probability of such pair is 0.

174

By using the conjugacy property, we can easily
compute the marginal distribution of w and t:
Z
P (w, t|Œ±) = PG (w, t|Œ∏)PD (Œ∏|Œ±)dŒ∏

In the M step, we find the optimal Œ∏? based on
the MLE principle:
?
Œ∏A‚ÜíŒ≤
=P

E[fA‚ÜíŒ≤ (t)|w]
A‚ÜíŒ≤ 0 ‚ààRA E[fA‚ÜíŒ≤ 0 (t)|w]

E[fA‚ÜíŒ≤ (t)|w] =

n
X

Œ∏

Y B(fA (t) + Œ±A )
=
B(Œ±A )

E[fA‚ÜíŒ≤ (ti )|wi ]

A‚ààN

i=1

where we define fA (t) to be a vector of rule frequencies in t indexed by A ‚Üí Œ≤ ‚àà RA . Hence,
the conditional distribution for a parse tree ti given
all others is:

VB inference is the generalisation of EM in the
sense that it allows arbitrary parametric forms of
Q(Œ∏). Thus, the update equation in the M step is:

P (ti |wi , w¬¨i , t¬¨i , Œ±) ‚àù P (wi , ti |w¬¨i , t¬¨i , Œ±)
Y B(fA (t) + Œ±A )
=
B(fA (t¬¨i ) + Œ±A )
A‚ààN

Q(Œ∏) ‚àù exp(EQ(t) [log P (w, t, Œ∏|Œ±)])
By the conjugacy property, the new Q(Œ∏) is still
in Dirichlet distribution form except with updated
hyperparameters as shown by Kurihara and Sato
(2006). Instead, Beal (2003) suggested an equivalent mean parameters Œ∏ÃÉ. Based on implementation
of the EM algorithm, we only need a minor modification in the M step.
Œ∏ÃÉA‚ÜíŒ≤ =

m(

(5)

m(E[fA‚ÜíŒ≤ (t)|w]+Œ±A‚ÜíŒ≤ )
A‚ÜíŒ≤ 0‚ààRA(E[fA‚ÜíŒ≤ 0 (t)|w]+Œ±A‚ÜíŒ≤ 0 ))

P

m(x) = exp(Œ®(x))

where Œ®(x) = ‚àÇŒì(x)
‚àÇx is the digamma function.
From the joint distribution in (1) proportional
to the true posterior, we notice that the parameters and hidden variables are intimately coupled.
Fluctuations in the parameters can induce changes
in the hidden variables and vice-versa. Hence, the
independence assumption in (2) and Figure 1(d)
seems too strong, leading to inaccurate local maximums, although it allows for efficient and deterministic updates in EM and VB. The dependencies
between parameters and hidden variables are kept
intact for the remaining algorithms in this paper.
2.3

(4)

where w¬¨i and t¬¨i denote all other sentences and
trees. It is noticeable that sampling a parse tree
from the above conditional distribution is difficult.
The frequencies fA (t) effectively mean that the
production probabilities are dependent on the current parse tree ti . That is rule parameters can be
updated on the fly inside a parse tree, which prohibits efficient dynamic programming tricks.
In order to solve this problem, Johnson et al.
(2007) proposed a Hastings sampler that specified
an alternative rule probabilities Œ∏H of a proposal
distribution P (ti |wi , Œ∏H ), where
fA‚ÜíŒ≤ (t¬¨i ) + Œ±A‚ÜíŒ≤
¬¨i
A‚ÜíŒ≤ 0 ‚ààRA (fA‚ÜíŒ≤ 0 (t ) + Œ±A‚ÜíŒ≤ 0 )

H
Œ∏A‚ÜíŒ≤
=P

The rule probabilities Œ∏H are based on the statistics
collected from all other parse trees, and they are
fixed for the conditional distribution of the current
parse tree. Therefore, by using a variant of inside
algorithm (Goodman, 1998), one can efficiently
sample a parse tree, which will be either accepted
or rejected based on the Metropolis choice.
The MCMC based algorithms do not make any
assumptions at all, and they can converge to the
true posterior, either in joint or collapsed space as
shown in Figure 1(b), 1(c). However, one needs to
have experience about the number of samples to
be collected and the burn-in period. For computationally intensive tasks such as learning PCFGs
from a large corpus, a sufficiently large number
of samples are required to decrease the sampling
variance. Therefore, MCMC algorithms improves
the performance over EM and VB at the cost of
much more training time.

Markov Chain Monte Carlo

The standard Gibbs sampler for PCFGs iteratively
samples the parameters Œ∏ and all the parse trees
t. Its mixing can be slowed by again the strong
dependencies between the parameters and hidden
variables. Instead of reparsing all the hidden trees
t for each sample of Œ∏, collapsed Gibbs sampling
(CGS) improves upon Gibbs sampling in terms of
convergence speed by integrating out the parameters, and sampling directly from P (t|w, Œ±) in a
component-wise manner. Thus, it also deals with
the dependencies exactly.
175

Figure 1: Graphical representations of the PCFG with n = 3 trees (a), and the (approximate) posteriors
for Gibbs sampling (b), collapsed Gibbs sampling (c), variational Bayesian inference (d), and collapsed
variational Bayesian inference (e). We use dashed lines to depict the weak dependencies.

3
3.1

Collapsed variational Bayesian
inference

very close to the true posterior.
Even in collapsed space, CVB remains a deterministic algorithm that updates the posterior distributions over the hidden variables just like VB
and EM. Therefore, we expect CVB to be computationally efficient as well.

For bag-of-words models

Leveraging the insight that a sampling algorithm
in collapsed space mixes faster than the standard
one, Teh et al. (2007) proposed a similar argument
that a VB inference algorithm in collapsed space
is more effective than the standard one. Following
the success in LDA (Teh et al., 2007), a number of
research results have been accumulated around applying CVB to a variety of ‚Äúbag-of-words‚Äù models (Sung et al., 2008; Sato et al., 2012; Wang and
Blei, 2012).
Formally, we define a model to be independent
and identically distributed (i.i.d.) (or informally
‚Äúbag-of-words‚Äù) if its hidden variables are conditionally independent given the parameters. LDA,
IBM word alignment model 1 and 2, and various
finite mixture models are typical examples.
For an i.i.d. model, integrating out parameters
induces dependencies that spread over many hidden variables, and thus the dependency between
any two variables is very weak. This provides an
ideal setting to apply the mean field method (i.e.
fully factorized VB), as its underlying assumption
is that any variable depends on only the summary
statistics collected from other variables called the
field, and any particular variable‚Äôs impact on the
field is very small. Hence, the mean field assumption is better satisfied in collapsed space with very
weak dependencies than in joint space with strong
dependencies. As a result, we expect that VB in
collapsed space can achieve more accurate results
than the standard VB, and the results would be

3.2

For structured NLP models

We notice that the basic condition for applying the
CVB algorithm to a specific model is for the model
to be i.i.d., such that the hidden variables are only
weakly dependent in collapsed space, providing an
ideal condition to operate VB. However, the i.i.d.
condition is certainly not true for structured NLP
models such as hidden Markov models (HMMs)
and PCFGs. Given the shape of a parse tree, a
hidden variable is strongly dependent on its parent, siblings and children, and weakly dependent
on the rest. Even worse, to infer a grammar from
terminal strings, we don‚Äôt even have access to the
shape of parse trees, let alone analyzing the dependencies of hidden variables inside trees.
Although the PCFG model is not i.i.d. at the
variable level, we can lift the idea of CVB up to
the tree level. As our research domain is those
large scale applications in language processing, a
common feature of those problems is that there
are usually many sentences, each of which has a
hidden parse tree behind it. Hence, we may consider each sentence together with its parse tree to
be drawn i.i.d. from the same set of parameters.
Therefore, at the tree level, a PCFG can be considered as an i.i.d. model as shown in Figure 1(a)
and thus, it can be fitted in the CVB framework
as described in Section 3.1. We summarise the as176

Q

QfA‚ÜíŒ≤ (ti )‚àí1
(fA‚ÜíŒ≤ (t¬¨i ) + Œ±A‚ÜíŒ≤ + j))])
A‚ÜíŒ≤‚ààRA exp(EQ(t¬¨i ) [log( j=0
P
Q(ti ) ‚àù
Q( A‚ÜíŒ≤0 fA‚ÜíŒ≤0 (ti ))‚àí1
P
exp(EQ(t¬¨i ) [log( A‚ÜíŒ≤ 0 ‚ààRA (fA‚ÜíŒ≤ (t¬¨i ) + Œ±A‚ÜíŒ≤ 0 +
A‚ààN
j=0
Y

j))])

Figure 2: The exact mean field update in collapsed space for the parse tree ti .

Q(ti ) ‚àù

Y

r=A‚ÜíŒ≤‚ààR



P

EQ(t¬¨i ) [fA‚ÜíŒ≤ (t¬¨i )] + Œ±A‚ÜíŒ≤

A‚ÜíŒ≤ 0 (EQ(t¬¨i ) [fA‚ÜíŒ≤ 0 (t

¬¨i )]

+ Œ±A‚ÜíŒ≤ 0 )

fr (ti )

Figure 3: The approximate mean field update in collapsed space for the parse tree ti .
The inner term P (wi , ti |w¬¨i , t¬¨i , Œ±) in the above
equation is just the unnormalized collapsed Gibbs
sampling in (5). Plugging in (5), and expanding
terms such as B(Œ±A ) and Œì(x), we obtain an exact
computation of Q(ti ) in Figure 2.
The exact computation is both intractable and
expensive. The intractability comes from the similar problem as in the collapsed Gibbs sampling
that we
P are unable to calculate the normalisation
term ti Q(ti ). Hence, we follow Johnson et al.
(2007) to approximate it by using only the statistics from other sentences, namely Œ∏H and ignoring
the local contribution.
Y
fA‚ÜíŒ≤ (ti )
H
P (wi , ti |w¬¨i , t¬¨i , Œ±) ‚âà
Œ∏A‚ÜíŒ≤

sumptions made by each algorithm in Figure 1(be) before presenting the CVB algorithm formally.
The CVB algorithm for the PCFG model keeps
the dependencies between the parameters and the
hidden parse trees in an exact fashion:
Q(t, Œ∏) = Q(t)Q(Œ∏|t)
We factorise Q(t) by breaking only the weak dependencies between parse trees, while keeping the
inside dependencies intact, as we don‚Äôt make further assumptions about Q(t) for each t.
Q(t) ‚âà

n
Y

Q(ti )

i=1

By the above factorisations, we compute the negative variational free energy ‚àíF(Q(t)Q(Œ∏|t)) as
follows:

A‚ÜíŒ≤‚ààR

(7)

We discuss the accuracy of (7) in Section 3.3. For
those expensive computations of the expected log
counts in Figure 2, Teh et al. (2007) and Sung et
al. (2008) suggested the use of a linear Gaussian
approximation based on the law of large numbers.

‚àí F(Q(t)Q(Œ∏|t))

=EQ(t)Q(Œ∏|t) [log P (w, t, Œ∏|Œ±) ‚àí log Q(t)Q(Œ∏|t)]
=EQ(t) [EQ(Œ∏|t) [log

P (w, t, Œ∏|Œ±)
] ‚àí log Q(t)]
Q(Œ∏|t)

EQ(t¬¨i ) [log(fA‚ÜíŒ≤ (t¬¨i ) + Œ±A‚ÜíŒ≤ )]

Maximizing ‚àíF(Q(t)Q(Œ∏|t)) requires to update
Q(Œ∏|t) and Q(t) in turn. In particular, Q(Œ∏|t) is
set equal to the true posterior P (Œ∏|w, t, Œ±):

‚âà log(EQ(t¬¨i ) [fA‚ÜíŒ≤ (t¬¨i )] + Œ±A‚ÜíŒ≤ )

(8)

Substituting (7) into (6), and employing the linear
approximation, we derive an approximate CVB algorithm as shown in Figure 3. In addition, its form
is much more simplified and interpretable compared with the exact computation in Figure 2.
The surprising similarity between the approximate CVB update in Figure 3 and E step update in
(3) indicates that the dynamic programming used
in both EM and VB can take over from now. To
run inside-outside recursion, the EM algorithm
employs the parameters Œ∏? based on maximum
likelihood estimation; the VB algorithm employs

‚àí F(Q(t)P (Œ∏|w, t))
P (w, t, Œ∏|Œ±)
] ‚àí log Q(t)]
=EQ(t) [EP (Œ∏|w,t,Œ±) [log
P (Œ∏|w, t, Œ±)
=EQ(t) [log P (w, t|Œ±) ‚àí log Q(t)]
Finally, we update the approximate posterior for
each parse tree t by using the mean field method
in the collapsed space:
Q(ti ) ‚àù exp(EQ(t¬¨i ) [log P (wi , ti |w¬¨i , t¬¨i , Œ±)])
(6)
177

the mean parameters Œ∏ÃÉ; and our CVB algorithm
employs the parameters Œ∏CVB computed from the
expected counts of all other sentences.
The implementation can be easily achieved by
modifying code of the EM algorithm. We keep
track of the expected counts at global level, subtract the local mean counts for ti before update,
run the inside-outside recursion using Œ∏CVB , and
finally add the updated distribution back into the
global counts. Therefore, we only need to replace
the parameters with the expected counts, and make
update after each sentence; the core of the insideoutside implementation remains the same.
Our CVB algorithm bears some similarities to
the online EM algorithm with maximum a posterior (MAP) updates (Neal and Hinton, 1998;
Liang and Klein, 2009), but they differ in several
ways. The online EM algorithm updates each tree
ti based on the statistics of all the trees, optimising
the same objective function p(w|Œ∏) as the batch
EM algorithm. MAP estimation searches for the
optimal posterior p(w|Œ∏)p(Œ∏). On the other hand,
our CVB algorithm optimises the data likelihood
p(w). The smoothing effects for the MAP estimation (Œ±A‚ÜíŒ≤ ‚àí 1) prevent the use of sparse priors,
whereas the CVB algorithm (Œ±A‚ÜíŒ≤ ) overcomes
such difficulty by parameter integration.
3.3

Figure 4: A fragment of a tree structure
are reasonable and weak, we expect its results to
be close to true posteriors.
3.4

An alternative approach

We briefly sketch an alternative CVB algorithm at
the variable level for completeness.
For a structured NLP model with its shape to
be fixed such as the PCFG with latent annotations
(PCFG-LA) (Matsuzaki et al., 2005) (See definition in Section 4.3), we can simply ignore all the
dependencies between the hidden variables in the
collapsed space, despite whether they are strong
(for adjacent nodes) or weak (for others). Although it seems that we have made unreasonable
assumptions, it is not transparent which is worse
comparing with the assumptions in the standard
VB. Following this assumption, we can derive a
CVB algorithm similar to the corresponding local
sampling algorithm that samples one hidden variable at a time. For example, the approximate posterior over the subtype of the node A in the above
tree fragment in Figure 4 is updated follows:

Discussion

Breaking the weak dependencies between hidden
variables and employing the linear approximation
have been argued to be accurate (Teh et al., 2007;
Sung et al., 2008; Sato and Nakagawa, 2012), and
they are the standard procedures in applying the
CVB algorithms to i.i.d. models.
In our CVB algorithm for PCFGs, we introduce
an extra approximation in (7), which we argue is
accurate. Theoretically, the inaccuracy only occurs when there are repeated rules in a parse tree as
shown in Figure 2, so the same rule seen later uses
a slightly different probability. Even if the inaccuracy indeed occurs, in our described scenario of
many sentences, the local contribution from a single sentence is small compared with the statistics
from all other sentences. Empirically, we replicate
the experiment of Setho language by Johnson et al.
(2007) in Section 4.1, and we find that the sampled
trees based on Œ∏H never get rejected, illustrating an
acceptance rate close to 100%, and meaning that
Œ∏H is a very accurate Metropolis proposal. Since
all the assumptions made by the CVB algorithm

q(A = a)
‚àù

E[fB‚ÜíaC (t¬¨A )] + Œ± E[fa‚ÜíDE (t¬¨A )] + Œ±
¬∑
E[fB (t¬¨A )] + |RB |Œ± E[fa (t¬¨A )] + |Ra |Œ±

where we use A to denote the node position, and
a to denote its hidden subtype. q(A = a) means
the probability of node A being in subtype a. In
addition, we need to take into account the distributions over its adjacent variables. In our case, A is
strongly dependent on nodes B, C, D, E, and only
weakly dependent on other variables (not shown in
the above tree fragment) via global counts, e.g.:
E[fB‚ÜíaC (t¬¨A )]
XX
=
q(B = b)q(C = c)E[fb‚Üíac (t¬¨A )]
b

c

However, it is not obvious how to use this alternative approach in general, and the performances
of resulting algorithms remain unclear. Therefore,
we implement only the CVB algorithm at the tree
level in Section 3.2 for our experiments.
178

4

Experiments

0

We conduct three simple experiments to validate
our CVB algorithm for PCFGs. In Section 4.1, we
illustrate the significantly reduced training time
of our CVB algorithm compared to the related
Hastings algorithm; whereas in later two sections,
we demonstrate the increased performance of our
CVB algorithm compared to the corresponding
VB and EM algorithms.
4.1

12
Test Perplexity

11

2000

CVB
Hastings

10
9
8
7
6

Inferring sparse grammars

0

Firstly, we conduct the same experiment of inferring sparse grammars describing the morphology of the Sotho language as in Johnson et al.
(2007). We use the same corpus of unsegmented
Sotho verb types from CHILDES (MacWhinney
and Snow, 1985), and define the same initial CFG
productions by allowing each non-terminal to emit
any substrings in the corpus as terminals plus five
predefined morphological rules at the top level.
We randomly withhold 10% of the verb types
from the corpus for testing, and use the rest 90%
for training. Both algorithms are evaluated by
their per word perplexity on the test data set with
prior set to 10‚àí5 as suggested by Johnson et al.
(2007). We run 5 times with random starts, and
report the averaged results in Figure 5. The Hastings algorithm2 takes roughly 1,000 iterations to
converge, while our CVB algorithm reaches the
convergence even before 10 iterations, consuming
only a fraction of training time (CVB: 1.5 minutes;
Hastings: 20 minutes). As well as little difference
margin in final perplexities shown in Figure 5, we
also evaluated segmentation quality measured by
the F1 scores, and again the difference is trivial
(CVB: 29.8%, Hastings: 31.3%).
4.2

Number of Iterations (Hastings)
500
1000
1500

5
10
15
Number of Iterations (CVB)

20

Figure 5: Perplexities averaged over 5 runs on the
extracted corpus of Sotho verbs.
improved inference on this core model will enable
future improvements to more complex models.
We evaluate a Dirichlet-Multinomial formulation of DMV in the standard fashion by training on sections 2-21 and testing on section 23 of
the Penn. Wall Street Journal treebank (Marcus
et al., 1993). We initialise our models using the
original harmonic initialiser (Klein and Manning,
2004). Figure 6 displays the directed accuracy results for DMV model trained with CVB and VB
with Dirichlet Œ± parameters of either 1 or 0.1, as
well as the previously reported MLE result. In
both cases we see superior results for CVB inference, providing evidence that CVB may be a better choice of inference algorithm for Bayesian formulations of generative grammar induction models such as DMV.
4.3

PCFG with latent annotations

The vanilla PCFGs estimated by simply taking the
empirical rule frequencies off treebanks are not accurate models to capture the syntactic structures in
most natural languages as demonstrated by Charniak (1997) and Klein and Manning (2003). Our
third experiment is to apply the CVB algorithm
to the PCFGs with latent annotations (PCFGsLA) (Matsuzaki et al., 2005), where each nonterminal symbol is augmented with hidden variables (or subtypes). Given a parsed corpus, training a PCFG-LA yields a finer grammar with the
automatically induced features represented by the
subtypes. For example, an augmented binary rule
takes the form A[a] ‚Üí B[b]C[c], where a, b, c ‚àà
[1, H] are the hidden subtypes, and H denotes the
number of subtypes for each non-terminal.

Dependency model with valence

As a second empirical validation of our CVB inference algorithm, we apply it to unsupervised
grammar induction with the popular Dependency
Model with Valence (DMV) (Klein and Manning,
2004). Although the original maximum likelihood
formulation of this model has long since been surpassed by more advanced models, all of the stateof-the-art approaches to unsupervised dependency
parsing still have DMV at their core (Headden III
et al., 2009; Blunsom and Cohn, 2010; Spitkovsky
et al., 2012). As such we believe demonstrating
2
Annealing is not used in order to facilitate the perplexity
calculation in the test set.

179

0.49

0.9

EM
0.85

VB

0.48

F1 Scores

F1 Scores

CVB

0.8

0.75

0.47

0.7

EM

0.46

VB

0.65

CVB

0.45

1

1.0
0.1
Bayesian Priors

Precision
75.84
76.98
78.85

Recall
72.92
73.32
76.98

F1
74.35
75.11
77.90

we can ignore the local dependencies induced by
collapsing the parameters. The assumptions in our
CVB algorithm are reasonable for a range of parsing applications and justified in three tasks by the
empirical observations: it produces more accurate
results than standard VB, and close results to sampling with significantly less training time.
While not state-of-the-art, the models we have
demonstrated our CVB algorithm on underlie a
number of high performance grammar induction
and parsing systems (Cohen and Smith, 2009;
Blunsom and Cohn, 2010; Petrov and Klein, 2007;
Liang et al., 2007). Therefore, our work naturally
extends to employing our CVB algorithm in more
advanced models such as hierarchical splitting and
merging system used in Berkeley parser (Petrov
and Klein, 2007), and generalising our CVB algorithm to the non-parametric models such as tree
substitution grammars (Blunsom and Cohn, 2010)
and infinite PCFGs (Liang et al., 2007).
We have also sketched an alternative CVB algorithm which makes a harsher independence assumption for the latent variables but then requires
no approximation of the variational posterior by
performing inference individually for each parse
node. This model breaks some strong dependencies within parse trees, but if we expect the posterior to be highly skewed by using a sparse prior,
the product of constituent marginals may well be a
good approximation. We leave further exploration
of this algorithm for future work.

Exact
11.13
11.49
12.56

Table 1: PCFG-LA (2 subtypes) trained by EM,
VB and CVB. Precision, Recall, F1 scores, Exact
match scores on section 23, WSJ.
We follow the same experiment set-up as DMV,
and report the results on the section 23, using the
best grammar tested on the development set (section 22) from 5 random runs for each algorithm.
We adopt Petrov et al. (2006)‚Äôs methods to process
the data: right binarising and replacing infrequent
words with the generic unknown word marker for
English, and to initialise: adding 1% randomness
to the parameters Œ∏0 to start the EM training. We
calculate the expected counts from (G, Œ∏0 ) to initialise our VB and CVB algorithms.
In Table 1, when each non-terminal is split into
2 hidden subtypes, we show that our CVB algorithm outperforms the EM and VB algorithms in
terms of all the evaluation objectives. We also
investigate the hidden state space with higher dimensions (4,8,16 subtypes), and find our CVB algorithm retains the advantages over the other two,
whereas the VB algorithm fails to surpass the EM
algorithm as reported in Figure 7.

5

16

Figure 7: PCFG-LA (2,4,8,16 subtypes) trained by
EM, VB and CVB. F1 scores on section 23, WSJ.

Figure 6: DMV trained by EM, VB and CVB. F1
scores on section 23, WSJ.
Objective
EM
VB
CVB

2
4
8
Number of Hidden States

Conclusion and future work

In this paper we have presented a collapsed variational Bayesian inference algorithm for PCFGs.
We make use of the common scenario where the
data consists of multiple short sentences, such that

Acknowledgments
We would like to thank Mark Johnson for the data
used in Section 4.1 and valuable advice.
180

References

Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics,
24:613‚Äì632.

James K. Baker. 1979. Trainable grammars for speech
recognition. The Journal of the Acoustical Society
of America, 65(S1):S132.

Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ‚Äô03, pages 423‚Äì
430, Stroudsburg, PA, USA. Association for Computational Linguistics.

Matthew Beal. 2003. Variational Algorithms for Approximate Bayesian Inference. Ph.D. thesis, The
Gatsby Computational Neuroscience Unit, University College London.
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for dependency parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, pages 1204‚Äì1213, Cambridge, MA, October. Association for Computational Linguistics.

Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: models of dependency and constituency. In ACL ‚Äô04:
Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 478.

Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proceedings of the fourteenth national conference
on artificial intelligence and ninth conference on
Innovative applications of artificial intelligence,
AAAI‚Äô97/IAAI‚Äô97, pages 598‚Äì603. AAAI Press.

Kenichi Kurihara and Taisuke Sato. 2006. Variational
bayesian grammar induction for natural language.
In Proceedings of the 8th international conference
on Grammatical Inference: algorithms and applications, ICGI‚Äô06, pages 84‚Äì96, Berlin, Heidelberg.
Springer-Verlag.

Shay B. Cohen and Noah A. Smith. 2009. Shared
logistic normal distributions for soft parameter tying in unsupervised grammar induction. In NAACL
‚Äô09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computational Linguistics, pages 74‚Äì82, Morristown, NJ,
USA. Association for Computational Linguistics.

Percy Liang and Dan Klein. 2009. Online EM for unsupervised models. In Proceedings HLT/NAACL.
Percy Liang, Slav Petrov, Michael Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchical Dirichlet processes. In Proc. of the 2007 Conference on Empirical Methods in Natural Language
Processing (EMNLP-2007), pages 688‚Äì697, Prague,
Czech Republic.

Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.

Brian MacWhinney and Catherine Snow. 1985. The
child language data exchange system. Child Language.

A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistics Society, Series B, 39(1):1‚Äì38.

Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313‚Äì330.

Joshua T. Goodman. 1998. Parsing inside-out.
Ph.D. thesis, Cambridge, MA, USA. Adviser-Stuart
Shieber.

Takuya Matsuzaki, Yusuke Miyao, and Jun‚Äôichi Tsujii. 2005. Probabilistic cfg with latent annotations.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ‚Äô05,
pages 75‚Äì82, Stroudsburg, PA, USA. Association
for Computational Linguistics.

William P. Headden III, Mark Johnson, and David McClosky. 2009. Improving unsupervised dependency parsing with richer contexts and smoothing.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguistics, pages 101‚Äì109, Boulder, Colorado, June.

Radford Neal and Geoffrey E. Hinton. 1998. A view of
the em algorithm that justifies incremental, sparse,
and other variants. In Learning in Graphical Models, pages 355‚Äì368. Kluwer Academic Publishers.

Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2006. Adaptor grammars: A framework for
specifying compositional nonparametric bayesian
models. In NIPS.

Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference.

Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proc. of the 7th International Conference on Human Language Technology Research and 8th Annual Meeting of the NAACL
(HLT-NAACL 2007), pages 139‚Äì146, Rochester,
New York, April.

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational

181

Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,
pages 433‚Äì440, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Issei Sato and Hiroshi Nakagawa. 2012. Rethinking
collapsed variational bayes inference for LDA. In
Proceedings of the 29th International Conference on
Machine Learning.
Issei Sato, Kenichi Kurihara, and Hiroshi Nakagawa.
2012. Practical collapsed variational bayes inference for hierarchical dirichlet process. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,
KDD ‚Äô12, pages 105‚Äì113, New York, NY, USA.
ACM.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2012. Three dependency-and-boundary
models for grammar induction. In Proceedings of
the 2012 Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL 2012).
Jaemo Sung, Zoubin Ghahramani, and Sung-Yang
Bang. 2008. Latent-space variational Bayes. IEEE
Trans. Pattern Anal. Mach. Intell., 30(12), December.
Yee Whye Teh, David Newman, and Max Welling.
2007. A collapsed variational Bayesian inference
algorithm for latent Dirichlet allocation. In In Advances in Neural Information Processing Systems,
volume 19.
Chong Wang and David Blei. 2012. Truncation-free
stochastic variational inference for bayesian nonparametric models. In Neural Information Processing Systems.

182

