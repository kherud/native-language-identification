A Coactive Learning View of Online Structured Prediction
in Statistical Machine Translation
Artem Sokolov and Stefan Riezler∗
Computational Linguistics & IWR∗
69120 Heidelberg, Germany

Shay B. Cohen
University of Edinburgh
Edinburgh EH8 9LE, UK

{sokolov,riezler}@cl.uni-heidelberg.de

scohen@inf.ed.ac.uk

Abstract

tuning via online structured prediction (see Liang
et al. (2006) for seminal early work), and in interactive learning from user post-edits (see CesaBianchi et al. (2008) for pioneering work on online computer-assisted translation). Online learning algorithms can be given a theoretical analysis in the framework of online convex optimization (Shalev-Shwartz, 2012), however, the application of online learning techniques to SMT sacrifices convexity because of latent derivation variables, and because of surrogate translations replacing human references that are unreachable in the
decoder search space. For example, the objective
function actually optimized in Liang et al.’s (2006)
application of Collins’ (2002) structure perceptron
has been analyzed by Gimpel and Smith (2012)
as a non-convex ramp loss function (McAllester
and Keshet, 2011; Do et al., 2008; Collobert et al.,
2006). Since online convex optimization does not
provide convergence guarantees for the algorithm
of Liang et al. (2006), Gimpel and Smith (2012)
recommend CCCP (Yuille and Rangarajan, 2003)
instead for optimization, but fail to provide a theoretical analysis of Liang et al.’s (2006) actual algorithm under the new objective.
The goal of this paper is to present an alternative
theoretical analysis of online learning algorithms
for SMT from the viewpoint of coactive learning
(Shivaswamy and Joachims, 2012). This framework allows us to make three main contributions:
• Firstly, the proof techniques of Shivaswamy
and Joachims (2012) are a simple and elegant tool
for a theoretical analysis of perceptron-style algorithms that date back to the perceptron mistake
bound of Novikoff (1962). These techniques provide an alternative to an online gradient descent
view of perceptron-style algorithms, and can easily be extended to obtain regret bounds for a la-

We present a theoretical analysis of online
parameter tuning in statistical machine
translation (SMT) from a coactive learning view. This perspective allows us to
give regret and generalization bounds for
latent perceptron algorithms that are common in SMT, but fall outside of the standard convex optimization scenario. Coactive learning also introduces the concept of
weak feedback, which we apply in a proofof-concept experiment to SMT, showing
that learning from feedback that consists
of slight improvements over predictions
leads to convergence in regret and translation error rate. This suggests that coactive
learning might be a viable framework for
interactive machine translation. Furthermore, we find that surrogate translations
replacing references that are unreachable
in the decoder search space can be interpreted as weak feedback and lead to convergence in learning, if they admit an underlying linear model.

1

Introduction

Online learning has become the tool of choice for
large scale machine learning scenarios. Compared
to batch learning, its advantages include memory
efficiency, due to parameter updates being performed on the basis of single examples, and runtime efficiency, where a constant number of passes
over the training sample is sufficient for convergence (Bottou and Bousquet, 2004). Statistical
Machine Translation (SMT) has embraced the potential of online learning, both to handle millions
of features and/or millions of data in parameter
1

Proceedings of the 19th Conference on Computational Language Learning, pages 1–11,
Beijing, China, July 30-31, 2015. c 2015 Association for Computational Linguistics


tent perceptron algorithm at a rate of O √1T , with
possible improvements by using re-scaling. This
bound can be directly used to derive generalization
guarantees for online and online-to-batch conversions of the algorithm, based on well-known concentration inequalities. Our analysis covers the approach of Liang et al. (2006) and supersedes Sun
et al. (2013)’s analysis of the latent perceptron by
providing simpler proofs and by adding a generalization analysis. Furthermore, an online learning
framework such as coactive learning covers problems such as changing n-best lists after each update that were explicitly excluded from the batch
analysis of Gimpel and Smith (2012) and considered fixed in the analysis of Sun et al. (2013).

2013), for which learning does not converge.
It is important to note that the goal of our experiments is not to present improvements of coactive learning over the “optimal” full-information
model in terms of standard SMT performance. Instead, our goal is to present experiments that serve
as a proof-of-concept of the feasibility of coactive
learning from weak feedback for SMT, and to propose a new perspective on standard practices of
learning from surrogate translations. The rest of
this paper is organized as follows. After a review
of related work (Section 2), we present a latent
percpetron algorithm and analyze its convergence
and generalization properties (Section 3). Our first
set of experiments (Section 4.1) confirms our theoretical analysis by showing convergence in regret
and TER for learning from weak and strong feedback. Our second set of experiments (Section 4.2)
analyzes the relation of different surrogacy modes
to minimization of regret and TER.

• Our second contribution is an extension of
the online learning scenario in SMT to include a
notion of “weak feedback” for the latent perceptron: Coactive learning follows an online learning
protocol, where at each round t, the learner predicts a structured object yt for an input xt , and
the user corrects the learner by responding with
an improved, but not necessarily optimal, object
ȳt with respect to a utility function U . The key asset of coactive learning is the ability of the learner
to converge to predictions that are close to optimal structures yt∗ , although the utility function is
unknown to the learner, and only weak feedback
in form of slightly improved structures ȳt is seen
in training. We present a proof-of-concept experiment in which translation feedback of varying
grades is chosen from the n-best list of an “optimal” model that has access to full information. We
show that weak feedback structures correspond to
improvements in TER (Snover et al., 2006) over
predicted structures, and that learning from weak
feedback minimizes regret and TER.

2

Related Work

Our work builds on the framework of coactive
learning, introduced by Shivaswamy and Joachims
(2012). We extend their algorithms and proofs to
the area of SMT where latent variable models are
appropriate, and additionally present generalization guarantees and an online-to-batch conversion.
Our theoretical analysis is easily extendable to the
full information case of Sun et al. (2013). We
also extend our own previous work (Sokolov et al.,
2015) with theory and experiments for online-tobatch conversion, and with experiments on coactive learning from surrogate translations.
Online learning has been applied for discriminative training in SMT, based on perceptron-type
algorithms (Shen et al. (2004), Watanabe et al.
(2006), Liang et al. (2006), Yu et al. (2013), inter
alia), or large-margin approaches (Tillmann and
Zhang (2006), Watanabe et al. (2007), Chiang et
al. (2008), Chiang et al. (2009), Chiang (2012), inter alia). The latest incarnations are able to handle
millions of features and millions of parallel sentences (Simianer et al. (2012), Eidelmann (2012),
Watanabe (2012), Green et al. (2013), inter alia).
Most approaches rely on hidden derivation variables, use some form of surrogate references, and
involve n-best lists that change after each update.
Online learning from post-edits has mostly been
confined to “simulated post-editing” where independently created human reference translations,

• Our third contribution is to show that certain practices of computing surrogate references
actually can be understood as a form of weak
feedback. Coactive learning decouples the learner
(performing prediction and updates) from the user
(providing feedback in form of an improved translation) so that we can compare different surrogacy modes as different ways of approximate utility maximization. We show experimentally that
learning from surrogate “hope” derivations (Chiang, 2012) minimizes regret and TER, thus favoring surrogacy modes that admit an underlying linear model, over “local” updates (Liang et
al., 2006) or “oracle” derivations (Sokolov et al.,
2

Algorithm 1 Feedback-based Latent Perceptron

or post-edits on the output from similar SMT
systems, are used as for online learning (CesaBianchi et al. (2008), López-Salcedo et al. (2012),
Martı́nez-Gómez et al. (2012), Saluja et al. (2012),
Saluja and Zhang (2014), inter alia). Recent
approaches extend online parameter updating by
online phrase extraction (Wäschle et al. (2013),
Bertoldi et al. (2014), Denkowski et al. (2014),
Green et al. (2014), inter alia). We exclude dynamic phrase table extension, which has shown to
be important in online learning for post-editing, in
our theoretical analysis (Denkowski et al., 2014).
Learning from weak feedback is related to binary response-based learning where a meaning
representation is “tried out” by iteratively generating system outputs, receiving feedback from world
interaction, and updating the model parameters.
Such world interaction consists of database access
in semantic parsing (Kwiatowski et al. (2013), Berant et al. (2013), or Goldwasser and Roth (2013),
inter alia). Feedback in response-based learning
is given by a user accepting or rejecting system
predictions, but not by user corrections.
Lastly, feedback in form of numerical utility
values for actions is studied in the frameworks of
reinforcement learning (Sutton and Barto, 1998)
or in online learning with limited feedback, e.g.,
multi-armed bandit models (Cesa-Bianchi and Lugosi, 2006). Our framework replaces quantitative
feedback with immediate qualitative feedback in
form of a structured object that improves upon the
utility of the prediction.

3
3.1

1: Initialize w ← 0
2: for t = 1, . . . , T do
3: Observe xt
4: (yt , ht ) ← arg max(y,h) wt> φ(xt , y, h)
5: Obtain weak feedback ȳt
6: if yt 6= ȳt then
7:
8:

h̄t ← arg maxh wt> φ(xt , ȳt , h)

wt+1 ← wt +∆h̄t ,ht φ(xt , ȳt , h̄t )−φ(xt , yt , ht )

language model probability, distance-based and
lexicalized reordering probabilities, and word
and phrase penalty. We assume that the feature function has a bounded radius, i.e. that
kφ(x, y, h)k ≤ R for all x, y, h. By ∆h,h0 we
denote a distance function that is defined for any
h, h0 ∈ H, and is used to scale the step size of
updates during learning. In our experiments, we
use the ordinary Euclidean distance between the
feature vectors of derivations. We assume a linear
model with fixed parameters w∗ such that each
input example is mapped to its correct derivation and structured output by using (y ∗ , h∗ ) =
arg maxy∈Y(x),h∈H(x,y) w∗ > φ(x, y, h). We define
for each given input x, its highest scoring derivation over all outputs Y(x) such that h(x; w) =
arg maxh0 ∈H(x,y) maxy∈Y(x) w> φ(x, y, h0 )
and the highest scoring derivation for
a given output y
∈
Y(x) such that
h(x|y; w) = arg maxh0 ∈H(x,y) w> φ(x, y, h0 ). In
the following theoretical exposition we assume
that the arg max operation can be computed
exactly.

Coactive Learning for Online Latent
Structured Prediction

3.2

Feedback-based Latent Perceptron

We assume an online setting, in which examples
are presented one-by-one. The learner observes
an input xt , predicts an output structure yt , and
is presented with feedback ȳt about its prediction,
which is used to make an update to an existing parameter vector. Algorithm 1 is called ”Feedbackbased Latent Perceptron” to stress the fact that
it only uses weak feedback to its predictions for
learning, but does not necessarily observe optimal
structures as in the full information case (Sun et
al., 2013). Learning from full information can be
recovered by setting the informativeness parameter α to 1 in Equation (2) below, in which case
the feedback structure ȳt equals the optimal structure yt∗ . Algorithm 1 differs from the algorithm
of Shivaswamy and Joachims (2012) by a joint
maximization over output structures y and hid-

Notation and Background

Let X denote a set of input examples, e.g.,
sentences, and let Y(x) denote a set of structured
outputs for x ∈ X , e.g., translations. We define
Y = ∪x Y(x). Furthermore, by H(x, y) we
denote a set of possible hidden derivations for a
structured output y ∈ Y(x), e.g., for phrase-based
SMT, the hidden derivation is determined by a
phrase segmentation and a phrase alignment between source and target sentences. Every hidden
derivation h ∈ H(x, y) deterministically identifies
an output y ∈ Y(x). We define H = ∪x,y H(x, y).
Let φ : X ×Y ×H → Rd denote a feature function
that maps a triplet (x, y, h) to a d-dimensional
vector. For phrase-based SMT, we use 14 features, defined by phrase translation probabilities,
3

den derivations h in prediction (line 4), by choosing a hidden derivation h̄ for the feedback structure ȳ (line 7), and by the use of the re-scaling
factor ∆h̄t ,ht in the update (line 8), where h̄t =
h(xt |ȳt ; wt ) and ht = h(xt ; wt ) are the derivations of the feedback structure and the prediction
at time t, respectively. In our theoretical exposition, we assume that ȳt is reachable in the search
space of possible outputs, that is, ȳt ∈ Y(xt ).
3.3

where ξt ≥ 0 are slack variables allowing for violations of (2) for given α. For slack ξt = 0, user
feedback is called strictly α-informative.
3.4

A central theoretical result in learning from weak
feedback is an analysis that shows that Algorithm 1 minimizes an upper bound on the average
regret (1), despite the fact that optimal structures
are not used in learning:
PT
2
Theorem 1. Let DT =
t=1 ∆h̄t ,ht . Then the
average regret of the feedback-based latent perceptron can be upper bounded for any α ∈ (0, 1],
for any w∗ ∈ Rd :
√
T
1 X
2Rkw∗ k DT
REGT ≤
ξt +
.
αT
α
T

Feedback of Graded Utility

The key in the theoretical analysis in Shivaswamy
and Joachims (2012) is the notion of a linear utility
function, determined by parameter vector w∗ , that
is unknown to the learner:
Uh (x, y) = w∗ > φ(x, y, h).

t=1

Upon a system prediction, the user approximately
maximizes utility, and returns an improved object
ȳt that has higher utility than the predicted yt s.t.

A proof for Theorem 1 is similar to the proof
of Shivaswamy and Joachims (2012) and the original mistake bound for the perceptron of Novikoff
(1962).1 The theorem can be interpreted as follows: we expect lower average regret for higher
values of α; due to the dominant term T , regret
will approach the minimum of the accumulated
slack (in case feedback structures violate Equation (2)) or 0 (in case of strictly α-informative
feedback). The main difference between the above
result and the result of Shivaswamy and Joachims
(2012) is the term DT following from the rescaled distance of latent derivations. Their analysis is agnostic of latent derivations, and can be
recovered by setting this scaling factor to 1. This
yields
DT = T , and thus recovers the main fac√
DT
tor T = √1T in their regret bound. In our algorithm, penalizing large distances of derivations
can help to move derivations ht closer to h̄t , therefore decreasing DT as learning proceeds. Thus in
case DT < T , our bound is better than the original
bound of Shivaswamy and Joachims (2012) for a
perceptron without re-scaling. As we will show
experimentally, re-scaling leads to a faster convergence in practice.

U (xt , ȳt ) > U (xt , yt )
where for given x ∈ X , y ∈ Y(x), and h∗ =
arg maxh∈H(x,y) Uh (x, y), we define U (x, y) =
Uh∗ (x, y) and drop the subscript unless h 6= h∗ .
Importantly, the feedback is typically not the optimal structure yt∗ that is defined as
yt∗ = arg max U (xt , y).
y∈Y(xt )

While not receiving optimal structures in training,
the learning goal is to predict objects with utility close to optimal structures yt∗ . The regret that
is suffered by the algorithm when predicting object yt instead of the optimal object yt∗ is
REGT =

Convergence Analysis

T

1X
U (xt , yt∗ ) − U (xt , yt ) . (1)
T
t=1

To quantify the amount of information in the
weak feedback, Shivaswamy and Joachims (2012)
define a notion of α-informative feedback, which
we generalize as follows for the case of latent
derivations. We assume that there exists a derivation h̄t for the feedback structure ȳt , such that
for all predictions yt , the (re-scaled) utility of the
weak feedback ȳt is higher than the (re-scaled)
utility of the prediction yt by a fraction α of the
maximum possible utility range (under the given
utility model). Thus ∀t, ∃h̄t , ∀h and for α ∈ (0, 1]:

Uh̄t (xt ,ȳt ) − Uh (xt , yt ) × ∆h̄t ,h

≥ α U (xt , yt∗ ) − U (xt , yt ) − ξt , (2)

3.5

Generalization Analysis

Regret bounds measure how good the average prediction of the current model is on the next example
in the given sequence, thus it seems plausible that
a low regret on a sequence of examples should imply good generalization performance on the entire
domain of examples.
1

4

Short proofs are provided in the appendix.

Generalization for Online Learning. First we
present a generalization bound for the case of online learning on a sequence of random examples,
based on generalization bounds for expected average regret as given by Cesa-Bianchi et al. (2004).
Let probabilities P and expectations E be defined with respect to the fixed unknown underlying distribution according to which all examples
are drawn. Furthermore, we bound our loss function `t = U (xt , yt∗ ) − U (xt , yt ) to [0, 1] by adding
a normalization
factor 2R||w∗ || s.t. REGT =
1 PT
`
.
Plugging
the bound on REGT of Thet=1 t
T
orem 1 directly into Proposition 1 of Cesa-Bianchi
et al. (2004) gives the following theorem:

Condition 1. Algorithm 1 has converged on training instances x1 , . . . , xT after K epochs if the
predictions on x1 , . . . , xT using the final weight
vector wT,K are the same as the predictions on
x1 , . . . , xT in the Kth epoch.
Denote by EX (`(x)) the expected loss on
unseen data when using wT,K where `(x) =
U (x, y ∗ ) − U (x, y 0 ), y ∗ = arg maxy U (x, y) and
> φ(x, y, h). We can
y 0 = arg maxy maxh wT,K
now state the following result:
Theorem 3. Let 0 < δ < 1, and let x1 , . . . , xT
be a sample for the multiple-epoch perceptron algorithm such that the algorithm converged on it
(Condition 1). Then, with probability at least 1−δ,
the expected loss of the feedback-based latent perceptron satisfies:

Theorem 2. Let 0 < δ < 1, and let x1 , . . . , xT be
a sequence of examples that Algorithm 1 observes.
Then with probability at least 1 − δ,

T
1 X
2Rkw∗ k
EX (`(x)) ≤
ξt,K +
αT
α
t=1
s
8 ln 2δ
.
+ Rkw∗ k
T

√
1
2Rkw∗ k DT
E[REGT ] ≤
ξt +
αT
α
T
t=1
r
2 1
+ 2||w∗ ||R
ln .
T
δ
T
X

The generalization bound tells us how far the
expected average regret E[REGT ] (or average
risk, in terms of Cesa-Bianchi et al. (2004)) is from
the average regret that we actually observe in a
specific instantiation of the algorithm.

p
DT,K
T

The theorem can be interpreted as a bound on
the generalization error (lefthand-side) by the empirical error (the first two righthand-side terms)
and the variance caused by the finite sample (the
third term in the theorem). The result follows directly from McDiarmid’s concentration inequality.

Generalization for Online-to-Batch Conversion. In practice, perceptron-type algorithms are
often applied in a batch learning scenario, i.e.,
the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set (Freund and Schapire,
1999; Collins, 2002). The difference to the online
learning scenario is that we treat the multi-epoch
algorithm as an empirical risk minimizer that selects a final weight vector wT,K whose expected
loss on unseen data we would like to bound. We
assume that the algorithm is fed with a sequence
of examples x1 , . . . , xT , and at each epoch k =
1, . . . , K it makes a prediction yt,k . The correct
label is yt∗ . For k = 1, . . . , K and t = 1, . . . , T ,
let `t,k = U (xt , yt∗ ) − U (xt , yt,k ), and denote by
∆t,k and ξt,k the distance at epoch k for example
t, and the slack at epoch k for example
Pt, respectively. Finally, we denote by DT,K = Tt=1 ∆2t,K ,
and by wT,K the final weight vector returned after
K epochs. We state a condition of convergence2 :

4

Experiments

We used the LIG corpus3 which consists of 10,881
tuples of French-English post-edits (Potet et al.,
2012). The corpus is a subset of the newscommentary dataset provided at WMT4 and contains input French sentences, MT outputs, postedited outputs and English references. To prepare
SMT outputs for post-editing, the creators of the
corpus used their own WMT10 system (Potet et
al., 2010), based on the Moses phrase-based decoder (Koehn et al., 2007) with dense features.
We replicated a similar Moses system using the
same monolingual and parallel data: a 5-gram
language model was estimated with the KenLM
toolkit (Heafield, 2011) on news.en data (48.65M
sentences, 1.13B tokens), pre-processed with the
tools from the cdec toolkit (Dyer et al., 2010).
perceptron cycling theorem (Block and Levin, 1970; Gelfand
et al., 2010) should suffice to show a similar bound.
3

http://www-clips.imag.fr/geod/User/marion.potet/
index.php?page=download
4
http://statmt.org/wmt10/translation-task.html

2

This condition is too strong for large datasets. However,
we believe that a weaker condition based on ideas from the

5

0.90

α = 0.1
α = 0.5
α = 1.0

α = 0.1
α = 0.5
α = 1.0

0.80
0.70

0.31

0.60
0.50

TER

regret

0.32

0.40

0.30

0.30
0.20
0.10
0.00

0

4000

8000

12000

16000

0

20000

4000

8000

iterations

12000

16000

0.90

scaled; α = 0.1
scaled; α = 0.5
scaled; α = 1.0

scaled; α = 0.1
scaled; α = 0.5
scaled; α = 1.0

0.80
0.70

0.32

0.31

0.60
0.50

TER

regret

0.29
20000

iterations

0.40

0.30

0.30
0.20
0.10
0.00

0

4000

8000

12000

16000

0

20000

4000

iterations

8000

12000

16000

0.29
20000

iterations

Figure 1: Regret and TER vs. iterations for α-informative feedback ranging from weak (α = 0.1) to
strong (α = 1.0) informativeness, with (lower part) and without re-scaling (upper part).
Parallel data (europarl+news-comm, 1.64M sentences) were similarly pre-processed and aligned
with fast align (Dyer et al., 2013). In all experiments, training is started with the Moses default weights. The size of the n-best list, where
used, was set to 1,000. Irrespective of the use of
re-scaling in perceptron training, a constant learning rate of 10−5 was used for learning from simulated feedback, and 10−4 for learning from surrogate translations.
Our experiments on online learning require
a random sequence of examples for learning.
Following the techniques described in Bertsekas
(2011) to generate random sequences for incremental optimization, we compared cyclic order (K
epochs of T examples in fixed order), randomized
order (sampling datapoints with replacement), and
random shuffling of datapoints after each cycle,
and found nearly identical regret curves for all
three scenarios. In the following, all figures are
shown for sequences in the cyclic order, with redecoding after each update. Furthermore note that
in all three definitions of sequence, we never see
the fixed optimal feedback yt∗ in training, but instead in general a different feedback structure ȳt
(and a different prediction yt ) every time we see
the same input xt .
4.1

# datapoints
TER(ȳt ) < TER(yt )
TER(ȳt ) = TER(yt )
TER(ȳt ) > TER(yt )

strict (ξt = 0)

slack (ξt > 0)

5,725

1,155

52.17%
23.95%
23.88%

32.55%
20.52%
46.93%

Table 1: Improved utility vs. improved TER distance to human post-edits for α-informative feedback ȳt compared to prediction yt using default
weights at α = 0.1.
this experiment is to confirm our theoretical analysis by showing convergence in regret for learning from weak and strong feedback. We select
feedback of varying grade by directly inspecting
the optimal w∗ , thus this feedback is idealized.
However, the experiment also has a realistic background since we show that α-informative feedback
corresponds to improvements under standard evaluation metrics such as lowercased and tokenized
TER, and that learning from weak and strong feedback leads to convergence in TER on test data.
For this experiment, the post-edit data from the
LIG corpus were randomly split into 3 subsets:
PE-train (6,881 sentences), PE-dev, and PE-test
(2,000 sentences each). PE-train was used for
our online learning experiments. PE-test was held
out for testing the algorithms’ progress on unseen
data. PE-dev was used to obtain w∗ to define the
utility model. This was done by MERT optimization (Och, 2003) towards post-edits under the TER
target metric. Note that the goal of our experi-

Idealized Weak and Strong Feedback

In a first experiment, we apply Algorithm 1 to
user feedback of varying utility grade. The goal of
6

% strictly α-informative
39.46%
47.73%
83.30%

test, α = 0.1
test, α = 0.5
test, α = 1.0
train, α = 0.1
train, α = 0.5
train, α = 1.0

0.20
average loss `t

local
filtered
hope

0.25

Table 2: α-informativeness of surrogacy modes.

0.15

0.10

ments is not to improve SMT performance over
any algorithm that has access to full information to
compute w∗ . Rather, we want to show that learning from weak feedback leads to convergence in
regret with respect to the optimal model, albeit
at a slower rate than learning from strong feedback. The feedback data in this experiment were
generated by searching the n-best list for translations that are α-informative at α ∈ {0.1, 0.5, 1.0}
(with possible non-zero slack). This is achieved
by scanning the n-best list output for every input
xt and returning the first ȳt 6= yt that satisfies
Equation (2).5 This setting can be thought of as an
idealized scenario where a user picks translations
from the n-best list that are considered improvements under the optimal w∗ .
In order to verify that our notion of graded utility corresponds to a realistic concept of graded
translation quality, we compared improvements in
utility to improved TER distance to human postedits. Table 1 shows that for predictions under
default weights, we obtain strictly α-informative
(for α = 0.1) feedback for 5,725 out of 6,881
datapoints in PE-train. These feedback structures
improve utility per definition, and they also yield
better TER distance to post-edits in the majority
of cases. A non-negative slack has to be used in
1,155 datapoins. Here the majority of feedback
structures do not improve TER distance.
Convergence results for different learning scenarios are shown in Figure 1. The left upper part
of Figure 1 shows average utility regret against
iterations for a setup without re-scaling, i.e., setting ∆h̄,h = 1 in the definition of α-informative
feedback (Equation (2)) and in the update of Algorithm 1 (line 8). As predicted by our regret
analysis, higher α leads to faster convergence, but
all three curves converge towards a minimal regret. Also, the difference between the curves for

0.05

1

2

3

4

5

6

7

8

9

10

epochs

Figure 3: Average loss `t on heldout and train data.
α = 0.1 and α = 1.0 is much smaller than a factor of ten. As expected from the correspondence of
α-informative feedback to improvements in TER,
similar relations are obtained when plotting TER
scores on test data for training from weak feedback at different utility grades. This is shown in
the right upper part of Figure 1.
The left lower part of Figure 1 shows average
utility regret plotted against iterations for a setup
that uses re-scaling. We define ∆h̄t ,h by the `2 distance between the feature vectors φ(xt , ȳt , h̄t )
of the derivation of the feedback structure and the
feature vector φ(xt , yt , ht ) of the derivation of the
predicted structure. We see that the curves for all
grades of feedback converge faster than the corresponding curves for un-scaled feedback shown in
the upper part Figure 1. Furthermore, as shown in
the right lower part of Figure 1, TER is decreased
on test data as well at a faster rate.6
Lastly, we present an experimental validation of
the online-to-batch application of our algorithm.
That is, we would like to evaluate predictions that
use the final weight vector wT,K by comparing the
generalization error with the empirical error stated
in Theorem 3. The standard way to do this is to
compare the average loss on heldout data with the
the average loss on the training sequence. Figure 3 shows these results for models trained on
α-informative feedback of α ∈ {0.1, 0.5, 1.0} for
10 epochs. Similar to the online learning setup,
higher α results in faster convergence. Furthermore, curves for training and heldout evaluation
converge at the same rate.
4.2

5

Note that feedback provided in this way might be
stronger than required at a particular value of α since for all
β ≥ α, strictly β-informative feedback is also strictly αinformative. On the other hand, because of the limited size of
the n-best list, we cannot assume strictly α-informative user
feedback with zero slack ξt . In experiments where updates
are only done if feedback is strictly α-informative we found
similar convergence behavior.

Feedback from Surrogate Translations

In this section, we present experiments on learning from real human post-edits. The goal of
this experiment is to investigate whether the stan6

We also conducted online-to-batch experiments for simulated feedback at α ∈ {0.1, 0.5, 1.0}. Similar to the online
learning setup, higher α results in faster convergence.

7

1.00
regret

0.35

α = 0.1
α = 1.0
oracles
local
filtered
hope

1.20

0.34
0.33

0.80

0.32

0.60

α = 0.1
α = 1.0
oracles
local
filtered
hope

0.40
0.20
0.00

0

4000

8000

12000

16000

0

20000

4000

8000

12000

16000

TER

1.40

0.31
0.30
0.29
20000

iterations

iterations

Figure 2: Regret and TER for online learning from oracles, local, filtered, and hope surrogates.
dard practices for extracting feedback from observed user post-edits for discriminative SMT can
be matched with the modeling assumptions of
the coactive learning framework. The customary practice in discriminative learning for SMT is
to replace observed user translations by surrogate
translations since the former are often not reachable in the search space of the SMT decoder. In
our case, only 29% of the post-edits in the LIGcorpus were reachable by the decoder. We compare four heuristics of generating surrogate translations: oracles are generated using the lattice oracle approach of Sokolov et al. (2013) which returns the closest path in the decoder search graph
as reachable surrogate translation.7 A local surrogate ỹ is chosen from the n-best list of the
linear model as the translation that achieves the
best TER score with respect to the actual postedit y: ỹ = arg miny0 ∈n-best(xt ;wt ) TER(y 0 , y).
This corresponds to the local update mode of
Liang et al. (2006). A filtered surrogate translation ỹ is found by scanning down the n-best
list, and accepting the first translation as feedback that improves TER score with respect to the
human post-edit y over the 1-best prediction yt
of the linear model: TER(ỹ, y) < TER(yt , y).
Finally, a hope surrogate is chosen from the nbest list as the translation that jointly maximizes
model score under the linear model and negative TER score with respect to the human postedit: ỹ = arg maxy0 ∈n-best(xt ;wt ) (−TER(y 0 , y) +
wt> φ(xt , y 0 , h)). This corresponds to what Chiang (2012) termed “hope derivations”. Informally,
oracles are model-agnostic, as they can pick a
surrogate even from outside of the n-best list;
local is constrained to the n-best list, though
still ignoring the ordering according to the linear

model; finally, filtered and hope represent different ways of letting the model score influence
the selected surrogate.
As shown in Figure 2, regret and TER decrease with the increased amount of information
about the assumed linear model that is induced by
the surrogate translations: Learning from oracle
surrogates does not converge in regret and TER.
The local surrogates extracted from 1,000-best
lists still do not make effective use of the linear
model, while filtered surrogates enforce an improvement over the prediction under TER towards
the human post-edit, and improve convergence in
learning. Empirically, convergence is achieved
only for hope surrogates that jointly maximize
negative TER and linear model score, with a convergence behavior that is very similar to learning
from weak α-informative feedback at α ' 0.1.
We quantify this in Table 2 where we see that the
improvement in TER over the prediction that holds
for any hope derivation, corresponds to an improvement in α-informativeness: hope surrogates
are strictly α-informative in 83.3% of the cases
in our experiment, whereas we find a correspondence to strict α-informativeness only in 45.74%
or 39.46% of the cases for filtered and local
surrogates, respectively.

5

Discussion

We presented a theoretical analysis of online
learning for SMT from a coactive learning perspective. This viewpoint allowed us to give regret
and generalization bounds for perceptron-style online learners that fall outside the convex optimization scenario because of latent variables and
changing feedback structures. We introduced the
concept of weak feedback into online learning for
SMT, and provided proof-of-concept experiments
whose goal was to show that learning from weak
feedback converges to minimal regret, albeit at a

7

While the original algorithm is designed to maximize the
BLEU score of the returned path, we tuned its two free parameters to maximize TER.

8

slower rate than learning from strong feedback.
Furthermore, we showed that the SMT standard
of learning from surrogate hope derivations can to
be interpreted as a search for weak improvements
under the assumed linear model. This justifies
the importance of admitting an underlying linear
model in computing surrogate derivations from a
coactive learning perspective.
Finally, we hope that our analysis motivates further work in which the idea of learning from weak
feedback is taken a step further. For example,
our results could perhaps be strengthened by applying richer feature sets or dynamic phrase table
extension in experiments on interactive SMT. Our
theory would support a new post-editing scenario
where users pick translations from the n-best list
that they consider improvements over the prediction. Furthermore, it would be interesting to see if
“light” post-edits that are better reachable and easier elicitable than “full” post-edits provide a strong
enough signal for learning.

The first equality again uses the update rule from Algorithm
1. The second follows by induction. The last equality applies
the definition of utility.
Next we upper bound the utility difference:
T
X

∆h̄t ,ht Uh̄t (xt , ȳt ) − Uht (xt , yt )
t=1

kwT +1 k = wT>+1 wT +1 .
The final result is obtained simply by lower bounding
Equation (5) using the assumption in Equation (2).
√
kw∗ k2R DT
≥

≥α

ξt .

Theorem 4 (McDiarmid, 1989). Let Z1 , . . . , Zm be a set
of random variables taking value in a set Z. Further, let
f : Z m → R be a function that satisfies for all i and
z1 , . . . , zm , zi0 ∈ Z:
|f (z1 , . . . , zi , . . . , zm )
− f (z1 , . . . , zi0 , . . . , zm )| ≤ c,

(6)

for some c. Then for all  > 0,
P(|f − E(f )| > ) ≤ 2 exp(−


φ(xT , ȳT , h̄T ) − φ(xT , yT , hT ) ∆h̄T ,hT
+
>
+ φ(xT , ȳT , h̄T ) − φ(xT , yT , hT ) ∆h̄T ,hT

φ(xT , ȳT , h̄T ) − φ(xT , yT , hT ) ∆h̄T ,hT
≤ wT> wT + 4R2 ∆2h̄T ,hT ≤ 4R2 DT .

(3)

>

>

(7)

|f (x1 , . . . , xt , . . . , xT ) − f (x1 , . . . , x0t , . . . , xT )|
=|

>
w>
T +1 w∗ = wT w∗

+ ∆h̄T ,hT φ(xT , ȳT , h̄T )) − φ(xT , yT , hT )

22
).
mc2

Let f be the average loss for predicting yt on example xt
P
in epoch K: f (x1 , . . . , xT ) = REGT,K = T1 Tt=1 `t,K .
Because of the convergence condition (Condition 1), `t,K =
P
`(xt ). The expectation of f is E(f ) = T1 Tt=1 E[`t,k ] =
P
T
1
t=1 E[`(xt )] = EX (`(x)).
T
The first and second term on the righthand-side of Theorem 3 follow from upper bounding REGT in the Kth epochs,
using Theorem 1. The third term is derived by calculating c
in Equation (6) as follows:

The first equality uses the update rule from Algorithm
1. The second uses the fact that wT> (φ(xT , ȳT , h̄T ) −
φ(xT , yT , hT )) ≤ 0 by definition of (yT , hT ) in Algorithm 1. By assumption kφ(x, y, h)k ≤ R, ∀x, y, h and
by the triangle inequality, kφ(x, y, h) − φ(x, y 0 , h0 )k ≤
kφ(x, y, h)k + kφ(x, y 0 , h0 )k ≤ 2R. Finally, DT =
PT
2
t=1 ∆h̄t ,ht by definition, and the last inequality follows
by induction.
The connection to average regret is as follows:

t=1

t=1
T
X

Proof. The theorem can be shown by an application of McDiarmid’s concentration inequality:

2wT>


∆h̄t ,ht Uh̄t (xt , ȳt ) − Uht (xt , yt ) .

T
 X
U (xt , yt∗ ) − U (xt , yt ) −
ξt

Proof of Theorem 3

>
w>
T +1 wT +1 = wT wT

=



t=1

Proof. First we bound wT>+1 wT +1 from above:

t=1

T
X

= α T REGT −

Proof of Theorem 1

T
X

∆h̄t ,ht Uh̄t (xt , ȳt ) − Uht (xt , yt )

t=1

Appendix: Proofs of Theorems

∆h̄t ,ht φ(xt , ȳt , h̄t ) − φ(xt , yt , ht )

T
X
t=1

This research was supported in part by DFG
grant RI-2221/2-1 “Grounding Statistical Machine
Translation in Perception and Action.”

=

(5)

The first inequality follows from applying the CauchySchwartz inequality wT>+1 w∗ ≤ kw∗ kkwT +1 k to Equation (4). The
q seond follows from applying Equation (3) to

Acknowledgments

T
X

√
≤ kw∗ kkwT +1 k ≤ kw∗ k2R DT .

≤
w∗

T
T
T

1 X 0
1 X
1 X
`t,K −
`t,K | = |
`t,K − `0t,K |
T t=1
T t=1
T t=1

T

4Rkw∗ k
1 X
|`t,k | + |`0t,K | ≤
= c.
T t=1
T

The first inequality uses the triangle inequality; the second uses the upper bound |`t,k | ≤ 2R||w∗ ||. Setting the
righthand-side of Equation (7) to at least δ and solving for ,
using c, concludes the proof.

w∗
(4)

9

References

Chuong B. Do, Quoc Le, and Choon Hui Teo. 2008.
Tighter bounds for structured estimation. In NIPS,
Vancouver, Canada.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In EMNLP, Seattle, WA.

Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Türe, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In ACL, Uppsala, Sweden.

Nicola Bertoldi, Patrick Simianer, Mauro Cettolo,
Katharina Wäschle, Marcello Federico, and Stefan
Riezler. 2014. Online adaptation to post-edits for
phrase-based statistical machine translation. Machine Translation, 29:309–339.

Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameterization of IBM model 2. In NAACL, Atlanta, GA.

Dimitri P. Bertsekas. 2011. Incremental gradient,
subgradient, and proximal methods for convex optimization: A survey. In Suvrit Sra, Sebastian
Nowozin, and Stephen J. Wright, editors, Optimization for Machine Learning. MIT Press.

Vladimir Eidelmann. 2012. Optimization strategies
for online large-margin learning in machine translation. In WMT, Montreal, Canada.

Henry D. Block and Simon A. Levin. 1970. On the
boundedness of an iterative procedure for solving
a system of linear inequalities. Proceedings of the
American Mathematical Society, 26(2):229–235.

Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Journal of Machine Learning Research, 37:277–
296.

Leon Bottou and Olivier Bousquet. 2004. Large scale
online learning. In NIPS, Vancouver, Canada.

Andrew E. Gelfand, Yutian Chen, Max Welling, and
Laurens van der Maaten. 2010. On herding and the
perceptron cycling theorem. In NIPS, Vancouver,
Canada.

Nicolò Cesa-Bianchi and Gàbor Lugosi. 2006. Prediction, Learning, and Games. Cambridge University
Press.

Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
NAACL, Montreal, Canada.

Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. 2004. On the generalization ablility of on-line
learning algorithms. IEEE Transactions on Information Theory, 50(9):2050–2057.

Dan Goldwasser and Dan Roth. 2013. Learning from
natural instructions. Machine Learning, 94(2):205–
232.

Nicolò Cesa-Bianchi, Gabriele Reverberi, and Sandor Szedmak. 2008. Online learning algorithms
for computer-assisted translation. Technical report,
SMART (www.smart-project.eu).

Spence Green, Jeffrey Heer, and Christopher D. Manning. 2013. The efficacy of human post-editing for
language translation. In CHI, Paris, France.
Spence Green, Sida I. Wang, Jason Chuang, Jeffrey
Heer, Sebastian Schuster, and Christopher D. Manning. 2014. Human effort and machine learnability in computer aided translation. In EMNLP, Doha,
Qatar.

David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and structural translation features. In EMNLP, Waikiki, HA.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation. In NAACL, Boulder, CO.

Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In WMT, Edinburgh, UK.

David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. Journal of
Machine Learning Research, 12:1159–1187.

Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Birch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, Prague, Czech Republic.

Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In EMNLP,
Philadelphia, PA.
Ronan Collobert, Fabian Sinz, Jason Weston, and Leon
Bottou. 2006. Trading convexity for scalability. In
ICML, Pittsburgh, PA.

Tom Kwiatowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In EMNLP, Seattle,
WA.

Michael Denkowski, Chris Dyer, and Alon Lavie.
2014. Learning from post-editing: Online model
adaptation for statistical machine translation. In
EACL, Gothenburg, Sweden.

Percy Liang, Alexandre Bouchard-Côté, Dan Klein,
and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In COLINGACL, Sydney, Australia.

10

Francisco-Javier López-Salcedo, Germán SanchisTrilles, and Francisco Casacuberta. 2012. Online
learning of log-linear weights in interactive machine
translation. In IberSpeech, Madrid, Spain.

Artem Sokolov, Guillaume Wisniewski, and François
Yvon. 2013. Lattice BLEU oracles in machine
translation. Transactions on Speech and Language
Processing, 10(4):18.

Pascual Martı́nez-Gómez, Germán Sanchis-Trilles, and
Francisco Casacuberta. 2012. Online adaptation
strategies for statistical machine translation in postediting scenarios. Pattern Recognition, 45(9):3193–
3202.

Artem Sokolov, Stefan Riezler, and Shay B. Cohen.
2015. Coactive learning for interactive machine
translation. In ICML Workshop on Machine Learning for Interactive Systems (MLIS), Lille, France.
Xu Sun, Takuya Matsuzaki, and Wenjie Li. 2013.
Latent structured perceptrons for large scale learning with hidden information. IEEE Transactions
on Knowledge and Data Engineering, 25(9):2064–
2075.

David McAllester and Joseph Keshet. 2011. Generalization bounds and consistency for latent structural
probit and ramp loss. In NIPS, Granada, Spain.
Colin McDiarmid. 1989. On the method of bounded
differences. Surveys in combinatorics, 141(1):148–
188.

Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement Learning. An Introduction. The MIT
Press.

Albert B.J. Novikoff. 1962. On convergence proofs on
perceptrons. Symposium on the Mathematical Theory of Automata, 12:615–622.

Christoph Tillmann and Tong Zhang. 2006. A discriminative global training algorithm for statistical MT.
In COLING-ACL, Sydney, Australia.

Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In NAACL, Edmonton, Canada.

Katharina Wäschle, Patrick Simianer, Nicola Bertoldi,
Stefan Riezler, and Marcello Federico. 2013. Generative and discriminative methods for online adaptation in SMT. In MT Summit, Nice, France.

Marion Potet, Laurent Besacier, and Hervé Blanchon.
2010. The LIG machine translation system for
WMT 2010. In WMT, Upsala, Sweden.

Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2006. NTT statistical machine
translation for IWSLT 2006. In IWSLT, Kyoto,
Japan.

Marion Potet, Emanuelle Esperança-Rodier, Laurent
Besacier, and Hervé Blanchon. 2012. Collection
of a large database of French-English SMT output
corrections. In LREC, Istanbul, Turkey.

Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In EMNLP,
Prague, Czech Republic.

Avneesh Saluja and Ying Zhang.
2014.
Online discriminative learning for machine translation
with binary-valued feedback. Machine Translation,
28:69–90.

Taro Watanabe. 2012. Optimized online rank learning for machine translation. In NAACL, Montreal,
Canada.

Avneesh Saluja, Ian Lane, and Ying Zhang. 2012.
Machine translation with binary feedback: A largemargin approach. In AMTA, San Diego, CA.

Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decoding for scalable MT training. In EMNLP, Seattle,
WA.

Shai Shalev-Shwartz. 2012. Online learning and online convex optimization. Foundations and Trends
in Machine Learning, 4(2):107–194.

Alan Yuille and Anand Rangarajan. 2003. The
concave-convex procedure. Neural Computation,
15:915–936.

Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
NAACL, Boston, MA.
Pannaga Shivaswamy and Thorsten Joachims. 2012.
Online structured prediction via coactive learning.
In ICML, Edinburgh, UK.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learning for large-scale discriminative training in SMT.
In ACL, Jeju, Korea.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA, Cambridge, MA.

11

