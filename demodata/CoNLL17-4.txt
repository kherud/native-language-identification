Parsing for Grammatical Relations via Graph Merging
Weiwei Sun, Yantao Du and Xiaojun Wan
Institute of Computer Science and Technology, Peking University
The MOE Key Laboratory of Computational Linguistics, Peking University
{ws,duyantao,wanxiaojun}@pku.edu.cn

Abstract

ing parsing algorithms. To deal with this problem,
we propose graph merging, a new perspective, for
building flexible representations. The basic idea is
to decompose a GR graph into several subgraphs,
each of which captures most but not the complete
information. On the one hand, each subgraph is
simple enough to allow efficient construction. On
the other hand, the combination of all subgraphs
enables whole target GR structure to be produced.
There are two major problems in the graph
merging perspective. First, how to decompose
a complex graph into simple subgraphs in a
principled way? To deal with this problem,
we considered structure-specific properties of the
syntactically-motivated GR graphs. One key property is their reachability: In a given GR graph,
almost every node is reachable from a same and
unique root. If a node is not reachable, it is disconnected from other nodes. This property ensures a GR graph to be successfully decomposed
into limited number of forests, which in turn can
be accurately and efficiently built via tree parsing.
We model the graph decomposition problem as an
optimization problem and employ Lagrangian Relaxation for solutions.
Second, how to merge subgraphs into one coherent structure in a principled way? The problem of finding an optimal graph that consistently
combines the subgraphs obtained through individual models is non-trivial. We treat this problem as
a combinatory optimization problem and also employ Lagrangian Relaxation to solve the problem.
In particular, the parsing phase consists of two
steps. First, graph-based models are applied to assign scores to individual arcs and various tuples of
arcs. Then, a Lagrangian Relaxation-based joint
decoder is applied to efficiently produces globally
optimal GR graphs according to all graph-based
models.
We conduct experiments on Chinese GRBank

This paper is concerned with building
deep grammatical relation (GR) analysis
using data-driven approach. To deal with
this problem, we propose graph merging, a
new perspective, for building flexible dependency graphs: Constructing complex
graphs via constructing simple subgraphs.
We discuss two key problems in this perspective: (1) how to decompose a complex graph into simple subgraphs, and (2)
how to combine subgraphs into a coherent complex graph. Experiments demonstrate the effectiveness of graph merging.
Our parser reaches state-of-the-art performance and is significantly better than two
transition-based parsers.

1

Introduction

Grammatical relations (GRs) represent functional
relationships between language units in a sentence. Marking not only local but also a wide
variety of long distance dependencies, GRs encode in-depth information of natural language sentences. Traditionally, GRs are generated as a byproduct by grammar-guided parsers, e.g. RASP
(Carroll and Briscoe, 2002), C&C (Clark and Curran, 2007b) and Enju (Miyao et al., 2007). Very
recently, by representing GR analysis using general directed dependency graphs, Sun et al. (2014)
and Zhang et al. (2016) showed that considerably
good GR structures can be directly obtained using
data-driven, transition-based parsing techniques.
We follow their encouraging work and study the
data-driven approach for producing GR analyses.
The key challenge of building GR graphs is due
to their flexibility. Different from surface syntax, the GR graphs are not constrained to trees,
which is a fundamental consideration in design26

Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 26–35,
Vancouver, Canada, August 3 - August 4, 2017. c 2017 Association for Computational Linguistics

obj
root
subj

subj*ldd

root

subj

comp

temp

temp

obj

prt

comp
obj
nmod

prt

relative
nmod

浦东
近年 来 颁布 实行 了 涉及
经济
领域 的 法规性
文件
Pudong recently
issue practice
involve economic field
regulatory document
Figure 1: An example: Pudong recently enacted regulatory documents involving the economic field.
proach can be applied to build Chinese GR structures with very promising results. This architecture is complementary to the traditional approach
to English GR analysis, which leverages grammarguided parsing under deep formalisms, such as
LFG (Kaplan et al., 2004), CCG (Clark and Curran,
2007a) and HPSG (Miyao et al., 2007). We follow
Sun et al.’s and Zhang et al.’s encouraging work
and study the discriminative, factorization models
for obtaining GR analysis.

(Sun et al., 2014). Though our parser does not
use any phrase-structure information, it produces
high-quality GR analysis with respect to dependency matching. Our parsers obtain a labeled fscore of 84.57 on the test set, resulting in an error reduction of 15.13% over Sun et al. (2014)’s
single system. and 10.86% over Zhang et al.
(2016)’s system. The remarkable parsing result
demonstrates the effectiveness of the graph merging framework. This framework can be adopted to
other types of flexible representations, e.g. semantic dependency graphs (Oepen et al., 2014, 2015)
and abstract meaning representations (Banarescu
et al., 2013).

2

3

The Idea

The key idea of this work is constructing a
complex structure via constructing simple partial
structures. Each partial structure is simple in the
sense that it allows efficient construction. For instance, projective trees, 1-endpoint-corssing trees,
non-crossing dependency graphs and 1-endpointcrossing, pagenumber-2 graphs can be taken as
simple structures, given that low-degree polynomial time parsing algorithms exist (Eisner, 1996;
Pitler et al., 2013; Kuhlmann and Jonsson, 2015;
Cao et al., 2017; Sun et al., 2017). To construct
each partial structure, we can employ mature parsing techniques. To get the final target output, we
also require the total of all partial structures enables whole target structure to be produced. In this
paper, we exemplify the above idea by designing
a new parser for obtaining GR graphs. Take the
GR graph in Figure 1 for example. It can be decomposed into two tree-like subgraphs, shown in
Figure 2. If we can parse the sentence into subgraphs and combine them in a principled way, we
get the original GR graph.
Under this perspective, we need to develop a
principled method to decompose a complex structure into simple sturctures, which allows us to generate data to train simple solvers. We also need
to develop a principled method to integrate partial
structures, which allows us to produce coherent

Background

In this paper, we focus on building GR analysis for Mandarin Chinese. Mandarin is an analytic language that lacks inflectional morphology (almost) entirely and utilizes highly configurational ways to convey syntactic and semantic
information. This analytic nature allows to represent all GRs as bilexical dependencies. Sun et al.
(2014) showed that analysis for a variety of complicated linguistic phenomena, e.g. coordination,
raising/control constructions, extraction, topicalization, can be conveniently encoded with directed
graphs. Moreover, such deep syntactic dependency graphs can be effectively derived from Chinese TreeBank (Xue et al., 2005) with very high
quality. Figure 1 is an example. In this graph,
“subj*ldd” between the word “涉及/involve” and
the word “文 件/documents” represents a longdistance subject-predicate relation. The arguments
and adjuncts of the coordinated verbs, namely “颁
布/issue” and “实行/practice,” are separately yet
distributively linked to the two heads.
By encoding GRs as directed graphs over
words, Sun et al. (2014) and Zhang et al. (2016)
showed that the data-driven, transition-based ap27

obj
root
subj
comp

comp
obj

prt

relative
nmod

nmod

temp

浦东
近年 来 颁布 实行 了 涉及
经济
领域 的 法规性
文件
Pudong recently
issue practice
involve economic field
regulatory document
comp

prt

temp

nmod

nmod

obj

subj
root

subj[inverse]
obj

Figure 2: A graph decomposition for the GR graph in Figure 1. The two subgraphs are shown on two
sides of the sentence respectively. The subgraph on the upper side of the sentence is exactly a tree,
while the one on the lower side is slightly different. The edge from the word “文件/document” to “涉
及/involve” is tagged “[inverse]” to indicate that the direction of the edge in the subgraph is in fact
opposite to that in the original graph.
sures that all edges in y appear at least in one subgraph.

structures as outputs. We are going to demonstrate
the techniques we use to solve these two problems.

4
4.1

For a specific graph decomposition task, we
should define good score functions sk and graph
classes Gk according to key properties of the target structure y.

Decomposing GR Graphs
Graph Decomposition as Optimization

Given a sentence s = w1 w2 · · · wn of length n,
we use a vector y of length n2 to denote a graph
on it. We use indices i and j to index the elements
in the vector, y(i, j) ∈ {0, 1}, denoting whether
there is an arc from wi to wj (1 ≤ i, j ≤ n).
Given a graph y, we hope to find m subgraphs
y1 , ..., ym , each of which belongs to a specific
class of graphs Gk (k = 1, 2, · · · , m). Each class
should allow efficient construction. For example,
we may need a subgraph to be a tree or a noncrossing dependency graph. The combination of
all yk gives enough information to construct y.
Furthermore, the graph decomposition procedure
is utilized to generate training data for building
sub-models. Therefore, we hope each subgraph yk
is informative enough to train a good disambiguation model. To do so, for each yk , we define a
score function sk that indicates the “goodness” of
yk . Integrating all ideas, we can formalize graph
decomposition as an optimization problem,
P
max.
k sk (yk )
s.t.
y
Pi belongs to Gi
k yk (i, j) ≥ y(i, j), ∀i, j

4.2

Decomposing GR Graphs into Tree-like
Subgraphs

One key property of GR graphs is their reachability: Every node is either reachable from a unique
root or by itself an independent connected component. This property allows a GR graph to be
decomposed into limited number of tree-like subgraphs. By tree-like we mean if we treat a graph
on a sentence as undirected, it is a tree, or it is a
subgraph of some tree on the sentence. The advantage of tree-like subgraphs is that they can be
effectively built by adapting data-driven tree parsing techniques. Take the sentence in Figure 1 for
example. For every word, there is at least one path
link the virtual root and this word. Furthermore,
we can decompose the graph into two tree-like
subgraphs, as shown in Figure 2. In this decomposition, one subgraph is exactly a tree, and the
other is very close to a tree.
We restrict the number of subgraphs to 3. The
intuition is that we use one tree to capture long
distance information and the other two to capture

The last condition in this optimization problem en28

coordination information.1 In other words, we decompose each given graph y into three tree-like
subgraphs g1 , g2 and g3 . The goal is to let g1 , g2
and g3 carry important information of the graph
as well as cover all edges in y. The optimization
problem can be written as
max. s1 (g1 ) + s2 (g2 ) + s3 (g3 )
s.t.
g1 , g2 , g3 are tree-like
g1 (i, j) + g2 (i, j) + g3 (i, j) ≥ y(i, j), ∀i, j
4.2.1 Scoring a Subgraph
We score a subgraph in a first order arc-factored
way, which first scores the edges separately and
then adds up the scores.
Formally, the score funcP
tion is sk (g) =
ωk (i, j)gk (i, j) (k = 1, 2, 3)
where ωk (i, j) is the score of the edge from i to
j. Under this score function, we can use the Maximum Spanning Tree (MST) algorithm (Chu and
Liu, 1965; Edmonds, 1967; Eisner, 1996) to decode the tree-like subgraph with the highest score.
After we define the score function, extracting a
subgraph from a GR graph works like this: We
first assign heuristic weights ωk (i, j) (1 ≤ i, j ≤
n) to the potential edges between all the pairs of
words, then compute a best projective tree gk using the Eisner’s Algorithm:
X
gk = arg max sk (g) = arg max
ωk (i, j)g(i, j).
g

subgraphs. We devise three variations of weight
assignment: ω1 , ω2 , and ω3 . Each ωk (k is 1,2
or 3) consists of two parts. One is shared by
all, denoted by S, and the other is different from
each other, denoted by V . Formally, ωk (i, j) =
S(i, j) + Vk (i, j) (k = 1, 2, 3 and 1 ≤ i, j ≤ n).
Given a graph y, S is defined as S(i, j) =
S1 (i, j) + S2 (i, j) + S3 (i, j) + S4 (i, j), where

S1 (i, j) =


c1 if y(i, j) = 1 or y(j, i) = 1
0 else

c2 if y(i, j) = 1
0 else
S3 (i, j) = c3 (n − |i − j|)

S2 (i, j) =

S4 (i, j) = c4 (n − lp (i, j))
In the definitions above, c1 , c2 , c3 and c4 are
coefficients, satisfying c1  c2  c3 , and lp is a
function of i and j. lp (i, j) is the length of shortest
path from i to j that either i is a child of an ancestor of j or j is a child of an ancestor of i. That is
to say, the paths are in the form i ← n1 ← · · · ←
nk → j or i ← n1 → · · · → nk → j. If no such
path exits, then lp (i, j) = n. The intuition behind
the design is illustrated below.
S1 indicates whether there is an edge between i
and j, and we want it to matter mostly;

g

S2 indicates whether the edge is from i to j, and
we want the edge with correct direction to be
selected more likely;

gk is not exactly a subgraph of y, because there
may be some edges in the tree but not in the graph.
To guarantee we get a subgraph of the original
graph, we add labels to the edges in trees to encode
necessary information. We label gk (i, j) with the
original label, if y(i, j) = 1; with the original label appended by “∼R” if y(j, i) = 1; with “None”
else. With this labeling, we can have a function
t2g to transform the extracted trees into tree-like
graphs. t2g(gk ) is not necessary the same as the
original graph y, but must be a subgraph of it.

S3 indicates the distance between i and j, and we
like the edge with short distance because it is
easier to predict;
S4 indicates the length of certain type of path between i and j that reflects c-commanding relationships, and the coefficient remains to be
tuned.

4.2.2 Three Variations of Scoring
With different weight assignments, we can extract
different trees from a graph, obtaining different

We want the score V to capture different information of the GR graph. In GR graphs, we have
an additional information (as denoted as “*ldd”
in Figure 1) for long distance dependency edges.
Moreover, we notice that conjunction is another
important structure, and they can be derived from
the GR graph. Assume that we tag the edges relating to conjunctions with “*cjt.” The three variation scores, i.e. V1 , V2 and V3 , reflect long distance
and the conjunction information in different ways.

1

In this paper, we employ projective parsers. The minimal number of sub-graphs is related to the pagenumber of GR
graphs. The pagenumber of 90.96% GR graphs is smaller
than or equal to 2, while the pagenumber of 98.18% GR
graphs is at most 3. That means 3 projective trees are perhaps
good enough to handle Chinese sentences, but 2 projective
trees are not. Due to the empirical results in Table 3, using
three projective trees can handle 99.55% GR arcs. Therefore,
we think three is suitable for our problem.

29

Algorithm 1: The Tree Extraction Algorithm

X*ldd
X*cjt

Initialization: set u(0) to 0
for k = 0 to K do
g1 ← arg maxg1 s1 (g1 ) + u(k)> g1
g2 ← arg maxg2 s2 (g2 ) + u(k)> g2
g3 ← arg maxg3 s3 (g3 ) + u(k)> g3
if max{g1 , g2 , g3 } = y then
return g1 , g2 , g3
(k+1)
u
←
u(k) − α(k) (max{g1 , g2 , g3 } − y)
return g1 , g2 , g3

X*cjt
X*cjt

wp ... wc1 ... wgc2 ... wgc1 ... wc2 ... wl

Figure 3: Examples to illustrate the additional
weights.
V1 . First for edges y(i, j) whose label is tagged
with *ldd, we assign V1 (i, j) = d. d is a coefficient to be tuned on validation data.. Whenever we come across a parent p with a set of conjunction children cjt1 , cjt2 , · · · , cjtn , we find the
rightmost child gc1r of the leftmost child in conjunction cjt1 , and add d to each V1 (p, cjt1 ) and
V1 (cjt1 , gc1r ). The edges in conjunction that are
added additional d’s to are shown in blue in Figure
3.

of the problem is
L(g1 , g2 , g3 ; u) = s1 (g1 ) + s2 (g2 ) + s3 (g3 )
+u> (gm − y)

V2 . Different from V1 , for edges y(i, j) whose
label is tagged with *ldd, we assign an V2 (j, i) =
d. Then for each conjunction structure with
a parent p and a set of conjunction children
cjt1 , cjt2 , · · · , cjtn , we find the leftmost child
gcnl of the rightmost child in conjunction cjtn ,
and add d to each V2 (p, cjtn ) and V2 (cjtn , gcnl ).
The concerned edges in conjunction are shown in
green in Figure 3.

where u is the Lagrangian multiplier.
Then the dual is
L(u) =

1
= max(s1 (g1 ) + u> gm )
g1
3
1
+ max(s2 (g2 ) + u> gm )
g2
3
1
+ max(s3 (g3 ) + u> gm ) − u> y
g3
3

V3 . We do not assign d’s to the edges with tag
*ldd. For each conjunction with parent p and conjunction children cjt1 , cjt2 , · · · , cjtn , we add an
d to V3 (p, cjt1 ), V3 (p, cjt2 ), · · · , and V3 (p, cjtn ).
4.3

max L(g1 , g2 , g3 ; u)

g1 ,g2 ,g3

According
to
the
duality
principle,
maxg1 ,g2 ,g3 ;u minu L(g1 , g2 , g3 ) = minu L(u),
so we can find the optimal solution for the
problem if we can find minu L(u). However it
is very hard to compute L(u), not to mention
minu L(u). The challenge is that gm in the three
maximizations must be consistent.
The idea is to separate the overall maximization
into three maximization problems by approximation. We observe that g1 , g2 , and g3 are very close
to gm , so we can approximate L(u) by

Lagrangian Relaxation with
Approximation

As soon as we get three trees g1 , g2 and g3 , we get
three subgraphs t2g(g1 ), t2g(g2 ) and t2g(g3 ). As
is stated above, we want every edge in a graph y to
be covered by at least one subgraph, and we want
to maximize the sum of the edge weights of all
trees. Note that the inequality in the constrained
optimization problem above can be replaced by a
maximization, written as

L0 (u) =

max L(g1 , g2 , g3 ; u)

g1 ,g2 ,g3

1
= max(s1 (g1 ) + u> g1 )
g1
3
1
+ max(s2 (g2 ) + u> g2 )
g2
3
1
+ max(s3 (g3 ) + u> g3 ) − u> y
g3
3

max. s1 (g1 ) + s2 (g2 ) + s3 (g3 )
s.t.
g1 , g2 , g3 are trees
max{t2g(g1 )(i, j), t2g(g2 )(i, j),
t2g(g3 )(i, j)} = y(i, j), ∀i, j
P
where sk (gk ) = ωk (i, j)gk (i, j)
Let gm = max{t2g(g1 ), t2g(g2 ), t2g(g3 )},
and by max{g1 , g2 , g3 } we mean to take the maximum of three vectors pointwisely. The Lagrangian

In this case, the three maximization problem can
be decoded separately, and we can try to find the
optimal u using the subgradient method.
30

4.4

5.1

The Algorithm

Algorithm 1 is our tree decomposition algorithm.
In the algorithm, we use subgradient method to
find minu L0 (u) iteratively. In each iteration, we
first compute g1 , g2 , and g3 to find L0 (u), then
update u until the graph is covered by the subgraphs. The coefficient 13 ’s can be merged into
the steps α(k) , so we omit them. The three separate problems gk ← arg maxgk sk (gk ) + u> gk
(k = 1, 2, 3) can be solved using Eisner’s algorithm, similar to solving arg maxgk sk (gk ). Intuitively, the Lagrangian multiplier u in our Algorithm can be regarded as additional weights for
the score function. The update of u is to increase
weights to the edges that are not covered by any
tree-like subgraph, so that it will be more likely
for them to be selected in the next iteration.

5

Capturing the Hidden Consistency

In order to capture the hidden consistency, we add
consistency tags to the labels of the extracted trees
to represent the co-occurrence. The basic idea is
to use additional tag to encode the relationship of
the edges in the three trees. The tag set is T =
{0, 1, 2, 3, 4, 5, 6}. Given a tag t ∈ T , t&1, t&2,
t&4 denote whether the edge is contained in g1 ,
g2 , g3 respectively, where the operator “&” is the
bitwise AND operator. Specially, since we do not
need to consider first bit of the tags of edges in g1 ,
the second bit in g2 , and the third bit in g3 , we
always assign 0 to them. For example, if y(i, j) =
1, g1 (i, j) = 1, g2 (j, i) = 1, g3 (i, j) = 0 and
t3 (j, i) = 0, we tag g1 (i, j) as 2 and g2 (j, i) as 1.
When it comes to parsing, we also get labels
with consistency information. Our goal is to guarantee the tags in edges of the parse trees for a
same sentence are consistent while graph merging. Since the consistency tags emerge, for convenience we index the graph and tree vector representation using three indices. g(i, j, t) denotes
whether there is an edge from word wi to word
wj with tag t in graph g.
The joint decoding problem can be written as a
constrained optimization problem as

Graph Merging

The extraction algorithm gives three classes of
trees for each graph. We apply the algorithm to
the graph training set, and get three training tree
sets. After that, we can train three parsing models
with the three tree sets. In this work, the parser
we use to train models and parse trees is Mate
(Bohnet, 2010), a second-order graph-based dependency parser.
Let the scores the three models use be
f1 , f2 , f3 respectively. Then the parsers can
find trees with highest scores for a sentence.
That is solving the following optimization problems: arg maxg1 f1 (g1 ), arg maxg2 f2 (g2 ) and
arg maxg2 f3 (g3 ). We can parse a given sentence with the three models, obtain three trees,
and then transform them into subgraphs, and combine them together to obtain the graph parse of
the sentence by putting all the edges in the three
subgraphs together. That is to say, we obtain the
graph y = max{t2g(g1 ), t2g(g2 ), t2g(g3 )}. We
call this process simple merging.
However, the simple merging process omits
some consistency that the three trees extracted
from the same graph achieve, thus losing some
important information. The information is that
when we decompose a graph into three subgraphs,
some edges tend to appear in certain classes of
subgraphs at the same time. We want to retain
the co-occurrence relationship of the edges when
doing parsing and merging. To retain the hidden
consistency, we must do joint decoding instead of
decode the three models separately.

max. f1 (g1 ) + f2 (g2 ) + f3 (g3 )P
s.t.
g10 (i, j, 2) + g10 (i, j, 6) ≤ Pt g20 (i, j, t)
g10 (i, j, 4) + g10 (i, j, 6) ≤ Pt g30 (i, j, t)
g20 (i, j, 1) + g20 (i, j, 5) ≤ Pt g10 (i, j, t)
g20 (i, j, 4) + g20 (i, j, 5) ≤ Pt g30 (i, j, t)
g30 (i, j, 1) + g30 (i, j, 3) ≤ Pt g10 (i, j, t)
g30 (i, j, 2) + g30 (i, j, 3) ≤ t g20 (i, j, t)
∀i, j
where gk0 = t2g(gk )(k = 1, 2, 3).
The inequality constraints in the problem are the
consistency constraints. Each of them gives the
constraint between two classes of trees. For example, the first inequality says that an edge in g1 with
tag t&2 6= 0 exists only when the same edge in g2
exist. If all of these constraints are satisfied, the
subgraphs achieve the consistency.
5.2

Lagrangian Relaxation with
Approximation

To solve the constrained optimization problem
above, we do some transformations and then apply the Lagrangian Relaxation to it with approximation.
31

Let a12 (i, j) = g1 (i, j, 2) + g1 (i, j, 6), then the
first constraint can be written as an equity constraint
X
g2 (:, :, t))
g1 (:, :, 2) + g1 (:, :, 6) = a12 . ∗ (

Algorithm 2: The Joint Decoding Algorithm
Initialization: set u(0) , A1 , A2 , A3 to 0,
for k = 0 to K do
g1 ← arg maxg1 f1 (g1 ) + u(k)> A1 g1
g2 ← arg maxg2 f2 (g2 ) + u(k)> A2 g2
g3 ← arg maxg3 f3 (g3 ) + u(k)> A3 g3
update A1 , A2 , A3
if A1 g1 + A2 g2 + A3 g3 = 0 then
return g1 , g2 , g3
(k+1)
u
←
u(k) − α(k) (A1 g1 + A2 g2 + A3 g3 )
return g1 , g2 , g3

t

where “:” is to take out all the elements in the
corresponding dimension, and “.∗” is to do multiplication pointwisely. So can the other inequality
constraints. If we take a12 , a13 , · · · , a32 as constants, then all the constraints are linear. The constraints thus can be written as
A1 g1 + A2 g2 + A3 g3 = 0
where A1 , A2 , and A3 are matrices that can be
constructed from a12 , a13 , · · · , a32 .
The Lagrangian of the optimization problem is

+ max(f2 (g2 ) + u> A2 g2 )

of the edges in the three models. Specifically, let
wk = u> Ak , then we can modify the ωk in sk to
ωk0 , such that ωk0 (i, j, t) = ωk (i, j, t)+wk (i, j, t)+
wk (j, i, t).
The update of w1 , w2 , w3 can be understood
in an intuitive way. When one of the constraints
is not satisfied, without loss of generality, say,
the first one for edge y(i, j). We know g1 (i, j)
is tagged to represent that g2 (i, j) = 1, but it
is not the case. So we increase the weight of
that edge with all kinds of tags in g2 , and decrease the weight of the edge with tag representing
g2 (i, j) = 1 in g1 . After the update of the weights,
the consistency is more likely to be achieved.

+ max(f3 (g3 ) + u> A3 g3 )

5.4

L(g1 , g2 , g3 ; u) = f1 (g1 ) + f2 (g2 ) + f3 (g3 ) +
u> (A1 g1 + A2 g2 + A3 g3 )
where u is the Lagrangian multiplier. Then the
dual is
L(u) =

max L(g1 , g2 , g3 ; u)

g1 ,g2 ,g3

= max(f1 (g1 ) + u> A1 g1 )
g1

g2
g3

For sake of formal concision, we illustrate our algorithms omitting the labels. It is straightforward
to extend the algorithms to labeled parsing. In the
joint decoding algorithm, we just need to extend
the weights w1 , w2 , w3 for every label that appears in the three tree sets, and the algorithm can
be deduced similarly.

Again, we use the subgradient method to minimize L(u). During the deduction, we take
a12 , a13 , · · · , a32 as constants, but unfortunately
they are not. We propose an approximation for the
a’s in each iteration: Using the a’s we got in the
previous iteration instead. It is a reasonable approximation given that the u’s in two consecutive
iterations are similar and so are the a’s.
5.3

Labeled Parsing

6

The Algorithm

6.1

The pseudo code of our algorithm is shown in Algorithm 2. We know that the score functions f1 ,
f2 , and f3 each consist of first-order scores and
higher order scores. So they can be written as

Evaluation and Analysis
Experimental Setup

We conduct experiments on Chinese GRBank
(Sun et al., 2014), an LFG-style GR corpus for
Mandarin Chinese. Linguistically speaking, this
deep dependency annotation directly encodes information such as coordination, extraction, raising, control as well as many other long-range dependencies. The selection for training, development, test data is also according to Sun et al.
(2014)’s experiments. Gold standard POS-tags are
used for deriving features for disambiguation.

h
fk (g) = s1st
k (g) + sk (g)
P
where s1st
ωk (i, j)g(i, j) (k = 1, 2, 3).
k (g) =
With this property, each individual problem gk ←
arg maxgk fk (gk ) + u> Ak gk can be decoded easily, with modifications to the first order weights

32

SM

LR

subgraph1
subgraph2
subgraph3
Merged
subgraph1
subgraph2
subgraph3
Merged

UP
88.63
88.04
88.91
83.23
89.76
89.30
89.42
88.07

UR
76.19
78.20
81.12
88.45
77.48
79.18
81.55
85.14

UF
81.94
82.83
84.84
85.76
83.17
83.93
85.31
86.58

UCompl
18.09
17.47
20.36
22.97
18.60
18.66
20.53
26.32

LP
85.94
85.31
86.57
80.59
87.17
86.68
87.09
85.55

LR
73.88
75.77
78.99
85.64
75.25
76.85
79.43
82.70

LF
79.46
80.26
82.61
83.04
80.77
81.47
83.08
84.10

LCompl
16.11
15.43
17.30
19.29
16.39
16.56
17.81
21.61

Table 1: Results on development set. SM is for Simple Merging, and LR for Lagrangian Relaxation.
subgraph1
subgraph2
subgraph3
Merged
Sun et al.
Zhang et al.[Single]
Zhang et al.[Ensemble]

UP
89.80
89.34
89.57
88.06
-

UR
76.74
78.66
81.23
85.11
-

UF
82.76
83.66
85.19
86.56
-

UCompl
18.69
18.46
20.18
26.24
-

LP
87.81
87.26
87.78
86.03
83.93
82.28
84.92

LR
75.04
76.84
79.61
83.16
79.82
83.11
85.28

LF
80.93
81.72
83.49
84.57
81.82
82.69
85.10

LCompl
17.13
16.97
18.22
22.84
-

Table 2: Lagrangian Relaxation Results on test set.
The measure for comparing two dependency
graphs is precision/recall of GR tokens which are
defined as hwh , wd , li tuples, where wh is the head,
wd is the dependent and l is the relation. Labeled
precision/recall (LP/LR) is the ratio of tuples correctly identified by the automatic generator, while
unlabeled precision/recall (UP/UR) is the ratio regardless of l. F-score is a harmonic mean of precision and recall. These measures correspond to
attachment scores (LAS/UAS) in dependency tree
parsing. To evaluate our GR parsing models that
will be introduced later, we also report these metrics.
6.2

SD

LR

Coverage
subgraph1
subgraph2
subgraph3
Merged
subgraph1
subgraph2
subgraph3
Merged

Edge
85.52
88.42
90.40
96.93
85.66
88.48
90.67
99.55

Sentence
28.73
28.36
34.37
71.66
29.01
28.63
34.72
96.90

Table 3: Results of graph decomposition. SD is
for Simple Decomposition and LR for Lagrangian
Relaxation

Results of Graph Decomposition

tion, indicating that Lagrangian Relaxation is very
effective on the task of decomposition.

Table 3 shows the results of graph decomposition
on the training set. If we use simple decomposition, say, directly extracting three trees from a
graph, we get three subgraphs. On the training
set, each kind of the subgraphs cover around 90%
edges and 30% sentences. When we merge them
together, they cover nearly 97% edges and over
70% sentences. This indicates that the ability of
a single tree is limited and three trees can cover
most of the edges.
When we apply Lagrangian Relaxation to the
decomposition process, both the edge coverage
and the sentence coverage gain great error reduc-

6.3

Results of Graph Merging

Table 1 shows the results of graph merging on the
development set, and Table 2 on test set. The three
training sets of trees are from the decomposition
with Lagrangian Relaxation and the models are
trained from them. In both tables, simple merging
(SM) refers to first decode the three trees for a sentence then combine them by putting all the edges
together. As is shown, the merged graph achieves
higher f-score than other single models. With Lagrangian Relaxation, the performance of not only
33

Press Media Technology). We thank anonymous
reviewers for their valuable comments.

the merged graph but also the three subgraphs are
improved, due to capturing the consistency information.
When we do simple merging, though the recall
of each kind of subgraphs is much lower than the
precision of them, it is opposite of the merged
graph. This is because the consistency between
three models is not required and the models tend
to give diverse subgraph predictions. When we require the consistency between the three models,
the precision and recall become comparable, and
higher f-scores are achieved.
The best scores reported by previous work, i.e.
(Sun et al., 2014) and (Zhang et al., 2016) are
also listed in Table 2. We can see that our subgraphs already achieve competitive scores, and
the merged graph with Lagrangian Relaxation improves both unlabeled and labeled f-scores substantially, with an error reduction of 15.13% and
10.86%. We also include Zhang et al.’s parsing result obtained by an ensemble model that integrate
six different transition-based models. We can see
that parser ensemble is very helpful for deep dependency parsing and the accuracy of our graph
merging parser is sightly lower than this ensemble
model. Given that the architecture of graph merging is quite different from transition-based parsing, we think system combination of our parser
and the transition-based parser is promising.

7

References
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation for sembanking.
In Proceedings of the
7th Linguistic Annotation Workshop and Interoperability with Discourse. Association for Computational Linguistics, Sofia, Bulgaria, pages 178–186.
http://www.aclweb.org/anthology/W13-2322.
Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computational Linguistics (Coling 2010). Coling 2010 Organizing Committee, Beijing, China, pages 89–97.
http://www.aclweb.org/anthology/C10-1011.
Junjie Cao, Sheng Huang, Weiwei Sun, and Xiaojun Wan. 2017. Parsing to 1-endpoint-crossing,
pagenumber-2 graphs. In Proceedings of the 55th
Annual Meeting of the Association for Computational Linguistics. Association for Computational
Linguistics.
John Carroll and Ted Briscoe. 2002. High precision extraction of grammatical relations.
In
Proceedings of the 19th International Conference
on Computational Linguistics - Volume 1. Association for Computational Linguistics, Stroudsburg, PA, USA, COLING ’02, pages 1–7.
https://doi.org/10.3115/1072228.1072241.

Conclusion

Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica pages
14:1396–1400.

To construct complex linguistic graphs beyond
trees, we propose a new perspective, namely graph
merging. We take GR parsing as a case study and
exemplify the idea. There are two key problems
in this perspective, namely graph decomposition
and merging. To solve these two problems in a
principled way, we treat both problems as optimization problems and employ combinatorial optimization techniques. Experiments demonstrate
the effectiveness of the graph merging framework.
This framework can be adopted to other types of
flexible representations, e.g. semantic dependency
graphs (Oepen et al., 2014, 2015) and abstract
meaning representations (Banarescu et al., 2013).

Stephen Clark and James Curran. 2007a. Formalismindependent parser evaluation with CCG and DepBank. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. Association for Computational Linguistics, Prague, Czech Republic, pages 248–255.
http://www.aclweb.org/anthology/P07-1032.
Stephen Clark and James R. Curran. 2007b.
Wide-coverage
efficient
statistical
parsing with CCG and log-linear models.
Computational
Linguistics
33(4):493–552.
https://doi.org/10.1162/coli.2007.33.4.493.

Acknowledgments

J. Edmonds. 1967. Optimum branchings. Journal of
Research of the NationalBureau of Standards pages
71B:233–240.

This work was supported by 863 Program of China
(2015AA015403), NSFC (61331011), and Key
Laboratory of Science, Technology and Standard
in Press Industry (Key Laboratory of Intelligent

Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: an exploration. In Proceedings of the 16th conference on Computational linguistics - Volume 1. Association for Computational
Linguistics, Stroudsburg, PA, USA, pages 340–345.

34

Xun Zhang, Yantao Du, Weiwei Sun, and Xiaojun
Wan. 2016. Transition-based parsing for deep dependency structures. Computational Linguistics
42(3):353–389.
http://aclweb.org/anthology/J163001.

Ron Kaplan, Stefan Riezler, Tracy H King, John T
Maxwell III, Alex Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004:
Main Proceedings. Association for Computational
Linguistics, Boston, Massachusetts, USA, pages
97–104.
Marco Kuhlmann and Peter Jonsson. 2015. Parsing to
noncrossing dependency graphs. Transactions of the
Association for Computational Linguistics 3:559–
570.

Yusuke Miyao, Kenji Sagae, and Jun’ichi Tsujii. 2007.
Towards framework-independent
evaluation of deep linguistic parsers.
In Ann
Copestake, editor, Proceedings of the GEAF 2007
Workshop. CSLI Publications, CSLI Studies in
Computational Linguistics Online, pages 238–258.
http://www.cs.cmu.edu/ sagae/docs/geaf07miyaoetal.pdf.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Silvie Cinková, Dan Flickinger, Jan
Hajic, and Zdenka Uresová. 2015. Semeval 2015
task 18: Broad-coverage semantic dependency parsing. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015).
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Hajic, Angelina Ivanova, and Yi Zhang. 2014. Semeval 2014
task 8: Broad-coverage semantic dependency parsing. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014). Association for Computational Linguistics and Dublin
City University, Dublin, Ireland, pages 63–72.
http://www.aclweb.org/anthology/S14-2008.
Emily Pitler, Sampath Kannan, and Mitchell Marcus. 2013. Finding optimal 1-endpoint-crossing
trees. TACL 1:13–24. http://www.transacl.org/wpcontent/uploads/2013/03/paper13.pdf.
Weiwei Sun, Junjie Cao, and Xiaojun Wan. 2017. Semantic dependency parsing via book embedding. In
Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Association
for Computational Linguistics.
Weiwei Sun, Yantao Du, Xin Kou, Shuoyang Ding, and
Xiaojun Wan. 2014. Grammatical relations in Chinese: GB-ground extraction and data-driven parsing. In Proceedings of the 52nd Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Baltimore, Maryland, pages 446–
456. http://www.aclweb.org/anthology/P14-1042.
Naiwen Xue, Fei Xia, Fu-dong Chiou, and Marta
Palmer. 2005.
The penn Chinese treebank:
Phrase structure annotation of a large corpus.
Natural Language Engineering 11:207–238.
https://doi.org/10.1017/S135132490400364X.

35

