A Nonconvex Optimization Framework for Low Rank
Matrix Estimation⇤
Tuo Zhao
Johns Hopkins University

Zhaoran Wang
Han Liu
Princeton University

Abstract
We study the estimation of low rank matrices via nonconvex optimization. Compared with convex relaxation, nonconvex optimization exhibits superior empirical
performance for large scale instances of low rank matrix estimation. However, the
understanding of its theoretical guarantees are limited. In this paper, we define the
notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization. We illustrate the consequences
of this general framework for matrix sensing. In particular, we prove that a broad
class of nonconvex optimization algorithms, including alternating minimization
and gradient-type methods, geometrically converge to the global optimum and
exactly recover the true low rank matrices under standard conditions.

1

Introduction

Let M ⇤ 2 Rm⇥n be a rank k matrix with k much smaller than m and n. Our goal is to estimate
M ⇤ based on partial observations of its entires. For example, matrix sensing is based on linear
measurements hAi , M ⇤ i, where i 2 {1, . . . , d} with d much smaller than mn and Ai is the sensing
matrix. In the past decade, significant progress has been established on the recovery of low rank matrix
[4, 5, 23, 18, 15, 16, 12, 22, 7, 25, 19, 6, 14, 11, 13, 8, 9, 10, 27]. Among all these existing works, most
are based upon convex relaxation with nuclear norm constraint or regularization. Nevertheless, solving
these convex optimization problems can be computationally prohibitive in high dimensional regimes
with large m and n [27]. A computationally more efficient alternative is nonconvex optimization. In
particular, we reparameterize the m ⇥ n matrix variable M in the optimization problem as U V >
with U 2 Rm⇥k and V 2 Rn⇥k , and optimize over U and V . Such a reparametrization automatically
enforces the low rank structure and leads to low computational cost per iteration. Due to this reason,
the nonconvex approach is widely used in large scale applications such as recommendation systems
[17].
Despite the superior empirical performance of the nonconvex approach, the understanding of its
theoretical guarantees is relatively limited in comparison with the convex relaxation approach. Only
until recently has there been progress on coordinate descent-type nonconvex optimization methods,
which is known as alternating minimization [14, 8, 9, 10]. They show that, provided a desired
initialization, the alternating minimization algorithm converges at a geometric rate to U ⇤ 2 Rm⇥k
and V ⇤ 2 Rn⇥k , which satisfy M = U ⇤ V ⇤ > . Meanwhile, [15, 16] establish the convergence of
gradient-type methods, and [27] further establish the convergence of a broad class of nonconvex
algorithms including both gradient-type and coordinate descent-type methods. However, [15, 16, 27]
only establish the asymptotic convergence for an infinite number of iterations, rather than the explicit
rate of convergence. Besides these works, [18, 12, 13] consider projected gradient-type methods,
which optimize over the matrix variable M 2 Rm⇥n rather than U 2 Rm⇥k and V 2 Rn⇥k . These
methods involve calculating the top k singular vectors of an m ⇥ n matrix at each iteration. For
⇤
Research supported by NSF IIS1116730, NSF IIS1332109, NSF IIS1408910, NSF IIS1546482-BIGDATA,
NSF DMS1454377-CAREER, NIH R01GM083084, NIH R01HG06841, NIH R01MH102339, and FDA
HHSF223201000072C.

1

k much smaller than m and n, they incur much higher computational cost per iteration than the
aforementioned methods that optimize over U and V . All these works, except [27], focus on specific
algorithms, while [27] do not establish the explicit optimization rate of convergence.
In this paper, we propose a general framework that unifies a broad class of nonconvex algorithms
for low rank matrix estimation. At the core of this framework is a quantity named projected oracle
divergence, which sharply captures the evolution of generic optimization algorithms in the presence
of nonconvexity. Based on the projected oracle divergence, we establish sufficiently conditions under
which the iteration sequences geometrically converge to the global optima. For matrix sensing, a direct
consequence of this general framework is that, a broad family of nonconvex algorithms, including
gradient descent, coordinate gradient descent and coordinate descent, converge at a geometric rate
to the true low rank matrices U ⇤ and V ⇤ . In particular, our general framework covers alternating
minimization as a special case and recovers the results of [14, 8, 9, 10] under standard conditions.
Meanwhile, our framework covers gradient-type methods, which are also widely used in practice
[28, 24]. To the best of our knowledge, our framework is the first one that establishes exact recovery
guarantees and geometric rates of convergence for a broad family of nonconvex matrix sensing
algorithms.
To achieve maximum generality, our unified analytic framework significantly differs from previous
works. In detail, [14, 8, 9, 10] view alternating minimization as a perturbed version of the power
method. However, their point of view relies on the closed form solution of each iteration of alternating
minimization, which makes it hard to generalize to other algorithms, e.g., gradient-type methods.
Meanwhile, [27] take a geometric point of view. In detail, they show that the global optimum of the
optimization problem is the unique stationary point within its neighborhood and thus a broad class of
algorithms succeed. However, such geometric analysis of the objective function does not characterize
the convergence rate of specific algorithms towards the stationary point. Unlike existing analytic
frameworks, we analyze nonconvex optimization algorithms as perturbed versions of their convex
counterparts. For example, under our framework we view alternating minimization as a perturbed
version of coordinate descent on convex objective functions. We use the key quantity, projected oracle
divergence, to characterize such a perturbation effect, which results from the local nonconvexity
at intermediate solutions. This framework allows us to establish explicit rate of convergence in an
analogous way as existing convex optimization analysis.
P
Notation: For a vector v = (v1 , . . . , vd )T 2 Rd , let the vector `q norm be kvkqq = j vjq . For a
matrix A 2 Rm⇥n , we use A⇤j = (A1j , ..., Amj )> to denote the j-th column of A, and Ai⇤ =
(Ai1 , ..., Ain )> to denote the i-th row of A. Let max (A) and min (A) be theP
largest and smallest
nonzero singular values of A. We define the following matrix norms: kAk2F = j kA⇤j k22 , kAk2 =
singular values of A. Given another matrix
max (A). Moreover, we define kAk⇤ to be the sum of allP
B 2 Rm⇥n , we define the inner product as hA, Bi = i,j Aij Bij . We define ei as an indicator
vector, where the i-th entry is one, and all other entries are zero. For a bivariate function f (u, v), we
define ru f (u, v) to be the gradient with respect to u. Moreover, we use the common notations of
⌦(·), O(·), and o(·) to characterize the asymptotics of two real sequences.

2

Problem Formulation and Algorithms

Let M ⇤ 2 Rm⇥n be the unknown low rank matrix of interest. We have d sensing matrices Ai 2
Rm⇥n with i 2 {1, . . . , d}. Our goal is to estimate M ⇤ based on bi = hAi , M ⇤ i in the high
dimensional regime with d much smaller than mn. Under such a regime, a common assumption
is rank(M ⇤ ) = k ⌧ min{d, m, n}. Existing approaches generally recover M ⇤ by solving the
following convex optimization problem
min
kM k⇤ subject to b = A(M ),
(2.1)
m⇥n
M 2R

where b = [b1 , ..., bd ] 2 R , and A(M ) : Rm⇥n ! Rd is an operator defined as
A(M ) = [hA1 , M i, ..., hAi , M i]> 2 Rd .
(2.2)
Existing convex optimization algorithms for solving (2.1) are computationally inefficient, in the sense
that they incur high per-iteration computational cost, and only attain sublinear rates of convergence to
the global optimum [14]. Instead, in large scale settings we usually consider the following nonconvex
>

d

2

optimization problem
1
kb A(U V > )k22 .
(2.3)
2
The reparametrization of M = U V > , though making the optimization problem in (2.3) nonconvex,
significantly improves the computational efficiency. Existing literature [17, 28, 21, 24] has established
convincing empirical evidence that (2.3) can be effectively solved by a board variety of gradient-based
nonconvex optimization algorithms, including gradient descent, alternating exact minimization (i.e.,
alternating least squares or coordinate descent), as well as alternating gradient descent (i.e., coordinate
gradient descent), which are shown in Algorithm 1.
min

U 2Rm⇥k ,V

2Rn⇥k

F(U, V ).

where F(U, V ) =

It is worth noting the QR decomposition and rank k singular value decomposition in Algorithm
1 can be accomplished efficiently. In particular, the QR decomposition can be accomplished in
O(k 2 max{m, n}) operations, while the rank k singular value decomposition can be accomplished
in O(kmn) operations. In fact, the QR decomposition is not necessary for particular update schemes,
e.g., [14] prove that the alternating exact minimization update schemes with or without the QR
decomposition are equivalent.
Algorithm 1 A family of nonconvex optimization algorithms for matrix sensing. Here (U , D, V )
KSVD(M ) is the rank k singular value decomposition of M . Here D is a diagonal matrix containing
the top k singular values of M in decreasing order, and U and V contain the corresponding top k left
and right singular vectors of M . Here (V , RV )
QR(V ) is the QR decomposition, where V is the
corresponding orthonormal matrix and RV is the corresponding upper triangular matrix.
Input: {bi }di=1 , {Ai }di=1
Parameter: Step size ⌘, Total number of iterations T
P
(0)
(0)
(0)
(0)
(U , D(0) , V )
KSVD( di=1 bi Ai ), V (0)
V D(0) , U (0)
U D(0)
For: t = 0, ...., T 1
9
(t)
>
Alternating Exact Minimization : V (t+0.5)
argminV F (U , V )
>
>
>
(t+1)
(t+0.5)
>
(t+0.5)
>
(V
, RV
)
QR(V
)
>
>
>
(t)
=
(t+0.5)
(t)
(t)
Alternating Gradient Descent : V
V
⌘rV F (U , V )
Updating V
(t+1)
(t)
(t+0.5)
(t+0.5)>
>
(V
, RV
)
QR(V (t+0.5) ), U (t)
U RV
>
>
>
(t)
>
>
Gradient Descent : V (t+0.5)
V (t) ⌘rV F (U , V (t) )
>
>
>
(t+1)
(t) (t+0.5)>
(t+0.5)
;
(t+0.5)
(t+1)
(V
, RV
)
QR(V
), U
U RV
9
(t+1)
>
Alternating Exact Minimization : U (t+0.5)
argminU F (U, V
)
>
>
>
(t+1)
(t+0.5)
>
(t+0.5)
>
(U
, RU
)
QR(U
)
>
>
>
(t+1)
=
(t+0.5)
(t)
(t)
Alternating Gradient Descent : U
U
⌘rU F (U , V
)
Updating U
(t+1)
t+1 (t+0.5)>
(t+0.5)
(t+0.5)
(t+1)
>
(U
, RU
)
QR(U
), V
V
RU
>
>
>
(t)
>
>
Gradient Descent : U (t+0.5)
U (t) ⌘rU F (U (t) , V )
>
>
>
(t+1)
t (t+0.5)>
(t+0.5)
;
(t+0.5)
(t+1)
(U
, RU
)
QR(U
), V
V RU
End for
(T )>
(T )
Output: M (T )
U (T 0.5) V
(for gradient descent we use U V (T )> )

3

Theoretical Analysis

We analyze the convergence properties of the general family of nonconvex optimization algorithms
illustrated in §2. Before we present the main results, we first introduce a unified analytic framework
based on a key quantity named projected oracle divergence. Such a unified framework equips our
theory with the maximum generality. Without loss of generality, we assume m  n throughout the
rest of this paper.
3.1 Projected Oracle Divergence
We first provide an intuitive explanation for the success of nonconvex optimization algorithms, which
forms the basis of our later proof for the main results. Recall that (2.3) is a special instance of the
following optimization problem,
min
f (U, V ).
(3.1)
U 2Rm⇥k ,V 2Rn⇥k

A key observation is that, given fixed U , f (U, ·) is strongly convex and smooth in V under suitable
conditions, and the same also holds for U given fixed V correspondingly. For the convenience of
3

discussion, we summarize this observation in the following technical condition, which will be later
verified for matrix sensing under suitable conditions.
Condition 3.1 (Strong Biconvexity and Bismoothness). There exist universal constants µ+ > 0 and
µ > 0 such that
µ
µ+ 0
kU 0 U k2F  f (U 0 , V ) f (U, V ) hU 0 U, rU f (U, V )i 
kU
U k2F for all U, U 0 ,
2
2
µ
µ+ 0
kV 0 V k2F  f (U, V 0 ) f (U, V ) hV 0 V, rV f (U, V )i 
kV
V k2F for all V, V 0 .
2
2
For the simplicity of discussion, for now we assume U ⇤ and V ⇤ are the unique global minimizers to
the generic optimization problem in (3.1). Assuming U ⇤ is given, we can obtain V ⇤ by
V ⇤ = argmin f (U ⇤ , V ).
(3.2)
V 2Rn⇥k

Condition 3.1 implies the objective function in (3.2) is strongly convex and smooth. Hence, we can
choose any gradient-based algorithm to obtain V ⇤ . For example, we can directly solve for V ⇤ in
rV f (U ⇤ , V ) = 0,
(3.3)
⇤
or iteratively solve for V using gradient descent, i.e.,
V (t) = V (t 1) ⌘rV f (U ⇤ , V (t 1) ),
(3.4)
where ⌘ is the step size. For the simplicity of discussion, we put aside the renormalization issue for
now. In the example of gradient descent, by invoking classical convex optimization results [20], it is
easy to prove that
kV (t) V ⇤ kF  kV (t 1) V ⇤ kF for all t = 0, 1, 2, . . . ,
where  2 (0, 1) is a contraction coefficient, which depends on µ+ and µ in Condition 3.1.
However, the first-order oracle rV f (U ⇤ , V (t 1) ) is not accessible in practice, since we do not know
U ⇤ . Instead, we only have access to rV f (U, V (t 1) ), where U is arbitrary. To characterize the
divergence between the ideal first-order oracle rV f (U ⇤ , V (t 1) ) and the accessible first-order oracle
rV f (U, V (t 1) ), we define a key quantity named projected oracle divergence, which takes the form
⌦
↵
D(V, V 0 , U ) = rV f (U ⇤ , V 0 ) rV f (U, V 0 ), V V ⇤ /(kV V ⇤ kF ) ,
(3.5)
where V 0 is the point for evaluating the gradient. In the above example, it holds for V 0 = V (t 1) .
Later we will illustrate that, the projection of the difference of first-order oracles onto a specific one
dimensional space, i.e., the direction of V V ⇤ , is critical to our analysis. In the above example of
gradient descent, we will prove later that for V (t) = V (t 1) ⌘rV f (U, V (t 1) ), we have
kV (t) V ⇤ kF  kV (t 1) V ⇤ kF + 2/µ+ · D(V (t) , V (t 1) , U ).
(3.6)
In other words, the projection of the divergence of first-order oracles onto the direction of V (t) V ⇤
captures the perturbation effect of employing the accessible first-order oracle rV f (U, V (t 1) ) instead
of the ideal rV f (U ⇤ , V (t 1) ). For V (t+1) = argminV f (U, V ), we will prove that
kV (t) V ⇤ kF  1/µ · D(V (t) , V (t) , U ).
(3.7)
According to the update schemes shown in Algorithm 1, for alternating exact minimization, we set
U = U (t) in (3.7), while for gradient descent or alternating gradient descent, we set U = U (t 1) or
U = U (t) in (3.6) respectively. Correspondingly, similar results hold for kU (t) U ⇤ kF .
To establish the geometric rate of convergence towards the global minima U ⇤ and V ⇤ , it remains to
establish upper bounds for the projected oracle divergence. In the example of gradient decent we will
prove that for some ↵ 2 (0, 1 ),
2/µ+ · D(V (t) , V (t 1) , U (t 1) )  ↵kU (t 1) U ⇤ kF ,

which together with (3.6) (where we take U = U (t 1) ) implies
kV (t) V ⇤ kF  kV (t 1) V ⇤ kF + ↵kU (t

1)

U ⇤ kF .

Correspondingly, similar results hold for kU
U kF , i.e.,
kU (t) U ⇤ kF  kU (t 1) U ⇤ kF + ↵kV (t 1) V ⇤ kF .
Combining (3.8) and (3.9) we then establish the contraction
max{kV (t) V ⇤ kF , kU (t) U ⇤ kF }  (↵ + ) · max{kV (t 1) V ⇤ kF , kU (t
⇤

(t)

4

(3.8)
(3.9)
1)

U ⇤ kF },

which further implies the geometric convergence, since ↵ 2 (0, 1 ). Respectively, we can establish
similar results for alternating exact minimization and alternating gradient descent. Based upon such a
unified analytic framework, we now simultaneously establish the main results.
Remark 3.2. Our proposed projected oracle divergence is inspired by previous work [3, 2, 1],
which analyzes the Wirtinger Flow algorithm for phase retrieval, the expectation maximization (EM)
Algorithm for latent variable models, and the gradient descent algorithm for sparse coding. Though
their analysis exploits similar nonconvex structures, they work on completely different problems, and
the delivered technical results are also fundamentally different.
3.2 Matrix Sensing
Before we present our main results, we first introduce an assumption known as the restricted isometry
property (RIP). Recall that k is the rank of the target low rank matrix M ⇤ .
Assumption 3.3. The linear operator A(·) : Rm⇥n ! Rd defined in (2.2) satisfies 2k-RIP with
parameter 2k 2 (0, 1), i.e., for all 2 Rm⇥n such that rank( )  2k, it holds that
2
2
2
(1
2k )k kF  kA( )k2  (1 + 2k )k kF .
Several random matrix ensembles satisfy k-RIP for a sufficiently large d with high probability. For
example, suppose that each entry of Ai is independently drawn from a sub-Gaussian distribution,
A(·) satisfies 2k-RIP with parameter 2k with high probability for d = ⌦( 2k2 kn log n).

The following theorem establishes the geometric rate of convergence of the nonconvex optimization
algorithms summarized in Algorithm 1.
Theorem 3.4. Assume there exists a sufficiently small constant C1 such that A(·) satisfies 2k-RIP
with 2k  C1 /k, and the largest and smallest nonzero singular values of M ⇤ are constants, which do
not scale with (d, m, n, k). For any pre-specified precision ✏, there exist an ⌘ and universal constants
C2 and C3 such that for all T C2 log(C3 /✏), we have kM (T ) M ⇤ kF  ✏.

The proof of Theorems 3.4 is provided in Appendices 4.1, A.1, and A.2. Theorem 3.4 implies that all
three nonconvex optimization algorithms geometrically converge to the global optimum. Moreover,
assuming that each entry of Ai is independently drawn from a sub-Gaussian distribution with mean
zero and variance proxy one, our result further suggests, to achieve exact low rank matrix recovery,
our algorithm requires the number of measurements d to satisfy
d = ⌦(k 3 n log n),
(3.10)
since we assume that 2k  C1 /k. This sample complexity result matches the state-of-the-art result
for nonconvex optimization methods, which is established by [14]. In comparison with their result,
which only covers the alternating exact minimization algorithm, our results holds for a broader variety
of nonconvex optimization algorithms.
Note that the sample complexity in (3.10) depends on a polynomial of max (M ⇤ )/ min (M ⇤ ), which
is treated as a constant in our paper. If we allow max (M ⇤ )/ min (M ⇤ ) to increase with the dimension,
we can plug the nonconvex optimization algorithms into the multi-stage framework proposed by [14].
Following similar lines to the proof of Theorem 3.4, we can derive a new sample complexity, which
is independent of max (M ⇤ )/ min (M ⇤ ). See more details in [14].

4

Proof of Main Results

Due to space limitation, we only sketch the proof of Theorem 3.4 for alternating exact minimization.
The proof of Theorem 3.4 for alternating gradient descent and gradient descent, and related lemmas
are provided in the appendix. For notational simplicity, let 1 = max (M ⇤ ) and k = min (M ⇤ ).
Before we proceed with the main proof, we first introduce the following lemma, which verifies
Condition 3.1.
Lemma 4.1. Suppose that A(·) satisfies 2k-RIP with parameter 2k . Given an arbitrary orthonormal
matrix U 2 Rm⇥k , for any V, V 0 2 Rn⇥k , we have
1 + 2k 0
1
2k
kV
V k2F F(U , V 0 ) F(U , V ) hrV F(U , V ), V 0 V i
kV 0 V k2F .
2
2
The proof of Lemma 4.1 is provided in Appendix B.1. Lemma 4.1 implies that F(U , ·) is strongly
convex and smooth in V given a fixed orthonormal matrix U , as specified in Condition 3.1. Equipped
with Lemma 4.1, we now lay out the proof for each update scheme in Algorithm 1.
5

4.1 Proof of Theorem 3.4 (Alternating Exact Minimization)
Proof. Throughout the proof of alternating exact minimization, we define a constant ⇠ 2 (1, 1)
for notational simplicity. We assume that at the t-th iteration, there exists a matrix factorization of
⇤(t)
⇤(t)
M ⇤ = U V ⇤(t)> , where U
is orthonormal. We choose the projected oracle divergence as
⌧
V (t+0.5) V ⇤(t)
(t)
⇤(t)
(t)
D(V (t+0.5) , V (t+0.5) , U )= rV F(U , V (t+0.5) ) rV F(U , V (t+0.5) ), (t+0.5)
.
kV
V ⇤(t) kF
Remark 4.2. Note that the matrix factorization is not necessarily unique. Because given a factorizae Ve > , where U
e = U O and
tion of M ⇤ = U V > , we can always obtain a new factorization of M ⇤ = U
k⇥k
Ve = V O for an arbitrary unitary matrix O 2 R
. However, this is not a issue to our convergence
analysis. As will be shown later, we can prove that there always exists a factorization of M ⇤ satisfying
the desired computational properties for each iteration (See Lemma 4.5, Corollaries 4.7 and 4.8).
The following lemma establishes an upper bound for the projected oracle divergence.
(t)

Lemma 4.3. Suppose that 2k and U satisfy
p
2
2(1
(1
(t)
⇤(t)
2k ) k
2k ) k
and kU
U kF 
.
2k 
4⇠k(1 + 2k ) 1
4⇠(1 + 2k ) 1
(1
(t)
(t)
⇤(t)
2k ) k
Then we have D(V (t+0.5) , V (t+0.5) , U ) 
kU
U kF .
2⇠

(4.1)

The proof of Lemma 4.3 is provided in Appendix B.2. Lemma 4.3 shows that the projected oracle di(t)
vergence for updating V diminishes with the estimation error of U .The following lemma quantifies
the progress of an exact minimization step using the projected oracle divergence.
Lemma 4.4. We have kV (t+0.5)

V ⇤(t) kF  1/(1

2k )

· D(V (t+0.5) , V (t+0.5) , U

(t)

).

The proof of Lemma 4.4 is provided in Appendix B.3. Lemma 4.4 illustrates that the estimation error
of V (t+0.5) diminishes with the projected oracle divergence. The following lemma characterizes the
effect of the renormalization step using QR decomposition.
Lemma 4.5. Suppose that V (t+0.5) satisfies
kV (t+0.5)

V ⇤(t) kF 

(4.2)

k /4.
⇤(t+1)

Then there exists a factorization of M ⇤ = U ⇤(t+1) V
such that V
(t+1)
⇤(t+1)
orthonormal matrix, and satisfies kV
V
kF  2/ k · kV (t+0.5)

⇤(t+0.5)

V

⇤(t)

2 Rn⇥k is an
kF .

The proof of Lemma 4.5 is provided in Appendix B.4. The next lemma quantifies the accuracy of the
(0)
initialization U .
Lemma 4.6. Suppose that

2k

satisfies
2k

2 4
(1
2k ) k
2
192⇠ k(1 + 2k )2



(4.3)

4.
1

⇤(0)

Then there exists a factorization of M ⇤ = U
V ⇤(0)> such that U
(0)
⇤
(1 2k ) k
matrix, and satisfies kU
U kF  4⇠(1+ 2k ) 1 .

⇤(0)

2 Rm⇥k is an orthonormal

The proof of Lemma 4.6 is provided in Appendix B.5. Lemma 4.6 implies that the initial solution
(0)
U attains a sufficiently small estimation error.
Combining the above Lemmas, we obtain the next corollary for a complete iteration of updating V .
(t)

Corollary 4.7. Suppose that 2k and U satisfy
2 4
(1
(t)
2k ) k
and kU
2k 
192⇠ 2 k(1 + 2k )2 14
We then have kV
(t)
1
⇠ kU

U

⇤(t)

(t+1)

V

kF and kV

⇤(t+1)

kF 

(t+0.5)

V

(1 2k ) k
4⇠(1+ 2k ) 1 .
⇤(t)

kF 

6

⇤(t)

kF 

(1
2k ) k
.
4⇠(1 + 2k ) 1

Moreover, we also have kV

2⇠ kU
k

U

(t)

U

⇤(t)

kF .

(t+1)

V

(4.4)
⇤(t+1)

kF 

The proof of Corollary 4.7 is provided in Appendix B.6. Since the alternating exact minimization
algorithm updates U and V in a symmetric manner, we can establish similar results for a complete
iteration of updating U in the next corollary.
(t+1)

Corollary 4.8. Suppose that 2k and V
satisfy
2 4
(1
(t+1)
2k ) k
and kV
2k 
192⇠ 2 k(1 + 2k )2 14

V

⇤(t+1)

⇤(t+1)

kF 

(1
2k ) k
.
4⇠(1 + 2k ) 1

(4.5)

⇤(t+1)

Then there exists a factorization of M ⇤ = U
V ⇤(t+1)> such U
is an orthonormal matrix,
(t+1)
⇤(t+1)
(t+1)
⇤(t+1)
(1 2k ) k
and satisfies kU
U
kF  4⇠(1+ 2k ) 1 . Moreover, we also have kU
U
kF 
(t+1)
1
⇠ kV

V

⇤(t+1)

kF and kU (t+0.5)

U ⇤(t+1) kF 

2⇠ kV
k

(t+1)

V

⇤(t+1)

kF .

The proof of Corollary 4.8 directly follows Appendix B.6, and is therefore omitted.
We then proceed with the proof of Theorem 3.4 for alternating exact minimization. Lemma 4.6
(0)
ensures that (4.4) of Corollary 4.7 holds for U . Then Corollary 4.7 ensures that (4.5) of Corollary
(1)
4.8 holds for V . By induction, Corollaries 4.7 and 4.8 can be applied recursively for all T iterations.
Thus we obtain
1 (T 1)
1
(T )
⇤(T )
⇤(T 1)
(T 1)
⇤(T 1)
kV
V
kF  kU
U
kF  2 kV
V
kF
⇠
⇠
1
(1
(0)
⇤(0)
2k ) k
 · · ·  2T 1 kU
U
kF  2T
,
(4.6)
⇠
4⇠ (1 + 2k ) 1
where the lastlinequality⇣ comes from⌘Lemma 4.6.
m Therefore, for a pre-specified accuracy ✏, we need
(1 2k ) k
1
at most T = 1/2 · log 2✏(1+ 2k ) 1 log ⇠ iterations such that
kV

(T )

V

⇤(T )

kF 

Moreover, Corollary 4.8 implies
kU (T

U ⇤(T ) kF 

0.5)

k

kV

(1
2k ) k
4⇠ 2T (1 + 2k )

(T )

V

⇤(T )

kF 

1



✏
.
2

(1

(4.7)

2
2k ) k

8⇠ 2T +1 (1

2⇠
+
where the last inequality comes from (4.6). Therefore, we need at most
⇠
✓
◆
⇡
2
(1
2k ) k
T = 1/2 · log
log 1 ⇠
4⇠✏(1 + 2k )
iterations such that
2
(1
✏
2k ) k
kU (T 0.5) U ⇤ kF  2T +1

.
8⇠
(1 + 2k ) 1
2 1
Then combining (4.7) and (4.8), we obtain
kM (T )

M ⇤ k = kU (T
 kV

(T )

0.5)

(T )>

V

k2 kU

(T

where the last inequality is from kV
1

5

(since U ⇤(T ) V

⇤(T )>

0.5)

(T )

= M ⇤ and V

U ⇤(T ) V
U

k2 =
⇤(T )

⇤(T )

⇤(T )>

kF

kF + kU ⇤(T ) k2 kV

1 (since V

(T )

(T )

V

2k ) 1

,

(4.8)

⇤(T )

kF  ✏,

(4.9)

is orthonormal) and kU ⇤ k2 = kM ⇤ k2 =

is orthonormal). Thus we complete the proof.

Extension to Matrix Completion

Under the same setting as matrix sensing, we observe a subset of the entries of M ⇤ , namely, W ✓
⇤
{1, . . . , m} ⇥ {1, . . . , n}. We assume that W is drawn uniformly at random, i.e., Mi,j
is observed
⇤
independently with probability ⇢¯ 2 (0, 1]. To exactly recover M , a common assumption is the
incoherence of M ⇤ , which will be specified later. A popular approach for recovering M ⇤ is to solve
the following convex optimization problem
min
kM k⇤ subject to PW (M ⇤ ) = PW (M ),
(5.1)
m⇥n
M 2R

where PW (M ) : Rm⇥n ! Rm⇥n is an operator defined as [PW (M )]ij = Mij if (i, j) 2 W, and
0 otherwise. Similar to matrix sensing, existing algorithms for solving (5.1) are computationally
7

inefficient. Hence, in practice we usually consider the following nonconvex optimization problem
min
FW (U, V ), where FW (U, V ) = 1/2 · kPW (M ⇤ ) PW (U V > )k2F . (5.2)
U 2Rm⇥k ,V 2Rn⇥k

Similar to matrix sensing, (5.2) can also be efficiently solved by gradient-based algorithms. Due to
space limitation, we present these matrix completion algorithms in Algorithm 2 of Appendix D. For
the convenience of later convergence analysis, we partition the observation set W into 2T + 1 subsets
W0 ,...,W2T using Algorithm 4 in Appendix D. However, in practice we do not need the partition
scheme, i.e., we simply set W0 = · · · = W2T = W.

Before we present the main results, we introduce an assumption known as the incoherence property.
Assumption 5.1. The target rank k matrix M ⇤ is incoherent with parameter µ, i.e., given the rank k
⇤
⇤>
singular value decomposition of M ⇤ = U ⌃⇤ V , we have
p
p
⇤
⇤
max kU i⇤ k2  µ k/m and max kV j⇤ k2  µ k/n.
i

j

The incoherence assumption guarantees that M ⇤ is far from a sparse matrix, which makes it feasible
to complete M ⇤ when its entries are missing uniformly at random. The following theorem establishes
the iteration complexity and the estimation error under the Frobenius norm.
Theorem 5.2. Suppose that there exists a universal constant C4 such that ⇢¯ satisfies
⇢¯ C4 µ2 k 3 log n log(1/✏)/m,
(5.3)
where ✏ is the pre-specified precision. Then there exist an ⌘ and universal constants C5 and C6 such
that for any T C5 log(C6 /✏), we have kM (T ) M kF  ✏ with high probability.
Due to space limit, we defer the proof of Theorem 5.2 to the longer version of this paper. Theorem
5.2 implies that all three nonconvex optimization algorithms converge to the global optimum at a
geometric rate. Furthermore, our results indicate that the completion of the true low rank matrix M ⇤
up to ✏-accuracy requires the entry observation probability ⇢¯ to satisfy
⇢¯ = ⌦(µ2 k 3 log n log(1/✏)/m).
(5.4)
This result matches the result established by [8], which is the state-of-the-art result for alternating
minimization. Moreover, our analysis covers three nonconvex optimization algorithms.

6

Experiments

Estimation Error

Estimation Error

We present numerical experiments for matrix sensing to support our theoretical analysis. We choose
m = 30, n = 40, and k = 5, and vary d from 300 to 900. Each entry of Ai ’s are independent sampled
e 2 Rm⇥k and Ve 2 Rn⇥k are two matrices
from N (0, 1). We then generate M = U V > , where U
with all their entries independently sampled from N (0, 1/k). We then generate d measurements by
bi = hAi , M i for i = 1, ..., d. Figure 1 illustrates the empirical performance of the alternating exact
minimization and alternating gradient descent algorithms for a single realization. The step size for the
alternating gradient descent algorithm is determined by the backtracking line search procedure. We
see that both algorithms attain linear rate of convergence for d = 600 and d = 900. Both algorithms
fail for d = 300, because d = 300 is below the minimum requirement of sample complexity for the
exact matrix recovery.
10 0

10

d = 300
d = 600
d = 900

-5

0

10

20

30

40

10 0

10

d = 300
d = 600
d = 900

-5

0

Number of Iterations

10

20

30

40

Number of Iterations

(a) Alternating Exact Minimization

(b) Alternating Gradient Descent

Figure 1: Two illustrative examples for matrix sensing. The vertical axis corresponds to estimation
error kM (t) M kF . The horizontal axis corresponds to numbers of iterations. Both the alternating
exact minimization and alternating gradient descent algorithms attain linear rate of convergence for
d = 600 and d = 900. But both algorithms fail for d = 300, because d = 300 is below the minimum
requirement of sample complexity for the exact matrix recovery.
8

References
[1] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse
coding. arXiv preprint arXiv:1503.00778, 2015.
[2] Sivaraman Balakrishnan, Martin J Wainwright, and Bin Yu. Statistical guarantees for the EM algorithm:
From population to sample-based analysis. arXiv preprint arXiv:1408.2156, 2014.
[3] Emmanuel J Candès, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger flow: Theory
and algorithms. IEEE Transactions on Information Theory, 61(4):1985–2007, 2015.
[4] Emmanuel J Candès and Benjamin Recht. Exact matrix completion via convex optimization. Foundations
of Computational Mathematics, 9(6):717–772, 2009.
[5] Emmanuel J Candès and Terence Tao. The power of convex relaxation: Near-optimal matrix completion.
IEEE Transactions on Information Theory, 56(5):2053–2080, 2010.
[6] Yudong Chen. Incoherence-optimal matrix completion. arXiv preprint arXiv:1310.0154, 2013.
[7] David Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions on
Information Theory, 57(3):1548–1566, 2011.
[8] Moritz Hardt. Understanding alternating minimization for matrix completion. In Symposium on Foundations of Computer Science, pages 651–660, 2014.
[9] Moritz Hardt, Raghu Meka, Prasad Raghavendra, and Benjamin Weitz. Computational limits for matrix
completion. arXiv preprint arXiv:1402.2331, 2014.
[10] Moritz Hardt and Mary Wootters. Fast matrix completion without the condition number. arXiv preprint
arXiv:1407.4070, 2014.
[11] Trevor Hastie, Rahul Mazumder, Jason Lee, and Reza Zadeh. Matrix completion and low-rank SVD via
fast alternating least squares. arXiv preprint arXiv:1410.2596, 2014.
[12] Prateek Jain, Raghu Meka, and Inderjit S Dhillon. Guaranteed rank minimization via singular value
projection. In Advances in Neural Information Processing Systems, pages 937–945, 2010.
[13] Prateek Jain and Praneeth Netrapalli. Fast exact matrix completion with finite samples. arXiv preprint
arXiv:1411.1087, 2014.
[14] Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating
minimization. In Symposium on Theory of Computing, pages 665–674, 2013.
[15] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries.
IEEE Transactions on Information Theory, 56(6):2980–2998, 2010.
[16] Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy entries.
Journal of Machine Learning Research, 11:2057–2078, 2010.
[17] Yehuda Koren. The Bellkor solution to the Netflix grand prize. Netflix Prize Documentation, 81, 2009.
[18] Kiryung Lee and Yoram Bresler. Admira: Atomic decomposition for minimum rank approximation. IEEE
Transactions on Information Theory, 56(9):4402–4416, 2010.
[19] Sahand Negahban and Martin J Wainwright. Estimation of (near) low-rank matrices with noise and
high-dimensional scaling. The Annals of Statistics, 39(2):1069–1097, 2011.
[20] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer, 2004.
[21] Arkadiusz Paterek. Improving regularized singular value decomposition for collaborative filtering. In
Proceedings of KDD Cup and workshop, volume 2007, pages 5–8, 2007.
[22] Benjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research,
12:3413–3430, 2011.
[23] Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM Review, 52(3):471–501, 2010.
[24] Benjamin Recht and Christopher Ré. Parallel stochastic gradient algorithms for large-scale matrix completion. Mathematical Programming Computation, 5(2):201–226, 2013.
[25] Angelika Rohde and Alexandre B Tsybakov. Estimation of high-dimensional low-rank matrices. The
Annals of Statistics, 39(2):887–930, 2011.
[26] Gilbert W Stewart, Ji-guang Sun, and Harcourt B Jovanovich. Matrix perturbation theory, volume 175.
Academic press New York, 1990.
[27] Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via non-convex factorization. arXiv preprint
arXiv:1411.8003, 2014.
[28] Gábor Takács, István Pilászy, Bottyán Németh, and Domonkos Tikk. Major components of the gravity
recommendation system. ACM SIGKDD Explorations Newsletter, 9(2):80–83, 2007.

9

