Eigenvalue Decay Implies Polynomial-Time
Learnability for Neural Networks

Surbhi Goel ∗
Department of Computer Science
University of Texas at Austin
surbhi@cs.utexas.edu

Adam Klivans †
Department of Computer Science
University of Texas at Austin
klivans@cs.utexas.edu

Abstract
We consider the problem of learning function classes computed by neural networks with various activations (e.g. ReLU or Sigmoid), a task believed to be computationally intractable in the worst-case. A major open problem is to understand
the minimal assumptions under which these classes admit provably efficient algorithms. In this work we show that a natural distributional assumption corresponding to eigenvalue decay of the Gram matrix yields polynomial-time algorithms in
the non-realizable setting for expressive classes of networks (e.g. feed-forward
networks of ReLUs). We make no assumptions on the structure of the network or
the labels. Given sufficiently-strong eigenvalue decay, we obtain fully-polynomial
time algorithms in all the relevant parameters with respect to square-loss. This is
the first purely distributional assumption that leads to polynomial-time algorithms
for networks of ReLUs. Further, unlike prior distributional assumptions (e.g., the
marginal distribution is Gaussian), eigenvalue decay has been observed in practice
on common data sets.

1

Introduction

Understanding the computational complexity of learning neural networks from random examples
is a fundamental problem in machine learning. Several researchers have proved results showing
computational hardness for the worst-case complexity of learning various networks– that is, when
no assumptions are made on the underlying distribution or the structure of the network [10, 16,
21, 26, 43]. As such, it seems necessary to take some assumptions in order to develop efficient
algorithms for learning deep networks (the most expressive class of networks known to be learnable
in polynomial-time without any assumptions is a sum of one hidden layer of sigmoids [16]). A
major open question is to understand what are the “correct” or minimal assumptions to take in
order to guarantee efficient learnability3 . An oft-taken assumption is that the marginal distribution is
equal to some smooth distribution such as a multivariate Gaussian. Even under such a distributional
assumption, however, there is evidence that fully polynomial-time algorithms are still hard to obtain
for simple classes of networks [19, 36]. As such, several authors have made further assumptions on
the underlying structure of the model (and/or work in the noiseless or realizable setting).
In fact, in an interesting recent work, Shamir [34] has given evidence that both distributional assumptions and assumptions on the network structure are necessary for efficient learnability using
gradient-based methods. Our main result is that under only an assumption on the marginal distribution, namely eigenvalue decay of the Gram matrix, there exist efficient algorithms for learning broad
∗

Work supported by a Microsoft Data Science Initiative Award.
Part of this work was done while visiting the Simons Institute for Theoretical Computer Science.
3
For example, a very recent paper of Song, Vempala, Xie, and Williams [36] asks “What form would such
an explanation take, in the face of existing complexity-theoretic lower bounds?”
†

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.

classes of neural networks even in the non-realizable (agnostic) setting with respect to square loss.
Furthermore, eigenvalue decay has been observed often in real-world data sets, unlike distributional
assumptions that take the marginal to be unimodal or Gaussian. As one would expect, stronger assumptions on the eigenvalue decay result in polynomial learnability for broader classes of networks,
but even mild eigenvalue decay will result in savings in runtime and sample complexity.
The relationship between our assumption on eigenvalue decay and prior assumptions on the
marginal distribution being Gaussian is similar in spirit to the dichotomy between the complexity of
certain algorithmic problems on power-law graphs versus Erdős-Rényi graphs. Several important
graph problems such as clique-finding become much easier when the underlying model is a
random graph with appropriate power-law decay (as opposed to assuming the graph is generated
from the classical G(n, p) model) [6, 22]. In this work we prove that neural network learning
problems become tractable when the underlying distribution induces an empirical gram matrix with
sufficiently strong eigenvalue-decay.
Our Contributions. Our main result is quite general and holds for any function class that can
be suitably embedded in an RKHS (Reproducing Kernel Hilbert Space) with corresponding kernel
function k (we refer readers unfamiliar with kernel methods to [30]). Given m draws from a distribution (x1 , . . . , xm ) and kernel k, recall that the Gram matrix K is an m × m matrix where the i, j
entry equals k(xi , xj ). For ease of presentation, we begin with an informal statement of our main
result that highlights the relationship between the eigenvalue decay assumption and the run-time and
sample complexity of our final algorithm.
Theorem 1 (Informal). Fix function class C and kernel function k. Assume C is approximated in the
corresponding RKHS with norm bound B. After drawing m samples, let K/m be the (normalized)
m × m Gram matrix with eigenvalues {λ1 , . . . , λm }. For error parameter  > 0,
1. If, for sufficiently large i, λi ≈ O(i−p ), then C is efficiently learnable with m = Õ(B 1/p /2+3/p ).
2. If, for sufficiently large i, λi ≈ O(e−i ), then C is efficiently learnable with m = Õ(log B/2 ).
We allow a failure probability for the event that the eigenvalues do not decay. In all prior work,
the sample complexity m depends linearly on B, and for many interesting concept classes (such as
ReLUs), B is exponential in one or more relevant parameters. Given Theorem 1, we can use known
structural results for embedding neural networks into an RKHS to estimate B and take a corresponding eigenvalue decay assumption to obtain polynomial-time learnability. Applying bounds recently
obtained by Goel et al. [16] we have
Corollary 2. Let C be the class of all fully-connected networks of ReLUs with one-hidden layer
of ` hidden ReLU activations feeding into a single ReLU output activation (i.e., two hidden layers
or depth-3). Then, assuming eigenvalue decay of O(i−`/ ), C is learnable in polynomial time with
respect
to square
loss on Sn−1 . If ReLU is replaced with sigmoid, then we require eigenvalue decay
√
√
− ` log( `/)
O(i
).
For higher depth networks, bounds on the required eigenvalue decay can be derived from structural results in [16]. Without taking an assumption, the fastest known algorithms for learning the
above networks run in time exponential in the number of hidden units and accuracy parameter (but
polynomial in the dimension) [16].
Our proof develops a novel approach for bounding the generalization error of kernel methods,
namely we develop compression schemes tailor-made for classifiers induced by kernel-based regression, as opposed to current Rademacher-complexity based approaches. Roughly, a compression
scheme is a mapping from a training set S to a small subsample S 0 and side-information I. Given
this compressed version of S, the decompression algorithm should be able to generate a classifier h.
In recent work, David, Moran and Yehudayoff [13] have observed that if the size of the compression
is much less than m (the number of samples), then the empirical error of h on S is close to its true
error with high probability.
At the core of our compression scheme is a method for giving small description length (i.e., o(m)
bit complexity), approximate solutions to instances of kernel ridge regression. Even though we
assume K has decaying eigenvalues, K is neither sparse nor low-rank, and even a single column
or row of K has bit complexity at least m, since K is an m × m matrix! Nevertheless, we can
prove that recent tools from Nyström sampling [28] imply a type of sparsification for solutions
2

of certain regression problems involving K. Additionally, using preconditioning, we can bound
the bit complexity of these solutions and obtain the desired compression scheme. At each stage
we must ensure that our compressed solutions do not lose too much accuracy, and this involves
carefully analyzing various matrix approximations. Our methods are the first compression-based
generalization bounds for kernelized regression.
Related Work. Kernel methods [30] such as SVM, kernel ridge regression and kernel PCA have
been extensively studied due to their excellent performance and strong theoretical properties. For
large data sets, however, many kernel methods become computationally expensive. The literature
on approximating the Gram matrix with the overarching goal of reducing the time and space complexity of kernel methods is now vast. Various techniques such as random sampling [39], subspace
embedding [2], and matrix factorization [15] have been used to find a low-rank approximation that
is efficient to compute and gives small approximation error. The most relevant set of tools for our
paper is Nyström sampling [39, 14], which constructs an approximation of K using a subset of
the columns indicated by a selection matrix S to generate a positive semi-definite approximation.
Recent work on leverage scores have been used to improve the guarantees of Nyström sampling in
order to obtain linear time algorithms for generating these approximations [28].
The novelty of our approach is to use Nyström sampling in conjunction with compression schemes
to give a new method for giving provable generalization error bounds for kernel methods. Compression schemes have typically been studied in the context of classification problems in PAC learning
and for combinatorial problems related to VC dimension [23, 24]. Only recently some authors
considered compression schemes in a general, real-valued learning scenario [13]. Cotter, ShalevShwartz, and Srebro have studied compression for classification using SVMs to prove that for general distributions, compressing classifiers with low generalization error is not possible [9].
The general phenomenon of eigenvalue decay of the Gram matrix has been studied from both a theoretical and applied perspective. Some empirical studies of eigenvalue decay and related discussion
can be found in [27, 35, 38]. There has also been prior work relating eigenvalue decay to generalization error in the context of SVMs or Kernel PCA (e.g., [29, 35]). Closely related notions to
eigenvalue decay are that of local Rademacher complexity due to Bartlett, Bousquet, and Mendelson
[4] (see also [5]) and that of effective dimensionality due to Zhang [42].
The above works of Bartlett et al. and Zhang give improved generalization bounds via datadependent estimates of eigenvalue decay of the kernel. At a high level, the goal of these works
is to work under an assumption
√ on the effective dimension and improve Rademacher-based generalization error bounds from 1/ m to 1/m (m is the number of samples) for functions embedded in a
RKHS of unit norm. These works do not address the main obstacle of this paper, however, namely
overcoming the complexity of the norm of the approximating RKHS. Their techniques are mostly
incomparable even though the intent of using effective dimension as a measure of complexity is the
same.
Shamir has shown that for general linear prediction problems with respect to square-loss and norm
bound B, a sample complexity of Ω(B) is required for gradient-based methods [33]. Our work
shows that eigenvalue decay can dramatically reduce this dependence, even in the context of kernel
regression where we want to run in time polynomial in n, the dimension, rather than the (much
larger) dimension of the RKHS.
Recent work on Learning Neural Networks. Due in part to the recent exciting developments in
deep learning, there have been several works giving provable results for learning neural networks
with various activations (threshold, sigmoid, or ReLU). For the most part, these results take various
assumptions on either 1) the distribution (e.g., Gaussian or Log-Concave) or 2) the structure of the
network architecture (e.g. sparse, random, or non-overlapping weight vectors) or both and often have
a bad dependence on one or more of the relevant parameters (dimension, number of hidden units,
depth, or accuracy). Another way to restrict the problem is to work only in the noiseless/realizable
setting. Works that fall into one or more of these categories include [20, 44, 40, 17, 31, 41, 11].
Kernel methods have been applied previously to learning neural networks [43, 26, 16, 12]. The
current broadest class of networks known to be learnable in fully polynomial-time in all parameters
with no assumptions is due to Goel et al. [16], who showed how to learn a sum of one hidden layer of
sigmoids over the domain of Sn−1 , the unit sphere in n dimensions. We are not aware of other prior

3

work that takes only a distributional assumption on the marginal and achieves fully polynomial-time
algorithms for even simple networks (for example, one hidden layer of ReLUs).
Much work has also focused on the ability of gradient descent to succeed in parameter estimation
for learning neural networks under various assumptions with an intense focus on the structure of
local versus global minima [8, 18, 7, 37]. Here we are interested in the traditional task of learning in
the non-realizable or agnostic setting and allow ourselves to output a hypothesis outside the function
class (i.e., we allow improper learning). It is well known that for even simple neural networks, for
example for learning a sigmoid with respect to square-loss, there may be many bad local minima
[1]. Improper learning allows us to avoid these pitfalls.

2

Preliminaries

Notation. The input space is denoted by X and the output space is denoted by Y. Vectors are represented with boldface letters such as x. We denote a kernel function by kψ (x, x0 ) = hψ(x), ψ(x0 )i
where ψ is the associated feature map and for the kernel and Kψ is the corresponding reproducing
kernel Hilbert space (RKHS). For necessary background material on kernel methods we refer the
reader to [30].
Selection and Compression Schemes. It is well known that in the context of PAC learning Boolean
function classes, a suitable type of compression of the training data implies learnability [25]. Perhaps
surprisingly, the details regarding the relationship between compression and ceratin other real-valued
learning tasks have not been worked out until very recently. A convenient framework for us will be
the notion of compression and selection schemes due to David et al. [13].
A selection scheme is a pair of maps (κ, ρ) where κ is the selection map and ρ is the reconstruction
map. κ takes as input a sample S = ((x1 , y1 ), . . . , (xm , ym )) and outputs a sub-sample S 0 and a
finite binary string b as side information. ρ takes this input and outputs a hypothesis h. The size of
the selection scheme is defined to be k(m) = |S 0 | + |b|. We present a slightly modified version of
the definition of an approximate compression scheme due to [13]:
Definition 3 ((, δ)-approximate agnostic compression scheme). A selection scheme (κ, ρ) is an
(, δ)-approximate agnostic compression scheme for hypothesis class H and sample satisfying
property P if for all samples P
S that satisfy P with probability 1 − δ, f = ρ(κ(S)) satisfies
P
m
m
i=1 l(f (xi ), yi ) ≤ minh∈H (
i=1 l(h(xi ), yi )) + .
Compression has connections to learning in the general loss setting through the following theorem
which shows that as long as k(m) is small, the selection scheme generalizes.
Theorem 4 (Theorem 30.2 [32], Theorem 3.2 [13]). Let (κ, ρ) be a selection scheme of size k =
k(m), and let AS = ρ(κ(S)). Given m i.i.d. samples drawn from any distribution D such that
k ≤ m/2, for constant bounded loss function l : Y 0 × Y → R+ with probability 1 − δ, we have

E(x,y)∼D [l(AS (x), y)] −

m
X

v
u
u
l(AS (xi ), yi ) ≤ t ·

i=1

where  = 50 ·

3

!
m
1 X
l(AS (xi ), yi ) + 
m i=1

k log(m/k)+log(1/δ)
.
m

Problem Overview

In this section we give a general outline for our main result. Let S = {(x1 , y1 ), . . . , (xm , ym )} be a
training set of samples drawn i.i.d. from some arbitrary distribution D on X × [0, 1] where X ⊆ Rn .
Let us consider a concept class C such that for all c ∈ C and x ∈ X we have c(x) ∈ [0, 1]. We
wish to learn the concept class C with respect to the square loss, that is, we wish to find c ∈ C that
approximately minimizes E(x,y)∼D [(c(x) − y)2 ]. A common way of solving this is by solving the
empirical minimization problem (ERM) given below and subsequently proving that it generalizes.
4

Optimization Problem 1
m

minimize
c∈C

1 X
(c(xi ) − yi )2
m i=1

Unfortunately, it may not be possible to efficiently solve the ERM in polynomial-time due to issues
such as non-convexity. A way of tackling this is to show that the concept class can be approximately
minimized by another hypothesis class of linear functions in a high dimensional feature space (this
in turn presents new obstacles for proving generalization-error bounds, which is the focus of this
paper).
Definition 5 (-approximation). Let C1 and C2 be function classes mapping domain X to R. C1 is approximated by C2 if for every c ∈ C1 there exists c0 ∈ C2 such that for all x ∈ X , |c(x)−c0 (x)| ≤ .
Suppose C can be -approximated in the above sense by the hypothesis class Hψ = {x →
hv, ψ(x)i|v ∈ Kψ , hv, vi ≤ B} for some B and kernel function kψ . We further assume that the
kernel is bounded, that is, |kψ (x, x’)| ≤ M for some M > 0 for all x, x’ ∈ X . Thus, the problem
relaxes to the following,
Optimization Problem 2
m

minimize
v∈Kψ

1 X
(hv, ψ(xi )i − yi )2
m i=1

subject to

hv, vi ≤ B

UsingP
the Representer theorem, we have that the optimum solution for the above is of the form
m
v∗ = i=1 αi ψ(xi ) for some α ∈ Rn . Denoting the sample kernel matrix be K such that Ki,j =
kψ (xi , xj ), the above optimization problem is equivalent to the following optimization problem,
Optimization Problem 3
minimize
m
α∈R

1
||Kα − Y ||22
m

subject to

αT Kα ≤ B

where Y is the vector corresponding to all yi and ||Y ||∞ ≤ 1 since ∀i ∈ [m], yi ∈ [0, 1]. Let αB
be the optimal solution of the above problem. This is known to be efficiently solvable in poly(m, n)
time as long as the kernel function is efficiently computable.
Applying Rademacher complexity
bounds to Hψ yields generalization error bounds that decrease,
√
roughly, on the order of B/ m (c.f. Supplemental 1.1). If B is exponential in 1/, the accuracy
parameter, or in n, the dimension, as in the case of bounded depth networks of ReLUs, then this
dependence leads to exponential sample complexity. As mentioned in Section 1, in the context
√ of
eigenvalue decay, various results [42, 4, 5] have been obtained to improve the dependence of B/ m
to B/m, but little is known about improving the dependence on B.
Our goal is to show that eigenvalue decay of the empirical Gram matrix does yield generalization bounds with better dependence on B. The key is to develop a novel compression scheme for
kernelized ridge regression. We give a step-by-step analysis for how to generate an approximate,
compressed version of the solution to Optimization Problem 3. Then, we will carefully analyze the
bit complexity of our approximate solution and realize our compression scheme. Finally, we can put
everything together and show how quantitative bounds on eigenvalue decay directly translate into
compressions schemes with low generalization error.

4

Compressing the Kernel Solution

Through a sequence of steps, we will sparsify α to find a solution of much smaller bit complexity
that is still an approximate solution (to within a small additive error). The quality and size of the
approximation will depend on the eigenvalue decay.

5

Lagrangian Relaxation. We relax Optimization Problem 3 and consider the Lagrangian version of
the problem to account for the norm bound constraint. This version is convenient for us, as it has a
nice closed-form solution.
Optimization Problem 4
minimize
m
α∈R

1
||Kα − Y ||22 + λαT Kα
m

We will later set λ such that the error of considering this relaxation is small. It is easy to see that the
−1
optimal solution for the above lagrangian version is α = (K + λmI) Y .
Preconditioning. To avoid extremely small or non-zero eigenvalues, we consider a perturbed version of K, Kγ = K + γmI. This gives us that the eigenvalues of Kγ are always greater than
or equal to γm. This property is useful for us in our later analysis. Henceforth, we consider the
following optimization problem on the perturbed version of K:
Optimization Problem 5
minimize
m
α∈R

1
||Kγ α − Y ||22 + λαT Kγ α
m
−1

The optimal solution for perturbed version is αγ = (Kγ + λmI)

Y = (K + (λ + γ)mI)

−1

Y.

Sparsifying the Solution via Nyström Sampling. We will now use tools from Nyström Sampling
to sparsify the solution obtained from Optimzation Problem 5. To do so, we first recall the definition
of effective dimension or degrees of freedom for the kernel [42]:
Definition 6 (η-effective dimension). For a positive semidefinite m × m matrix K and parameter
η, the η-effective dimension of K is defined as dη (K) = tr(K(K + ηmI)−1 ).
Various kernel approximation results have relied on this quantity, and here we state a recent result
due to [28] who gave the first application independent result that shows that there is an efficient way
of computing a set of columns of K such that K̄, a matrix constructed from the columns is close in
terms of 2-norm to the matrix K. More formally,
Theorem 7 ([28]). For kernel matrix K, there exists an algorithm that gives a set of
O (dη (K) log (dη (K)/δ)) columns, such that K̄ = KS(S T KS)† S T K where S is the matrix that
selects the specific columns, satisfies with probability 1 − δ, K̄  K  K̄ + ηmI.
It can be shown that K̄ is positive semi-definite. Also, the above implies ||K − K̄||2 ≤ ηm. We use
the decay to approximate the Kernel matrix with a low-rank matrix constructed using the columns
of K. Let K̄γ be the matrix obtained by applying Theorem 7 to Kγ for η > 0 and consider the
following optimization problem,
Optimization Problem 6
minimize
m
α∈R

1
||K̄γ α − Y ||22 + λαT K̄γ α
m

−1
The optimal solution for the above is ᾱγ = K̄γ + λmI
Y . Since K̄γ = Kγ S(S T Kγ S)† S T Kγ ,
∗
solving for the above enables us to get a solution α = S(S T Kγ S)† S T Kγ ᾱγ , which is a k-sparse
vector for k = O (dη (Kγ ) log (dη (Kγ )/δ)).
Bounding the Error of the Sparse Solution. We bound the additional error incurred by our sparse
hypothesis α∗ compared to αB . To do so, we bound the error for each of the approximations: sparsification, preconditioning and lagrangian relaxation and then combine them to give the following
theorem.
2
3
3
1
, η ≤ 729B
and γ ≤ 729B
, we have m
||Kγ α∗ − Y ||22 ≤
Theorem 8 (Total Error). For λ = 81B
1
2
m ||KαB − Y ||2 + .
6

Computing the Sparsity of the Solution. To compute the sparsity of the solution, we need to bound
dη (Kβ ). We consider the following different eigenvalue decays.
Definition 9 (Eigenvalue Decay). Let the real eigenvalues of a symmetric m × m matrix A be
denoted by λ1 ≥ · · · ≥ λm .
1. A is said to have (C, p)-polynomial eigenvalue decay if for all i ∈ {1, . . . , m}, λi ≤ Ci−p .
2. A is said to have C-exponential eigenvalue decay if for all i ∈ {1, . . . , m}, λi ≤ Ce−i .
Note that in the above definitions C and p are not necessarily constants. We allow C and p to
depend on other parameters (the choice of these parameters will be made explicit in subsequent
theorem statements). We can now bound the effective dimension in terms of eigenvalue decay:
Theorem 10 (Bounding effective dimension). For γm ≤ η,

1/p
C
1. If K/m has (C, p)-polynomial eigenvalue decay for p > 1 then dη (Kγ ) ≤ (p−1)η
+ 2.


C
+ 2.
2. If K/m has C-exponential eigenvalue decay then dη (Kγ ) ≤ log (e−1)η

5

Compression Scheme

The above analysis gives us a sparse solution for the problem and, in turn, an -approximation for
the error on the overall sample S with probability 1 − δ. We can now fully define our compression
scheme for the hypothesis class Hψ with respect to samples satisfying the eigenvalue decay property.
Selection Scheme κ: Given input S = (xi , yi )m
i=1 ,
1. Use RLS-Nyström Sampling [28] to compute K¯γ = Kγ S(S T Kγ S)† S T Kγ for η =
3
γ = 5832Bm
. Let I be the sub-sample corresponding to the columns selected using S.
2. Solve Optimization Problem 6 for λ =

2
324B

3
5832B

and

to get ᾱγ .

3. Compute the |I|-sparse vector α∗ = S(S T Kγ S)† S T Kγ ᾱγ = Kγ−1 K̄γ ᾱγ (Kγ is invertible as all
eigenvalues are non-zero).
4. Output subsample I along with α̃∗ which is α∗ truncated to precision


4M |I|

per non-zero index.

∗

Reconstruction Scheme ρ:
Given input subsample I and α̃ , output hypothesis,
hS (x) = clip0,1 (wT α̃∗ ) where w is a vector with entries K(xi , x) + γm1[x = xi ] for
3
. Note, clipa,b (x) = max(a, min(b, x)) for some a < b.
i ∈ I and 0 otherwise where γ = 5832Bm
The following theorem shows that the above is a compression scheme for Hψ .
Theorem 11. (κ, ρ) is an (, δ)-approximate agnostic
compression
hypothesis class

 √ scheme for the

mBM d log(d/δ)
d
Hψ for sample S of size k(m, , δ, B, M ) = O d log δ log
where d is the
4
η-effective dimension of Kγ for η =

6

3
5832B

and γ =

3
5832Bm .

Putting It All Together: From Compression to Learning

We now present our final algorithm: Compressed Kernel Regression (Algorithm 1). Note that the
algorithm is efficient and takes at most O(m3 ) time.
For our learnability result, we restrict distributions to those that satisfy eigenvalue decay.
Definition 12 (Distribution Satisfying Eigenvalue Decay). Consider distribution D over X and
kernel function kψ . Let S be a sample drawn i.i.d. from the distribution D and K be the empirical
gram matrix corresponding to kernel function kψ on S.
1. D is said to satisfy (C, p, N )-polynomial eigenvalue decay if with probability 1 − δ over the
drawn sample of size m ≥ N , K/m satisfies (C, p)-polynomial eigenvalue decay.
7

Algorithm 1 Compressed Kernel Regression

1:

2:
3:
4:

Input: Samples S = (xi , yi )m
i=1 , gram matrix K on S, constants , δ > 0, norm bound B and
maximum kernel function value M on X .
3
3
and η = 5832B
Using RLS-Nyström Sampling [28] with input (Kγ , ηm) for γ = 5832Bm
compute K¯γ = Kγ S(S T Kγ S)† S T Kγ . Let I be the subsample corresponding to the columns
selected using S. Note that the number of columns selected depends on the η effective dimension of Kγ .
2
Solve Optimization Problem 6 for λ = 324B
to get ᾱγ over S
Compute α∗ = S(S T Kγ S)† S T Kγ ᾱγ = Kγ−1 K̄γ ᾱγ
Compute α̃∗ by truncating each entry of α∗ up to precision 4M|I|
Output: hS such that for all x ∈ X , hS (x) = clip0,1 (wT α̃∗ ) where w is a vector with entries
K(xi , x) + γm1[x = xi ] for i ∈ I and 0 otherwise.

2. D is said to satisfy (C, N )-exponential eigenvalue decay if with probability 1 − δ over the drawn
sample of size m ≥ N , K/m satisfies C-exponential eigenvalue decay.
Our main theorem proves generalization of the hypothesis output by Algorithm 1 for distributions
satisfying eigenvalue decay in the above sense.
Theorem 13 (Formal for Theorem 1). Fix function class C with output bounded in [0, 1] and
M -bounded kernel function kψ such that C is 0 -approximated by Hψ = {x → hv, ψ(x)i|v ∈
Kψ , hv, vi ≤ B} for some ψ, B. Consider a sample S = {(xi , yi )m
i=1 } drawn i.i.d. from D on
X × [0, 1]. There exists an algorithm A that outputs hypothesis hS = A(S), such that,
1. If DX satisfies (C, p, m)-polynomial eigenvalue decay with probability 1 − δ/4 then with probability 1 − δ for m = Õ((CB)1/p log(M ) log(1/δ)/2+3/p ),

E(x,y)∼D (hS (x) − y)2 ≤ min E(x,y)∼D (c(x) − y)2 + 20 + 
c∈C

2. If DX satisfies (C, m)-exponential eigenvalue decay with probability 1−δ/4 then with probability
1 − δ for m = Õ(log CB log(M ) log(1/δ)/2 ),

E(x,y)∼D (hS (x) − y)2 ≤ min E(x,y)∼D (c(x) − y)2 + 20 + 
c∈C

Algorithm A runs in time poly(m, n).
Remark: The above theorem can be extended to different rates of eigenvalue decay. For example,
for finite rank r the obtained bound is independent of B but dependent instead on r. Also, as in the
proof of Theorem 10, it suffices for the eigenvalue decay to hold only after sufficiently large i.

7

Learning Neural Networks

Here we apply our main theorem to the problem of learning neural networks. For technical definitions of neural networks, we refer the reader to [43].
Definition 14 (Class of Neural Networks [16]). Let N [σ, D, W, T ] be the class of fully-connected,
feed-forward networks with D hidden layers, activation function σ and quantities W and T described as follows:
1. Weight vectors in layer 0 have 2-norm bounded by T .
2. Weight vectors in layers 1, . . . , D have 1-norm bounded by W .
3. For each hidden unit σ(w · z) in the network, we have |w · z| ≤ T (by z we denote the input feeding
into unit σ from the previous layer).
We consider activation functions σrelu (x) = max(0, x) and σsig = 1+e1−x , though other activation
functions fit within our framework. Goel et al. [16] showed that the class of ReLUs/Sigmoids along
with their compositions can be approximated by linear functions in a high dimensional Hilbert space
8

(corresponding to a particular type of polynomial kernel). As mentioned earlier, the sample complexity of prior work depends linearly on B, which, for even a single ReLU, is exponential in 1/.
Assuming sufficiently strong eigenvalue decay, we can show that we can obtain fully polynomial
time algorithms for the above classes.
Theorem 15. For , δ > 0, consider D on Sn−1 × [0, 1] such that,
1. For Crelu = N [σrelu , 0, ·, 1], DX satisfies (C, p, m)-polynomial eigenvalue decay for p ≥ ξ/,
2. For Crelu−D = N [σrelu , D, W, T ], DX satisfies (C, p, m)-polynomial eigenvalue decay for
p ≥ (ξW D DT /)D ,
3. For Csig−D = N [σsig , D, W, T ], DX satisfies (C, p, m)-polynomial eigenvalue decay for p ≥
(ξT log(W D D/)))D ,
where DX is the marginal distribution on X = Sn−1 , ξ > 0 is some sufficiently large constant and
C ≤ (n · 1/ · log(1/δ))ζp for some constant ζ > 0. The value of m is obtained from Theorem 13
as m = Õ(C 1/p 2+3/p ).
Each decay assumption above implies an algorithm for agnostically learning the corresponding
class on Sn−1 × [0, 1] with respect to the square loss in time poly(n, 1/, log(1/δ)).
Note that assuming an exponential eigenvalue decay (stronger than polynomial) will result in efficient learnability for much broader classes of networks.
Since it is not known how to agnostically learn even a single ReLU with respect to arbitrary distributions on Sn−1 in polynomial-time4 , much less a network of ReLUs, we state the following corollary
highlighting the decay we require to obtain efficient learnability for simple networks:
Corollary 16 (Restating Corollary 2). Let C be the class of all fully-connected networks of ReLUs
with one-hidden layer of size ` feeding into a final output ReLU activation where the 2-norms of
all weight vectors are bounded by 1. Then, (suppressing the parameter m for simplicity), assuming
(C, i−`/ )-polynomial eigenvalue decay for C = poly(n, 1/, `), C is learnable in polynomial time
with respect√to square
loss on Sn−1 . If ReLU is replaced with sigmoid, then we require eigenvalue
√
decay of i− ` log( `/) .

8

Conclusions and Future Work

We have proposed the first set of distributional assumptions that guarantee fully polynomial-time
algorithms for learning expressive classes of neural networks (without restricting the structure of
the network). The key abstraction was that of a compression scheme for kernel approximations,
specifically Nyström sampling. We proved that eigenvalue decay of the Gram matrix reduces the
dependence on the norm B in the kernel regression problem.
Prior distributional assumptions, such as the underlying marginal equaling a Gaussian, neither lead
to fully polynomial-time algorithms nor are representative of real-world data sets5 . Eigenvalue decay, on the other hand, has been observed in practice and does lead to provably efficient algorithms
for learning neural networks.
A natural criticism of our assumption is that the rate of eigenvalue decay we require is too strong.
In some cases, especially for large depth networks with many hidden units, this may be true6 . Note,
however, that our results show that even moderate eigenvalue decay will lead to improved algorithms. Further, it is quite possible our assumptions can be relaxed. An obvious question for future
work is what is the minimal rate of eigenvalue decay needed for efficient learnability? Another direction would be to understand how these eigenvalue decay assumptions relate to other distributional
assumptions.
Goel et al. [16] show that agnostically learning a single ReLU over {−1, 1}n is as hard as learning sparse
parities with noise. This reduction can be extended to the case of distributions over Sn−1 [3].
5
Despite these limitations, we still think uniform or Gaussian assumptions are worthwhile and have provided
highly nontrivial learning results.
6
It is useful to keep in mind that agnostically learning even a single ReLU with respect to all distributions
seems computationally intractable, and that our required eigenvalue decay in this case is only a function of the
accuracy parameter .
4

9

Acknowledgements. We would like to thank Misha Belkin and Nikhil Srivastava for very helpful
conversations regarding kernel ridge regression and eigenvalue decay. We also thank Daniel Hsu,
Karthik Sridharan, and Justin Thaler for useful feedback. The analogy between eigenvalue decay
and power-law graphs is due to Raghu Meka.

References
[1] Peter Auer, Mark Herbster, and Manfred K. Warmuth. Exponentially many local minima for
single neurons. In Advances in Neural Information Processing Systems, volume 8, pages 316–
322. The MIT Press, 1996.
[2] Haim Avron, Huy Nguyen, and David Woodruff. Subspace embeddings for the polynomial
kernel. In Advances in Neural Information Processing Systems, pages 2258–2266, 2014.
[3] Peter Bartlett, Daniel Kane, and Adam Klivans. personal communication.
[4] Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities.
33(4), August 16 2005.
[5] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds
and structural results. Journal of Machine Learning Research, 3:463–482, 2002.
[6] Pawel Brach, Marek Cygan, Jakub Lacki, and Piotr Sankowski. Algorithmic complexity of
power law networks. CoRR, abs/1507.02426, 2015.
[7] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with
gaussian inputs. CoRR, abs/1702.07966, 2017.
[8] Anna Choromanska, Mikael Henaff, Michaël Mathieu, Gérard Ben Arous, and Yann LeCun.
The loss surfaces of multilayer networks. In AISTATS, volume 38 of JMLR Workshop and
Conference Proceedings. JMLR.org, 2015.
[9] Andrew Cotter, Shai Shalev-Shwartz, and Nati Srebro. Learning optimally sparse support
vector machines. In Proceedings of the 30th International Conference on Machine Learning
(ICML-13), pages 266–274, 2013.
[10] Amit Daniely. Complexity theoretic limitations on learning halfspaces. In STOC, pages 105–
117. ACM, 2016.
[11] Amit Daniely. SGD learns the conjugate kernel class of the network. CoRR, abs/1702.08503,
2017.
[12] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In NIPS, pages 2253–2261,
2016.
[13] Ofir David, Shay Moran, and Amir Yehudayoff. On statistical learning via the lens of compression. arXiv preprint arXiv:1610.03592, 2016.
[14] Petros Drineas and Michael W Mahoney. On the nyström method for approximating a
gram matrix for improved kernel-based learning. journal of machine learning research,
6(Dec):2153–2175, 2005.
[15] Petros Drineas, Michael W Mahoney, and S Muthukrishnan. Relative-error cur matrix decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844–881, 2008.
[16] Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in
polynomial time. arXiv preprint arXiv:1611.10258, 2016.
[17] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of nonconvexity: Guaranteed training of neural networks using tensor methods. arXiv preprint
arXiv:1506.08473, 2015.
[18] Kenji Kawaguchi. Deep learning without poor local minima. In NIPS, pages 586–594, 2016.
[19] Adam R. Klivans and Pravesh Kothari. Embedding hard learning problems into gaussian space.
In APPROX-RANDOM, volume 28 of LIPIcs, pages 793–809. Schloss Dagstuhl - LeibnizZentrum fuer Informatik, 2014.
[20] Adam R. Klivans and Raghu Meka. Moment-matching polynomials. Electronic Colloquium
on Computational Complexity (ECCC), 20:8, 2013.
10

[21] Adam R. Klivans and Alexander A. Sherstov. Cryptographic hardness for learning intersections of halfspaces. J. Comput. Syst. Sci, 75(1):2–12, 2009.
[22] Anton Krohmer. Finding Cliques in Scale-Free Networks. Master’s thesis, Saarland University,
Germany, 2012.
[23] Dima Kuzmin and Manfred K. Warmuth. Unlabeled compression schemes for maximum
classes. Journal of Machine Learning Research, 8:2047–2081, 2007.
[24] Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical
report, Technical report, University of California, Santa Cruz, 1986.
[25] Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. Technical
report, 1986.
[26] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training
neural networks. In Advances in Neural Information Processing Systems, pages 855–863,
2014.
[27] Siyuan Ma and Mikhail Belkin. Diving into the shallows: a computational perspective on
large-scale shallow learning. CoRR, abs/1703.10622, 2017.
[28] Cameron Musco and Christopher Musco. Recursive sampling for the nyström method. arXiv
preprint arXiv:1605.07583, 2016.
[29] B. Schölkopf, J. Shawe-Taylor, AJ. Smola, and RC. Williamson. Generalization bounds via
eigenvalues of the gram matrix. Technical Report 99-035, NeuroCOLT, 1999.
[30] Bernhard Schölkopf and Alexander J Smola. Learning with kernels: support vector machines,
regularization, optimization, and beyond. MIT press, 2002.
[31] Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with
sparse connectivity. arXiv preprint arXiv:1412.2693, 2014.
[32] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
[33] Ohad Shamir. The sample complexity of learning linear predictors with the squared loss.
Journal of Machine Learning Research, 16:3475–3486, 2015.
[34] Ohad Shamir. Distribution-specific hardness of learning neural networks. arXiv preprint
arXiv:1609.01037, 2016.
[35] John Shawe-Taylor, Christopher KI Williams, Nello Cristianini, and Jaz Kandola. On the
eigenspectrum of the gram matrix and the generalization error of kernel-pca. IEEE Transactions on Information Theory, 51(7):2510–2522, 2005.
[36] Le Song, Santosh Vempala, John Wilmes, and Bo Xie. On the complexity of learning neural
networks. arXiv preprint arXiv:1707.04615, 2017.
[37] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for multilayer neural networks. CoRR, abs/1605.08361, 2016.
[38] Ameet Talwalkar and Afshin Rostamizadeh. Matrix coherence and the nystrom method. CoRR,
abs/1408.2044, 2014.
[39] Christopher KI Williams and Matthias Seeger. Using the nyström method to speed up kernel machines. In Proceedings of the 13th International Conference on Neural Information
Processing Systems, pages 661–667. MIT press, 2000.
[40] Bo Xie, Yingyu Liang, and Le Song. Diversity leads to generalization in neural networks.
CoRR, abs/1611.03131, 2016.
[41] Qiuyi Zhang, Rina Panigrahy, and Sushant Sachdeva. Electron-proton dynamics in deep learning. CoRR, abs/1702.00458, 2017.
[42] Tong Zhang. Effective dimension and generalization of kernel learning. In Advances in Neural
Information Processing Systems, pages 471–478, 2003.
[43] Yuchen Zhang, Jason D Lee, and Michael I Jordan. l1-regularized neural networks are improperly learnable in polynomial time. In International Conference on Machine Learning, pages
993–1001, 2016.
[44] Yuchen Zhang, Jason D. Lee, Martin J. Wainwright, and Michael I. Jordan. Learning halfspaces and neural networks with random initialization. CoRR, abs/1511.07948, 2015.

11

