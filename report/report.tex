\documentclass[bachelor,german]{info1thesis}

\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage{fontspec}
%\usepackage{lmodern} TODO: braucht man das?

\usepackage{tabularx}
\usepackage{ltablex}

\usepackage{path}
\usepackage{color}
\usepackage{booktabs}
\usepackage{multirow}

%\usepackage[disable,colorinlistoftodos]{luatodonotes}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Titelseite -- hier Titel und Autorennamen eintragen
\title{Machine Learning for Natural Language Processing} % Geben Sie hier den Titel Ihrer Arbeit an.
\subtitle{am Lehrstuhl für Informatik X}
\author{Konstantin Herud\and Thomas Schaffroth} % Geben Sie Ihren Namen an. \and Maximilian Meißner
%\date{Eingereicht am \abgabedatum}
\titlehead{Julius-Maximilians-Universität Würzburg\\
Institut für Informatik\\
Lehrstuhl für Informatik X\\
Data Science}
\supervisors{Daniel Schlör\and Albin Zehe\and Konstantin Kobs\and Tobias Koopmann} %Prof. Dr. Andreas Hotho\and

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% Bitte nur ab hier Änderungen vornehmen %%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
Diese Arbeit untersucht die Fragestellung, ob die Muttersprache von Autoren englischsprachiger, wissenschaftlicher Texte durch linguistische Merkmale ermittelt werden kann. Die Grundlage dafür bildet ein Korpus mit über 18.000 Veröffentlichungen auf Konferenzen des künstlichen Intelligenzbereichs. Hierzu werden zunächst umfangreich direkte und indirekte Informationen, die Nationalitäten preisgeben, wie Namen, Adressen oder Referenzen, entfernt. Anschließend werden zahlreiche Klassifikationsverfahren basierend auf Stylometrie- und Deep-Learning-Methoden untersucht. Die stylometrischen Modelle erreichen über zwölf Muttersprachen hinweg einen Makro-F1-Wert von 44\,\%. Neben verschiedenen anderen Deep-Learning Ansätzen erreicht ein End-to-End-LSTM-Modell einen Wert von 53\,\%. Die Idee hinter dessen Architektur ist, globale Vorhersagen als Summe gewichteter, lokaler Entscheidungen zu treffen. Zudem ist es möglich, einzelne Textzeilen zu klassifizieren und die erlernte Gewichtung zu visualisieren, um zu überprüfen, welche Informationen zu Vorhersagen geführt haben.
\end{abstract}

\thesistableofcontents

\chapter{Einleitung}
\textbf{Thomas Schaffroth} \\
% Nachdem der in diesen Tagen gängige Begriff der \textit{Artificial Intelligence} (AI) auf der Dartmouth Conference 1956 erstmals geprägt wurde, bezeichnet dieser bis heute das entsprechende Forschungs- und Entwicklungsfeld. Was damals mit dem computergestützten Lösen algebraischer Probleme oder der Demonstration geometrischer Theoreme begann, brachte im Laufe der Jahre einen umfangreichen, sich schnell entwickelndem Bereich der Informatik hervor. Nicht zuletzt stellten Innovationen, wie die Bayesschen Lernverfahren 1960, die Einführung des „Parallel Computings“ 1980 oder die Erfindung des Backpropagation-Algorithmus für Neuronale Netze 1984, wesentliche Meilensteine in dieser Entwicklung dar. \\
Durch aufkommende Neuerungen zu Beginn der 2000er Jahre, wie der Herausforderung von \textit{Big Data} sowie der Entfaltung des Internets und der mobilen Kommunikation, wurden seit dem Beginn der \textit{künstlichen Intelligenz} (KI) große Fortschritte in Feldern, wie der Computer Vision, intelligenten Agenten und Mustererkennung, erzielt. Es sind unter anderem ebendiese Weiterentwicklungen, die den Weg für moderne Herausforderungen, wie Spracherkennung, selbstfahrende Fahrzeuge und \textit{Natural Language Processing} (NLP), geebnet haben \cite{Perez2018}.
Besonders das Themengebiet des NLP nimmt einen wichtigen Stellenwert in vielen Software-Anwendungen unseres täglichen Lebens ein. Einige der herausragendsten Beispiele sind E-Mail-Plattformen, wie Microsoft Outlook (Spam-Klassifikation, Auto-Complete etc.), sprachbasierte Assistenzsysteme, wie Apple Siri und Amazon Alexa oder Services für maschinelle Übersetzung, wie Google Translate \cite{Vajjala2020}. \\
Die zunehmende Bedeutung von NLP lässt sich neben seiner Rolle in unserem Alltag auch in der Forschung beobachten. Die ACL Anthology (AA) ist ein digitales Repository, dass zehntausende Veröffentlichungen von NLP-Papern aus der Familie der ACL- sowie anderer NLP-Konferenzen beinhaltet. Lag die Anzahl der Veröffentlichungen im Jahr 2000 noch bei 1050, wurden 2018 bereits 4173 publizierte Arbeiten verzeichnet \cite{Mohammad2019}. \\
% In diesem Praktikum wird im Folgenden das Natural Language Processing an sich mit wissenschaftlichen Arbeiten, die zu diesem Thema veröffentlicht wurden, zusammengeführt.
%Ziel dieser Arbeit ist die Bearbeitung der Fragestellung, ob die Erkennung der Herkunft von Autoren in wissenschaftlichen Publikationen aufgrund textueller Eigenschaften ihrer Veröffentlichungen möglich ist, sowie welche Methoden bessere oder schlechtere Ergebnisse liefern. 
Ziel dieser Arbeit ist die Bearbeitung der Fragestellung, ob die Erkennung der Muttersprache von Autoren in anderssprachigen, wissenschaftlichen Texten möglich ist. Dieses Problem zu lösen ist deshalb relevant, weil so beispielsweise Autoren besser identifiziert, forensische Analysen unterstützt oder pädagogische Anwendungen stärker personalisiert werden können \cite{Estival2007,Gibbons2003,Rozovskaya2011}.
Der hierfür zur Verfügung stehende Datensatz beinhaltet über 18.000 Dokumente verschiedener NLP- und AI-Konferenzen in PDF- und Text-Form. \\
Das Praktikum gliedert sich in zwei Bestandteile. Zunächst müssen die Dokumente einer grundsätzlichen Datenbereinigung unterzogen werden. Informationen, von denen direkt oder indirekt auf die Herkunft der Autoren geschlossen werden kann, müssen dabei aus den Arbeiten entfernt werden. Beispiele hierfür sind Autorennamen, E-Mail-Adressen, Ländernamen, Institutionen oder Referenzen im Text. Der Grund dafür ist, dass für den zweiten Teil der Arbeit sichergestellt werden muss, dass die Klassifikation der Dokumente aufgrund allgemeiner Texteigenschaften, wie der Syntax und dem verwendeten Vokabular, stattfindet.
Informationen, die sich auf die Herkunft des Autors beziehen, würden für eine Verfälschung der Ergebnisse sorgen. \\
Der folgende Schritt beschäftigt sich mit der Feature-Modellierung, dem Training und der Evaluierung im Kontext verschiedener Techniken der KI und NLP auf dem genannten Korpus in Bezug auf die Klassifikationsaufgabe. Ziel soll sein, einen Klassifikator zu trainieren, der für eine möglichst große Zahl der Dokumente das Herkunftsland des jeweiligen Autors korrekt vorhersagt. 
%Grundlegendes Wissen über die Funktionsweise, typische Architekturen und dem Lernprozess neuronaler Netze sowie klassische Vorverarbeitungsschritte im NLP-Bereich werden in Folgenden vorausgesetzt und nicht explizit behandelt.

\chapter{Vorausgehende Arbeit}

Die Aufgabe der Muttersprachenerkennung basierend auf Texten einer zweiten Sprache ist in der Literatur als \textit{Native Language Identification} (NLI) bekannt. Die Association for Computational Linguistics (ACL) bietet dafür einen eigenen Wettbewerb zur Entwicklung des besten Systems \cite{Tetreault2013,Malmasi2017}, welcher das Interesse an diesem Forschungsgebiet mit seiner Einführung massiv steigerte. Generell wird NLI als überwachtes Trainingsproblem behandelt. Als eine der ersten extrahieren Koppel et al. 2005 eine Vielzahl an linguistischen Merkmalen, wie Zeichen und Part-Of-Speech-N-Gramme, Funktionswörter und insbesondere Rechtschreib- und Grammatikfehler. Damit trainieren sie eine \textit{Support-Vektor-Maschine} (SVM) und erreichen auf dem ICLE-Korpus mit fünf Muttersprachen etwa 80\,\% Genauigkeit \cite{Koppel2005}. \\
Spätere Arbeiten versuchen syntaktische Merkmale miteinzubeziehen. Wong und Dras nutzen 2011 Parsebäume und Parsereranking, um von syntaktischen Fehlern auf die Muttersprache zu schließen, und erreichen damit über sieben Muttersprachen 80\,\% Genauigkeit \cite{Wong2011}. Tetreault et al. sowie Swanson und Charniak untersuchen 2012 jeweils Tree-Substitution-Grammars und erreichen damit auf sieben Muttersprachen ebenfalls etwa 80\,\% Genauigkeit \cite{Tetreault2012,Swanson2012}. Jarvis et al. gewannen 2013 den ersten NLI-Workshop der ACL mit einer SVM und einer Vielzahl linguistischer Merkmale, wobei sie fast 84\,\% Genauigkeit erreichen \cite{Jarvis2013}. \\
Bis zum nächsten ACL-Workshop 2017 entstanden zahlreiche weitere Ansätze \cite{Malmasi2017}. Dabei zeigten sich verschiedene Erkenntnisse. Zum einen setzten sich Deep-Learning Ansätze nicht durch. Stattdessen ging die Forschung weiter in Richtung extrahierter Merkmale und Klassifikation mittels Support-Vektor-Maschinen und anderen linearen Modellen. Zum anderen zeigte sich, dass Systeme mit mehreren Klassifikatoren gut funktionieren. Der beste Ansatz von Cimino und Dell'Orletta 2017 nutzt beispielsweise lineare Regression zur Klassifikation auf Satzebene und anschließend eine SVM, die mit diesen Ergebnissen eine Vorhersage auf Dokumentebene trifft, wodurch sie eine Genauigkeit von etwa 88,2\,\% auf 11 Muttersprachen und 1000 Dokumenten pro Sprache erreichen \cite{Cimino2017}. Ein weiterer Ansatz von Kulmizev et al. nutzt eine lineare SVM mit Zeichen-N-Grammen und erreicht dabei etwa 87,6\,\% Genauigkeit \cite{Kulmizev2017}. Dabei weisen sie darauf hin, dass bei einer Aufgabe wie NLI Zeichen-N-Gramme besonders geeignet sind, um Muster in Rechtschreibfehlern zu erkennen, und dabei wesentlich dünner besetzt sind, als Wort-N-Gramme. Ein Deep-Learning-Ansatz mit LSTM-Netzwerken von Bjerva et al. erreicht hingegen nur 83\,\% Genauigkeit \cite{Bjerva2017}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Methodik}
\textbf{Konstantin Herud} \\
Zunächst wird das Vorgehen bei der Datenbereinigung erläutert und anschließend die Implementierung verschiedener Verfahren zur Bestimmung der Autornationalitäten.

\section{Task 1: Dataset Preparation}

Da verschiedene Informationen die Nationalität von Autoren entweder direkt oder indirekt preisgeben, müssen diese aus der Datenbasis entfernt werden. Die Zielsetzung sieht hierbei vor,
Namen der Autoren,
E-Mails,
Institutionen und Firmen,
Herkunftsländer,
Förderungen und Danksagungen,
persönliche Daten
sowie Referenzen
zu entfernen.
Referenzen sind beispielsweise indirekte Hinweise, weil Arbeitsgruppen der gleichen Herkunft und somit dem selben Fachgebiet die selbe Literatur referenzieren. \\
Da diese Aufgabe essentieller Bestandteil der Validität späterer Ergebnisse ist, wurde eine mehrstufige Daten-Pipeline entwickelt, um alle Informationen bestmöglich aus den Texten zu entfernen. Jeder folgende Paragraph entspricht einer der Stufen.

\paragraph{Textextraktion} Im ersten Schritt muss der rohe Text der Dokumente eingelesen werden. Die Daten liegen sowohl in PDF- als auch in Textform vor, wobei letztere durch das Befehlszeilenprogramm \textit{pdftotext} erstellt wurde und somit bereits Potenzial für Fehler bietet. Ursprünglich wurde deshalb ein Ansatz mit dem Werkzeug \textit{pdfminer} verfolgt, um den Text der Dokumente selbstständig zu extrahieren und dabei direkt Strukturinformationen ausnutzen zu können (z.\,B. um Referenz-Blöcke zu erkennen). Bei einigen Dokumenten entstanden jedoch Fehler mit pdfminer, weshalb dieser Ansatz verworfen wurde. So werden in der Pipeline zunächst die Textdateien sowie zugehörige Metadaten eingelesen \footnote{\url{https://github.com/marekrei/ml_nlp_paper_data}}. Da verschiedene spätere Schritte außerdem Informationen über die Zeichenposition des Abstract-Anfangs brauchen, wird dieser zudem bereits mit einem regulären Ausdruck, unterstützt durch verschiedene Fehlermaßnahmen, bestimmt. So wird beispielsweise überprüft, ob der Titel des Dokuments das Wort Abstract enthält, um zu frühe Treffer des regulären Ausdrucks zu überspringen. Falls der Start nicht gefunden werden kann oder eine bestimmte Obergrenze überschreitet, wird ein mittelwert-basierter Defaultwert verwendet.
\paragraph{Dokumentkopf} Als Dokumentkopf definieren wir jeglichen Text (in Rohform), der vor dem Abstract erscheint. Die hohe Dichte unterschiedlicher zu entfernender Informationen, gepaart mit Artefakten und Strukturfehlern, machen diese Stufe zu einer der größten Herausforderungen der Aufgabe. Insbesondere Namen, E-Mails, Herkunftsländer, Institutionen und persönliche Daten beziehen sich auf diesen Schritt. Initial wurde dafür die Idee verfolgt, \textit{Named-Entity-Recognition} mittels \textit{spaCy} \cite{Honnibal2017} einzusetzen, um all diese Informationen (außer E-Mails, welche leicht durch reguläre Ausdrücke zu erkennen sind) zu entfernen. SpaCy bietet dafür verschiedene vortrainierte Modelle, welche für diese Arbeit mit im Text durch reguläre Ausdrücke erkannten Informationen nachtrainiert wurden. Da dieser Ansatz Entitäten nicht nur zu unverlässlich erkannte (insbesondere keine Phrasen, die über mehrere Tokens reichen), sondern auch zu schlecht klassifzierte wurde ein weiterer Ansatz entwickelt. So wurde ein \textit{LSTM}-basiertes \cite{Hochreiter1997}, neuronales Netzwerk implementiert, um jede Zeile des Dokumentkopfs in der Textdatei in die Klassen Autor, E-Mail, private Informationen, Institution oder Anderes zu unterteilen. Länder werden in einem späteren Schritt entfernt.  Zum Training wurden 250 Dokumente zeilenweise mit einer Kommandozeilenanwendung annotiert. Dabei wurden zufällig Dokumente aus dem Korpus gezogen, allerdings wurde sichergestellt, dass jede Konferenz vorkommt. Zudem sind verschiedene Fehlerfälle enthalten, bei denen der Start des Abstracts nicht korrekt erkannt wurde und somit späterer Text enthalten ist. Das Modell erreicht auf weiteren 50 Dokumenten einen Makro-F1-Wert von etwa 96\%. Die Architektur verfolgt einen ähnlichen Ansatz wie \autoref{eq:embedding}--\ref{eq:zd}, nutzt jedoch nach $X_{zd}$ ein zweites LSTM zur zeilenweisen Klassifikation.
\paragraph{Referenzen} Referenzen unterteilen sich in zwei Typen: Verweise innerhalb des Texts und ausführliche Referenzblöcke. Erstere sind aufgrund ihrer einheitlichen Struktur leicht mittels regulärer Ausdrücke zu ermitteln. Dafür wurden zahlreiche zufällige Dokumente betrachtet und drei Strukturtypen identifiziert, für die eigene reguläre Ausdrücke entwickelt wurden: Klammer-Ausdrücke, die Jahreszahlen im 20. und 21. Jahrhundert enthalten, Phrasen mit ``et. al.'' und zuletzt Aufzählungen (z.\,B. verbunden durch ``und'', ``,'' oder ``\&'' etc.), gefolgt von Jahreszahlen. \\
Referenzblöcke zu identifizieren stellt sich jedoch als zweitschwerste Aufgabe heraus. Ein initialer Ansatz sah vor, die zuvor im Text gefundenen Referenzen zusammen mit verschiedenen Schlüsselwörtern dazu zu nutzen, Zeilen-Blöcke als Referenzen zu erkennen. Da durch die Konvertierung von zweispaltigen, mehrseitigen Dokumenten in eindimensionalen Text jedoch häufig Strukturfehler auftreten, sind Teile der Referenzblöcke oftmals stark über die Dokumente verteilt, weshalb dieser initiale Ansatz zu keinen befriedigenden Ergebnissen führte. Deshalb wurde ebenfalls ein neuronales Netz, mit einer Architektur äquivalent zur vorherigen, trainiert, um zeilenweise zu annotieren, ob es sich um Referenzen handelt. Dafür wurden etwa 50 Dokumente manuell annotiert, wobei neben zufällig gewählten Werken insbesondere darauf geachtet wurde, Fehlerfälle zu verwenden. Das Netz verarbeitet den Text iterativ in Stücken von 80 Zeilen (um Padding und Grafikspeicher-Anforderungen möglichst gering zu halten, wurde kein kompletter \textit{End-to-End} Ansatz gewählt). Um dennoch von Informationen über die Position dieser 80 Zeilen im Dokument zu profitieren, wird auf die $d$-dimensionalen ($1, ..., i, ..., d$), internen Zustände des Netzwerks ein Positionssignal $PE$ addiert (vgl. \cite{Vaswani2017}, \autoref{eq:pe1} \& \ref{eq:pe2}).
\begin{align}
PE(zeile, 2i) &= \sin\left(\frac{zeile}{10000^{2i / d}}\right) \label{eq:pe1} \\ \label{eq:pe2}
PE(zeile, 2i+1) &= \cos\left(\frac{zeile}{10000^{2i / d}}\right)
\end{align}
Das Modell erreicht auf 20\% separaten, zufälligen Daten eine Genauigkeit von beinahe 99,7\%.
Sowohl das Dokumentkopf- als auch das Referenzblocknetz sind mit ihren Gewichten aufgrund der hoch-rekurrenten Struktur unter einem Megabyte groß.
\paragraph{Gutachter} Generell wurden wenige Gutachter innerhalb der Dokumente festgestellt. Diese geringe Menge folgt jedoch einem festen Schema, welches mittels eines regulären Ausdrucks ermittelt wird.
\paragraph{Danksagungen} Danksagungen sind meist wenige Zeilen in geschlossenen Blöcken und somit leichter zu ermitteln als Referenzblöcke. Vorerst stand im Raum, ebenfalls ein neuronales Netz einzusetzen, allerdings genügen reguläre Ausdrücke. So wird nach dem letzten Vorkommen der gängigen Überschrift ``Acknowledgment'' gesucht (wobei unterschiedliche Schreibweisen zu beachten sind, i.\,e. Acknowledge?ments?). Das Ende der Danksagung wird durch eine Liste mit Schlüsselwörtern, kombiniert mit den zuvor gefunden Referenzen sowie einem regulären Ausdruck für Referenzen (da Danksagungen diesen häufig vorausgehen), ermittelt. Kann dennoch kein Ende gefunden werden, wird der Text bis zum Ende des Paragraphen gewählt.
\paragraph{E-Mails} Der Großteil aller E-Mails wurde in diesem Schritt bereits durch die Dokumentkopfstufe entfernt (insbesondere schwer identifizierbare Fälle, deren gezielt irreführende Schreibweise z.\,B. Spam verhindern soll). Der Rest aller E-Mails wird in diesem Schritt mit zwei weiteren regulären Ausdrücken entfernt.
\paragraph{Herkunftsländer} Länder und Nationalitäten werden mit dem Python-Modul \textit{geotext} \footnote{\url{https://github.com/elyase/geotext}} ermittelt. Das Modul arbeitet mit regulären Ausdrücken und der geographischen Datenbank \textit{GeoNames}, die jegliche Länder und über elf Millionen Ortsnamen enthält.
\paragraph{Fußnoten} Fußnoten sind unscharf definiert und aufgrund der fehlenden visuellen Struktur sehr schwer im rohen Text zu identifizieren. Um diese Informationen dennoch zu entfernen, wird der Text zunächst mittels spaCy in Sätze unterteilt. Anschließend werden alle Sätze mit einer Liste von Schlüsselwörtern, wie ``sponsored'' oder ``internship'', sowie bereits ermittelten Autor- und Organisationsnamen untersucht. Wird ein Schlüsselwort gefunden, wird der entsprechende Satz entfernt.
\paragraph{Sprache} Zuletzt wird nicht-englische Sprache entfernt. Diese Aufgabe stellt ebenfalls eine Herausforderung dar, da viele Rohtext-Fragmente aufgrund kryptischer Formeln oder anderen Artefakten von gängigen Sprachidentifikationssystemen fälschlicherweise als nicht-englisch erkannt werden. Dennoch wird die spaCy-Erweiterung \textit{CLD} eingesetzt, welche die \textit{Compact Language Detection 2} Bibliothek \footnote{\url{https://github.com/CLD2Owners/cld2}} anbindet. Diese ermöglicht die Klassifikation von rohem  Text in über 80 verschiedene Sprachen mittels Zeichen-N-Grammen und einem Naiven-Bayes-Modell. Dieser Ansatz führt zu einer akzeptablen Sensitivität, allerdings zu einer schlechten Genauigkeit. Um letztere zu verbessern, wird lediglich eine geringe Teilmenge relevanter Sprachen der 80+ möglichen berücksichtigt.
\\
\\
Die einzelnen Stufen der Pipeline lassen sich nicht nur jeweils über verschiedene Prozesse verteilen und vervielfältigen, die Pipeline lässt sich auch als Ganzes parallelisieren. Zusätzlich ermöglicht die Anwendung Hardware-Beschleunigung für die neuronalen Netzwerke. Das Wissen über zu entfernende Informationen wird akkumuliert und letztlich in eine CSV-Datei geschrieben, außerdem wird der inkrementell bereinigte sowie der ursprüngliche Text zwischen Stufen weitergegeben und abschließend in eine Textdatei geschrieben.

\section{Task 2: Learning to Discriminate}

Um die bereinigten Dokumente hinsichtlich der Muttersprache ihrer Autoren zu klassifizieren, wurden zwei Ansätze verfolgt: Zum einen Klassifikation mittels verschiedenen \textit{stylometrischen} Merkmalen, zum anderen ein \textit{End-to-End-Deep-Learning}-Modell.

\subsection{Stylometrische Klassifikation}

Stylometrie befasst sich mit der Untersuchung von Sprachstil in der Regel zur Bestimmung des Autors \cite{Spillner1974}. Diesem Abschnitt liegt die Annahme zugrunde, dass sich die selben Mittel auch auf die Bestimmung des Herkunftlands übertragen lassen. Statistische und rechnerische Zuweisung der Urheberschaft haben generell eine lange Geschichte, die bis ins 19. Jahrhundert reicht \cite{Stamatatos2009}. Dementsprechend existieren viele Methoden, die von einfachen Ideen, wie Mendenhalls Kurven zu Wortlängen \cite{Mendenhall1887}, über Kilgarriffs $\chi^2$-Tests zu Vokabularähnlichkeit \cite{Kilgarriff2001} bis zu fortgeschritteneren Ideen, wie Burrows Delta Statistik \cite{Burrows2002}, reicht. In dieser Arbeit wurden verschiedene linguistische Merkmale extrahiert.
\begin{itemize}
\item \textbf{Wortschatz}: Für verschiedene Maße zur lexikalischen Diversität einzelner Dokumente wurde das Python-Modul \textit{lexicalrichness} eingesetzt \footnote{\url{https://github.com/LSYS/lexicalrichness}}. Insgesamt wurden die Maße Type-Token-Ration (TTR) \cite{Templin1957}, Guirauds Index \cite{Guiraud1954},  Corrected-TTR \cite{Carroll1964}, Herdans Maß \cite{Herdan1964}, Somers Maß \cite{Somers1966}, Dugasts Index \cite{Dugast1978}, Maas Index \cite{Maas1972}, Mean-Segmental-TTR \cite{Torruella2013}, Moving-Average-TTR \cite{Covington2010}, Measure of Textual Lexical Diversity  \cite{McCarthy2010} und die Hypergeometrische-Verteilungs-Diversität \cite{McCarthy2007} ermittelt.
\item \textbf{Wort- und Satzlängen}: Für Wort- und Satzlängen werden Histogramme der Dokumente gebildet, die mittels der Anzahl an Wörtern beziehungsweise Sätzen insgesamt normalisiert wird. Wortlängen werden in Zeichen gemessen und auf 50 beschränkt, Satzlängen mittels ihrer Anzahl an Wörtern bis 100 (\autoref{app:word-sentence-lengths} zeigt solche Histogramme für alle Dokumente kombiniert).
\item \textbf{Interpunktion}: Häufigkeiten von genutzten Interpunktionszeichen (jegliche in Python durch \textit{string.punctuation} definierte Zeichen), normalisiert über die gesamte Zeichenanzahl im Dokument.
\item \textbf{Funktionswörter}: Häufigkeiten von insbesondere Stopwörtern und weiteren Kandidaten (insgesamt 277 Funktionswörter \footnote{\url{https://semanticsimilarity.wordpress.com/function-word-lists}}).
\item \textbf{N-Gramme}: Den wichtigsten Teil bilden die N-Gramm-Histogramme. Dafür werden jeweils die 200 häufigsten 2- und 3-Gramme über die Lemmata der Wörter, über OntoNotes5-POS-Tags und über universelle POS-Tags mittels spaCy ermittelt (später in dieser Reihenfolge mit \textit{Lemmata}, \textit{Tag} und \textit{POS} bezeichnet) \cite{RalphWeischedel2013}.
\end{itemize}
%
Für jedes Dokument werden alle Merkmale extrahiert, über den gesamten Korpus standardisiert und anschließend mit einem einfachen neuronalen Netzwerk klassifiziert. Das Netzwerk besteht aus einer beliebigen Anzahl Blöcken, die jeweils aus einer verdeckten Schicht, Batch-Normalisierung \cite{Ioffe2015}, SELU-Aktivierung \cite{Klambauer2017} und anschließend Dropout \cite{Srivastava2014} bestehen. Aufgrund der einfachen und schnell zu trainierenden Architektur werden die Hyperparameter (wie Anzahl Blöcke, Anzahl Neuronen, Dropout etc.) mit einer umfangreichen Zufallssuche ermittelt. Neben einem Netzwerk mit der Kombination aller Merkmale existieren separate Netzwerke für alle Merkmalstypen alleinstehend, mit der Intention diese vortrainierten Subnetzwerke später mit dem \textit{End-To-End}-Modell zu kombinieren.
%
\subsection{End-to-End Klassifikation}
%
Als \textit{End-to-End} werden Modelle bezeichnet, die keine vorherigen oder manuellen Schritte, wie Merkmalsextraktion, benötigen, sondern ausschließlich anhand der rohen, bereinigten Texte klassifizieren. Dafür ist zunächst eine Methode zur Textrepräsentation notwendig. Gängige frühe Verfahren zur Repräsentation ganzer Wörter sind latente \textit{Word2Vec}- oder \textit{GloVe}-Vektoren \cite{Mikolov2013,Pennington2014}. Diese sind jedoch aus zahlreichen Gründen problematisch, insbesondere werden die Wörter dabei in der Regel auf Lemmata, Wortstamm oder Ähnliches normalisiert, um die Anzahl benötigter Vektoren zu reduzieren. Zudem existieren für falsche Schreibweisen, seltene oder zusammengesetzte Wörter und andere Eigenheiten keine individuellen Vektoren. All diese Informationen spielen aber potentiell eine wichtige Rolle zur Klassifizierung. \textit{FastText} löst diese Probleme teilweise, indem Wörter als Summen von Zeichen-n-Gramm-Vektoren dargestellt werden und somit auch unbekannte Sequenzen repräsentiert werden können \cite{Bojanowski2016}. Der gegenwärtige Standard und auch in dieser Arbeit gewählte Ansatz ist jedoch eine \textit{Byte Pair Encoding}-Subwortdarstellung \cite{Sennrich2015}. Hierbei werden ausgehend von einer Repräsentation auf Zeichenebene inkrementell die häufigsten Symbolfolgen zu einzelnen Subwörtern kombiniert, wodurch letztlich häufige Wörter als eigene Vokabeln existieren, unbekannte Wörter jedoch auch durch Subwortsequenzen ausgedrückt werden können. Dies ermöglicht trotz geringer Vokabulargröße Texte kompakt zu repräsentieren. Aufgrund der eigenen wissenschaftlichen Semantik der Konferenzen und der spezifischen Syntax der Textdateien mit ihren Strukturfehlern und Artefakten, wird keine vortrainiertes Embedding verwendet, sondern während dem Trainingsprozess von Grund auf erlernt. Zudem wurde probiert, das Embedding eigenständig vorzutrainieren, indem das spätere, anschließend weiterverwendete LSTM (\autoref{eq:embedding} \& \ref{eq:lstm}) mittels \textit{Masked Lanugage Modelling} trainiert wird. Die Idee dabei ist, einzelne Wörter zu maskieren, um anschließend mit dem unmaskierten Kontext vorherzusagen, welche Wörter an den jeweiligen Stellen standen, wodurch das Modell Syntax und teilweise Semantik erlernt und die Embeddings entsprechend anpasst. Dabei zeigte sich zwar keine Verbesserung der späteren Ergebnisse, allerdings eine erhebliche Beschleunigung des Trainingsfortschritts, was eine effiziente Hyperparametersuche erlaubt. \\
Um die zentrale, für diese Arbeit entwickelte End-to-End-Netzarchitektur zu beschreiben werden zunächst einige Indizes definiert.
%
\begin{itemize}
\itemsep-.5em
\item $z$: Zeilen in der Textdatei eines Dokuments (dynamisch)
\item $w$: (Sub-)Worte pro Zeile (dynamisch, Padding kürzerer Zeilen)
\item $d$: Dimension des Modells (150)
\item $e$: Embedding-Dimension (100)
\item $c$: Anzahl Klassen (12)
\end{itemize}
%
Die Netzarchitektur lässt sich dann durch \autoref{eq:embedding} -- \ref{eq:classification} beschreiben (die Batchdimension wird hierbei ausgelassen).
%
\begin{align}
X_{zwe} &= \text{Embedding}(X_{zw}) \label{eq:embedding} \\
X_{zwd} &= \text{LSTM}_w(X_{zwe}) \label{eq:lstm} \\
A_{zw} &= \text{softmax}_w\left(\sum_d X_{zwd} W_{d}^1\right) \\
X_{zd} &= \sum_w X_{zwd} A_{zw} \label{eq:zd} \\
A_{z} &= \text{softmax}_z\left(\sum_d X_{zd} W_{d}^2\right) \label{eq:az} \\
X_{d} &= \sum_z X_{zd} A_z \label{eq:d} \\
Y_c &= X_d W_{dc}^3 \label{eq:classification}
\end{align}
%
So läuft zunächst eine LSTM-Schicht parallel und individuell über die Embedding-Vektoren jeder Zeile im Dokument. Für diesen Schritt wird die Zeilen- als Batchaxe behandelt (d.\,h. beide Dimensionen werden zusammengeführt), wodurch alle Zeilen in einem einzigen Vorwärtsschritt des Netzwerks verarbeitet werden können, was die Architektur extrem schnell macht. Das Netzwerk lernt anschließend eine Gewichtung $W_d^1$ (in Form eines einzelnen Neurons oder Subnetzwerks mit einzelnem Ausgabeneuron, daher eindimensional), wie wichtig jeder verdeckte Zustand der LSTM-Schicht $X_{zwd}$ zu jedem Subwort ist. Dabei werden die mit der \textit{Softmax}-Funktion normalisierten Anteile für jedes Subwort $A_{zw}$ innerhalb einer Zeile bestimmt, um damit die LSTM-Zustände zu gewichten und anschließend zu einem Vektor pro Zeile $X_{zd}$ aufzuaddieren. Das Netzwerk lernt danach eine zweite, äquivalente Gewichtung $W_d^2$, um damit alle Zeilen gemäß ihrer Relevanz zu gewichten und zu einem finalen Vektor $X_d$, der das gesamte Dokument repräsentiert (siehe \autoref{app:hidden-state-projection}), zu summieren. Eine letzte lineare Schicht ergibt dann die Klassen-Logits $Y_c$.
Eine zweite LSTM-Schicht zwischen \autoref{eq:zd} und \ref{eq:az}, um potentiell Abhängigkeiten zwischen Zeilen besser zu erfassen, führte empirisch zu keiner Verbesserung. Dokumente satz- statt zeilenweise zu verarbeiten ergibt vergleichbare Ergebnisse. Durch zufällige Suche wurde $e = 100$ und $d = 150$ ermittelt, das Vokabular besteht aus 12000 Subwörtern. \\
Um extremes Padding zu vermeiden, wird mit einer Batch-Größe von eins trainiert (andernfalls müssten alle Daten im Batch auf die Anzahl des Dokuments mit den meisten Zeilen angepasst werden). Die Subwortaxe wird auf die längste Zeile im Dokument, höchstens jedoch auf 50 Subwörter mittels Padding angepasst (was 99,9\% aller Zeilen einschließt). Um sowohl Speicher- als auch Zeitkomplexität zu reduzieren, wird \textit{Automatic Mixed Precision} \footnote{\url{https://github.com/NVIDIA/apex}}, also auf Float16 statt Float32 basierende Parameter, eingesetzt.
Zudem wurde mit Normalisierung und Dropout nach \autoref{eq:zd} und \ref{eq:d} experimentiert, jedoch ohne resultierende Verbesserungen. \\
Da sich die Häufigkeiten der Dokumente jeder eindeutigen Nationalität extrem unterscheiden (\autoref{tab:label-frequencies}), wurden verschiedene Maßnahmen für das Training untersucht.
%
\begin{table}[h!]
\begin{tabular}{@{}lllllllllllll@{}}
%\toprule
Land       & USA  & CN   & UK   & DE  & JP  & CA  & FR  & IL  & AU  & CH  & SG  & IN  \\ \midrule
Häufigkeit & 8699 & 2119 & 1042 & 770 & 557 & 542 & 470 & 278 & 277 & 256 & 254 & 200 \\ %\bottomrule
\end{tabular}
\caption{Häufigkeiten der eindeutigen Herkunftsländer im Korpus}
\label{tab:label-frequencies}
\end{table}
%
Sowohl Auslassen von Datensätzen größerer Klassen (\textit{Downsampling}) als auch Vervielfachen von Dokumenten seltener Länder (\textit{Upsampling}) zeigten keinen Erfolg. Erstes verschlechterte die Ergebnisse maßgeblich, letzteres führte hauptsächlich zu einer längeren Trainingsdauer. Stattdessen wurde die Loss-Funktion (Kreuzentropie) gewichtet (\autoref{eq:loss-weights}). Der Wert von $\alpha$ wurde dabei durch Ausprobieren aller Werte von 0.0--1.0 in Zehntelschritten ermittelt.
%
\begin{equation}
W_{C_i}^L = \left(\frac{|C_{\max}|}{|C_i|}\right)^\alpha,\;\;\;\alpha = 0.6 \label{eq:loss-weights}
\end{equation}
%
\newpage
\noindent Außerdem wurden verschiedene andere Architekturen ausprobiert. Generell sind Transformer der vorherrschende Architekturstandard. Da deren Komplexität jedoch quadratisch mit der Anzahl an Eingabewörtern skaliert und die Parameteranzahl von vortrainierten Modellen meist in den Gigabyte-Bereich reicht, eignen sich entsprechende Modelle nur bedingt. Zudem ist der Domänentransfer problematisch, da sich die bereinigten Textdateien mit oftmals fehlenden (oder schlichtweg nicht vorhandenen, z.\,B. Titel, Überschriften etc.) Satzgrenzen und zahlreichen Artefakten stark von den strukturierten Daten, auf denen die Transformer vortrainiert wurden, unterscheiden.
\begin{itemize}
\item \textbf{CNN}: Hierbei wurden \autoref{eq:lstm}--\ref{eq:zd} durch ein Text-CNN ersetzt \cite{Kim2014}. Dabei wurden Faltungen mit Breite zwei bis sechs und jeweils 20 Filtern verwendet. Anstatt den Text eindimensional in das CNN zu geben, wurde die zweidimensionale Form $z \times w$ beibehalten, was die Zeitkomplexität dank Parallelisierung von O($zw$) maßgeblich auf O($w$) reduziert. Hierbei werden N-Gramme über Zeilen hinweg nicht beachtet, weshalb statt Zeilen $z$ ganze Sätze $s$ verwendet wurden, welche vorab mittels spaCy ermittelt wurden.
\item \textbf{RoBERTa}: Dieses 125 Millionen Parameter starke, vortrainierte Sprachmodell basiert auf der Transformer-Architektur und kann Sequenzlängen bis 512 verarbeiten \cite{Liu2019}. Der Text wird deshalb in entsprechende Stücke aufgeteilt, welche separat mit dem Modell verarbeitet werden. Anschließend läuft eine LSTM-Schicht über die gepoolte Ausgabe dieser Stücke, um das Dokument letztlich zu klassifizieren.
\item \textbf{Longformer}: Dieses ebenfalls vortrainierte Modell verbessert die Transformer- Architektur, sodass Speicher- und Zeitkomplexität linear mit der Sequenzlänge skalieren \cite{Beltagy2020}. Dadurch wird es möglich, gleichzeitig 4096 Subwörter zu verarbeiten, was durchschnittlich 53\% einzelner Dokumente abdeckt. So werden die ersten 4096 der Subtokens mit dem Modell und einer einfachen linearen Schicht klassifiziert. 
\end{itemize}
%
%O($t$) statt O($zt$)

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}

\chapter{Ergebnisse}

Die Aufgabe der Datenbereinigung wurde von vier Personen mit jeweils 25 Dokumenten ausgewertet, wobei die entwickelte Pipeline einen Jaccard-Index von etwa 0,847 zu den manuellen Annotationen erreicht. \\\autoref{tab:results} zeigt die Ergebnisse der Klassifikation. Aufgrund des Klassenungleichgewichts wurde der Makro-F1 Wert als Metrik verwendet. % Hierbei existieren zwei Baselines \textit{Häufigste Klasse} und \textit{Zufällig}, wobei für alle Dokumente das häufigste beziehungsweise ein zufälliges Herkunftsland bestimmt wird.

\begin{table}[h!]
\centerline{
\begin{tabular}{@{}llllllllllllll@{}}
\toprule
Modell           & AU   & CA  & CN   & FR   & DE   & IN   & IL   & JP   & SG   & CH  & UK   & USA  & Makro \\ \midrule
Häufigste Klasse  &      &      &      &      &      &      &      &      &      &      &      & 72,0 & 6,0   \\
Zufällig          & 3,1  & 6,6  & 9,1  & 4,8  & 5,3  & 1,8  & 2,9  & 5,6  & 2,3  & 3,2  & 7,2  & 13,9 & 5,5 \\
\\
Interpunktion    &      &     & 10,4 &      &      &      &      &      &      &     &      & 72,0 & 8,0   \\
Wortschatz       &      &     &      &      &      &      &      & 3,5  &      &     &      & 71,9 & 8,2   \\
Wort-\&Satzlängen &      &     & 34,3 &      & 2,5  &      &      &      &      &     &      & 71,7 & 9,5   \\
Tag              & 6,2  &     & 70,2 & 45,5 & 37,6 & 16,0 &      & 58,5 &      &     & 17,4 & 77,3 & 30,7  \\
POS              & 6,7  &     & 73,7 & 52,2 & 35,5 & 18,2 & 35,9 & 53,0 &      &     & 20,3 & 78,1 & 31,4  \\
Funktionswörter  &      &     & 68,2 & 23,7 & 14,0 & 46,2 & 43,8 & 59,6 & 6,5  &     & 4,9  & 75,8 & 34,1  \\
Lemmata          &      & 3,4 & 74,2 & 56,1 & 47,7 & 27,6 & 22,7 & 60,2 & 17,6 &     & 17,5 & 79,5 & 36,9  \\
Auswahl          &      &     & 75,5 & 68,1 & 54,0 & 33,3 & 49,0 & 70,3 & 13,8 & 6,9 & 20,4 & 80,5 & 40,9  \\
Kombination      & 10,8 & 3,5 & 74,4 & 64,2 & 55,6 & 32,3 & 44,4 & 68,8 & 18,6 & 7,4 & 29,6 & 79,4 & 44,2 \\
\\
Longformer  &      &      &      &      &      &      &      &      &      &      &      & 72,0 & 6,0   \\
RoBERTa  &      &      &      &      &      &      &      &      &      &      &      & 72,0 & 6,0   \\
CNN               &      & 6,1  & 80,0 & 49,5 & 40,7 & 16,2 & 15,8 & 57,1 &      &      & 48,6 & 83,2 & 33,1  \\
LSTM\textbackslash Gewichtung & 25,0 & 12,3 & 78,7 & 65,9 & 52,9 & 12,9 & 41,7 & 68,4 & 19,5 & 10,0 & 54,3 & 83,5 & 43,8  \\
LSTM              & \textbf{37.5}	& \textbf{28.6}	& 77.9	& 52.8	& 55.3	& \textbf{58.8}	& 57.8	& 78.5	& 38.1	& 13.6	& \textbf{56.6}	& 83.2 & \textbf{53.2} \\
\\
Voting & 23.8	& 21.1	& \textbf{81.9}	& \textbf{69.7}	& \textbf{63.1}	& 22.2	& \textbf{59.1}	& \textbf{79.6}	& \textbf{41.0}	& \textbf{17.6}	& 53.6	& \textbf{85.2} & 51.5  \\
Konkatenation & 13.3	& 26.7	& 78.7	& 60.8	& 54.2	& 41.0	& 49.0	& 74.1	& 40.0	& 14.3	& 49.0	& 82.8 & 48.7 \\
 \bottomrule
\end{tabular}
}
\caption{Auswertung aller Modelle mit F1-Werten jeder Klasse sowie zuletzt Makro-F1 (in Prozent). Zuerst zwei Baselines, dann alle stylometrischen Komponenten und zuletzt Deep-Learning Modelle. \textit{Auswahl} ist ein Netzwerk aus \textit{Tag}, \textit{POS}, \textit{Funktionswörter} sowie \textit{Lemmata}. \textit{Kombination} verbindet alle stylometrischen Komponenten. \textit{LSTM} wurde sowohl mit Gewichtung als auch ohne durch reines Summieren getestet. \textit{Voting} ist zwischen stylometrischen Komponenten und LSTM-Modell gemäß Makro-F1-Wert. \textit{Konkatenation} kombiniert letztere durch ein einfaches Feed-Forward-Netzwerk.}
\label{tab:results}
\end{table}
%
\begin{table}[]
\centering
%\scalebox{1.3}{
\begin{tabular}{@{}clrrrrrrrrrrrr@{}}
\multicolumn{1}{l}{}       & \multicolumn{13}{c}{Voraussage}                                   \\ \cmidrule(l){2-14} 
\multicolumn{1}{l}{}       &     & AU & CA & CN  & FR & DE & IN & IL & JP & SG & CH & UK & USA \\ \cmidrule(l){2-14} 
%\multirow{11}{*}{\rotatebox[origin=c]{90}{Wahrheit}} & AU  & 6  &    & 8   &    & 1  &    &    &    &    &    & 7  & 6   \\
%                           & CA  & 1  & 8  & 4   &    &    &    &    & 2  &    & 1  & 7  & 32  \\
%                           & CN  & 1  & 1  & 188 &    & 1  &    &    & 2  & 1  &    & 1  & 17  \\
%                           & FR  &    &    & 1   & 36 & 3  &    &    & 1  &    &    & 2  & 4   \\
%                           & DE  &   & 2  & 2   & 2  & 47 &    &    & 1  &    &    & 12 & 11  \\
%                           & IN  &    &    & 1   &    &    & 6  &    &    &    &    & 2  & 11  \\
%                           & IL  &    & 1  &     & 1  &    &    & 14 & 2  &    &    &    & 10  \\
%                           & JP  &    &    & 2   & 1  &    &    &    & 45 &    & 1  & 1  & 6   \\
%                           & SG  &    & 1  & 6   &    &    &    &    &    & 9  &    & 2  & 8   \\
%                           & CH  &    &    &    & 3   & 7  &    &    &    &    & 4  & 4  & 8   \\
%                           & UK  & 3  & 5  & 1   & 6  & 1  & 2  &    & 1  &    & 2  & 66 & 18  \\
%                           & USA & 3  & 13 & 38  & 17 & 6  & 1  & 2  & 8  & 2  & 3  & 32 & 746 \\ \cmidrule(l){2-14} 
 & AU  & 9 & 1  & 6   & 1  & 5  &    &    &    &   &    & 3  & 3   \\
 & CA  &   & 13 & 1   & 4  & 6  &    &    &    &   & 1  & 5  & 25  \\
 & CN  & 2 & 2  & 160 & 1  & 3  &    &    & 1  & 1 &    & 2  & 40  \\
 & FR  &   & 1  &     & 28 & 5  &    &    & 1  &   &    & 2  & 10  \\
 & DE  &   &    & 1   & 4  & 55 &    &    &    & 1 & 2  & 4  & 10  \\
 & IN  & 1 & 1  &     &    & 2  & 10 &    &    & 1 &    & 1  & 4   \\
 & IL  &   & 2  &     & 2  & 1  &    & 13 &    &   &    &    & 10  \\
 & JP  &   & 1  &     & 1  &    &    &    & 42 & 2 & 1  & 1  & 8   \\
 & SG  &   &    & 1   & 1  &    & 2  &    &    & 8 &    & 1  & 13  \\
 & CH  &   &    &     & 3  & 8  &    &    &    &   & 3  &    & 12  \\
 & UK  & 1 & 3  & 1   & 3  & 12 &    &    & 1  &   & 1  & 60 & 23  \\
 & USA & 7 & 12 & 29  & 11 & 25 & 2  & 4  & 6  & 3 & 10 & 28 & 733 \\ \cmidrule(l){2-14} 
\end{tabular}
%}
\caption{Konfusionsmatrix des LSTM-Modells über die Klassen Australien (AU), Canada (CA), China (CN), Frankreich(FR), Deutschland (DE), Indien (IN), Israel (IL), Japan (JP), Singapur (SG), Schweitz (CH), Großbritannien (UK), Amerika (USA).}
\label{tab:confusion-matrix}
\end{table}
%
\noindent\autoref{tab:confusion-matrix} zeigt die Konfusionsmatrix der LSTM-Architektur. Viele Fehler sind hierbei nachvollziehbar, beispielsweise die Klassifizierung von Dokumenten der Schweiz als französisch. \\ \\
Die entwickelte Architektur besticht nicht nur durch ihre Rekurrenz sowie Effizienz, sie ermöglicht auch ein gewisses Maß an Einblick in das Modell. Wie \autoref{app:visualized-texts} zeigt, kann beispielsweise die gelernte, min-max-normalisierte Gewichtung jedes Subworts $A_{zw}' = A_{zw}A_{z}$ visualisiert werden (hierbei ist zu beachten, dass die Gewichtung auf rekurrenten Zuständen beruht und nicht auf Subwörtern selbst). Wirkliche Erkenntnisse ergeben sich dadurch nicht, allerdings lässt sich sicherstellen, dass das Modell keinen falschen Hinweisen, wie übersehenen, zu entfernenden Informationen, folgt. Lernt das Modell mit den unzensierten Texten, zeigt sich, dass hauptsächlich E-Mail-Domänen berücksichtigt werden, welche vermutlich selten in anderen Kontexten vorkommen und das Herkunftsland somit am zuverlässigsten preisgeben. \\
%
%\begin{align}
%A_\text{global} &= A_{zt} \cdot A_{z} \\
%A_\text{global,norm} &= \frac{A_\text{global} - \min A_\text{global}}{\max A_\text{global} - \min A_\text{global}}
%\end{align}
%
%
%
Zudem ermöglicht die Architektur einen weiteren Weg Einblick in das Modell zu gewinnen. Der finale Dokumentvektor $X_d$ ist lediglich eine gewichtete Summe der Zeilenvektoren $X_{zd}$. Deshalb können aus allen Dokumentvektoren $X_d$ geometrische Schwerpunkte $Z_i$ jeder Klasse $i$ berechnet werden (nachvollziehbar in \autoref{app:hidden-state-projection}), um anschließend für jede Zeile $j$ mit der Kosinus-Ähnlichkeit (\autoref{eq:cos-sim}) zu bestimmen, welche Nationalität am wahrscheinlichsten ist.
\begin{equation}
sim(X_{zd,j}, Z_i) = \frac{X_{zd,j} \cdot Z_i}{||X_{zd,j}||_2\,||Z_i||_2} \label{eq:cos-sim}
\end{equation}
\autoref{app:line-projection} zeigt dies beispielhaft.

\chapter{Zusammenfassung}

\textbf{Konstantin Herud und Thomas Schaffroth} \\
Sprache ist vielfältig. So haben unterschiedliche Länder verschiedene Eigenheiten, die sich beim Schreiben und Sprechen einer Fremdsprache in zu wörtlichen Übersetzungen, spezifischen Fehlern oder ungewöhnlichen Sprachkonstrukten äußern. Das führt zu der Frage, ob sich diese Eigenschaften nutzen lassen, um die Muttersprache eines Autors zu identifizieren. \\
Dafür wurden in dieser Arbeit über 18.000 englischsprachige, wissenschaftliche Dokumente -- also mit grundsätzlich hoher fremdsprachlicher Kompetenz -- im Themengebiet der künstlichen Intelligenz untersucht.
Um sicherzustellen, dass keine nicht-linguistischen Informationen die Herkunft der Autoren direkt oder indirekt preisgeben, werden die Dokumenttexte einer aufwändigen Bereinigung unterzogen. So werden in einer mehrstufigen Pipeline die Namen der Autoren, E-Mails, Institutionen und Firmen, Herkunftsländer, Förderungen und Danksagungen, persönliche Daten und Referenzen entfernt. Die Pipeline erreicht dabei vor allem mit neuronalen Netzen und regulären Ausdrücken einen Jaccard-Index von etwa 85\,\% zu vier menschlichen Prüfern. \\
Anschließend wurden verschiedene Klassifikationsverfahren vorgestellt. Zunächst wurden verschiedene stylometrische Methoden untersucht. So haben insbesondere Histogramme von N-Grammen über verschiedene POS-Tags, Wortlemmata und Funktionswörter Erfolg gezeigt, mit Makro-F1-Werten von 30--37\,\% (was einer Genauigkeit von etwa 94\,\% entspricht). Verschiedene Maße zur lexikalischen Diversität, Wort- und Satzlängen sowie Interpunktion waren mit einem Makro-F1-Wert von unter 10\,\% weniger erfolgreich. Alle stylometrischen Merkmale kombiniert erreichten über 44\,\%. \\
Anschließend wurden verschiedene Deep-Learning-Ansätze untersucht. CNN- und Trans\-former-Architekturen konnten sich dabei nicht durchsetzen, allerdings wurde eine ergebnisreiche LSTM-Architektur mit einem Makro-F1-Wert von 53\,\% entwickelt. Die Idee dieser Architektur ist, globale Vorhersagen als Summe gewichteter, lokaler Entscheidungen auf Zeilenebene zu treffen. Zum einen können diese globalen Entscheidungen nicht nur auf lokale Ebene zurückgeführt werden, wodurch sich einzelne Textzeilen klassifizieren lassen, zum anderen kann auch die erlernte Gewichtung visualisiert werden, um zu überprüfen, welche Informationen zu Klassifikationsergebnissen führen. \\
Die Frage, ob sich die Muttersprache von Autoren basierend auf Texten einer zweiten Sprache bestimmen lässt, konnte so weitestgehend beantwortet werden. \\
Zukünftig könnte der Fokus stärker auf vortrainierte Transformer-Architekturen gesetzt werden, welche oftmals auf mehr als einer Sprache trainiert werden und somit nicht nur englische Dokumente klassifizieren könnten, sondern dank ihres Parameterreichtums und des rapiden Forschungsfortschritts auf einer Vielzahl anderssprachiger Texte zusätzlich noch wesentlich bessere Ergebnisse erzielen könnten.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thesisbibliography
\bibliography{report}

\appendix

\chapter{Wort- und Satzlängenhistogramme}
\label{app:word-sentence-lengths}

\begin{figure}[h!]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/histogram-word-lengths.pdf}
         \subcaption{Wortlängen}
         \label{fig:hist-mentions}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/histogram-sentence-lengths.pdf}
         \subcaption{Satzlängen}
         \label{fig:hist-clusters}
     \end{subfigure}
     \caption{Histogramme über alle Dokumente hinsichtlich durchschnittlicher Zeichenlänge von Wörtern und Wortlänge von Sätzen. Artefakte und fehlerhafte Strukturinformationen verzerren die Ergebnisse.}
     \label{fig:droc-histograms}
\end{figure}

\chapter{Visualisierung der gelernten Subwortgewichtung}
\label{app:visualized-texts}

\begin{figure}[h!]
     \centering
     \begin{subfigure}[b]{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/attention-visualized.pdf}
         \subcaption{Bereinigter Text}
         \label{fig:visualized-cleaned-text}
     \end{subfigure}
     \begin{subfigure}[b]{\textwidth}
         \centering
         \vspace{.5em}
         \includegraphics[width=\textwidth]{img/attention-visualized-uncensored.pdf}
         \subcaption{Originaltext}
         \label{fig:visualized-original-text}
     \end{subfigure}
     \caption{Visualisierung der gelernten Gewichtung des verdeckten LSTM-Zustands zu jedem Subwort zur Klassifikation des Herkunftslandes mit einem Ausschnitt des bereinigten und ursprünglichen Text.}
\end{figure}

\chapter{PCA-Projektion des internen Modellzustands}
\label{app:hidden-state-projection}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{img/hidden-state-projection.png}
\caption{Dreidimensionale \textit{Principal Component Analysis}-Projektion des internen Modellzustands $X_d$ (siehe \autoref{eq:d}) aller Dokumente der Klassen USA (blau), China (rot) und UK (rosa). Zur besseren Übersicht auf drei Klassen beschränkt. Erstellt mittels \url{https://projector.tensorflow.org/}.}
\end{figure}

\chapter{Klassifikation einzelner Zeilen}
\label{app:line-projection}

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{img/line-pca.png}
\caption{Dreidimensionale \textit{Principal Component Analysis}-Projektion des internen Modellzustands jeder Zeile $X_{zd}$ eines zufälligen Dokuments (siehe \autoref{eq:zd}). Über die Kosinus-Ähnlichkeit zu Schwerpunkten der Dokumentvektoren $X_d$ kann jeder Zeile eines der Länder USA (blau), China (rot) oder UK (rosa) zugeordnet werden.}
\end{figure}

\textbf{Beispiele für Zeilen mit hoher Ähnlichkeit}
\begin{itemize}
\item \textbf{USA}:%
\vspace{-.5em}
\begin{figure}[h!]
\centering
\includegraphics[height=1.2em]{img/line-usa.pdf}
\end{figure}
\vspace{-1.5em}
\item \textbf{China}:%
\vspace{-.5em}
\begin{figure}[h!]
\centering
\includegraphics[height=1.2em]{img/line-china.pdf}
\end{figure}
\vspace{-1.5em}
\item \textbf{UK}:%
\vspace{-.5em}
\begin{figure}[h!]
\centering
\includegraphics[height=1.2em]{img/line-uk.pdf}
\end{figure}
\end{itemize}

%\clearpage
%\pdfbookmark[0]{ToDo-Liste}{todos}
%\listoftodos

\end{document}
