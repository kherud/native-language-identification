\documentclass[bachelor,german]{info1thesis}

\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage{fontspec}
%\usepackage{lmodern} TODO: braucht man das?

\usepackage{tabularx}
\usepackage{ltablex}

\usepackage{path}
\usepackage{color}
\usepackage{booktabs}
\usepackage{multirow}

%\usepackage[disable,colorinlistoftodos]{luatodonotes}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Titelseite -- hier Titel und Autorennamen eintragen
\title{Machine Learning for Natural Language Processing} % Geben Sie hier den Titel Ihrer Arbeit an.
\subtitle{am Lehrstuhl für Informatik X}
\author{Konstantin Herud\and Thomas Schaffroth} % Geben Sie Ihren Namen an. \and Maximilian Meißner
%\date{Eingereicht am \abgabedatum}
\titlehead{Julius-Maximilians-Universität Würzburg\\
Institut für Informatik\\
Lehrstuhl für Informatik X\\
Data Science}
\supervisors{Daniel Schlör\and Albin Zehe\and Konstantin Kobs\and Tobias Koopmann} %Prof. Dr. Andreas Hotho\and

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% Bitte nur ab hier Änderungen vornehmen %%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
    Dieses Dokument soll Studenten an unserem Lehrstuhl bei der Erstellung
    ihrer Abschlussarbeit unterstützen.
    Wir zeigen eine beispielhafte Gliederung einer Arbeit und beschreiben
    die Inhalten der einzelnen Kapitel.
    Zusätzlich geben wir an vielen Stellen auch Hinweise zur Benutzung von
    \LaTeX\ für die Erstellung der Arbeit.
    Im Anhang~\ref{appendix:orga} geben wir ein paar Hinweise zum Ablauf der
    Betreuung von Abschlussarbeiten an unserem Lehrstuhl.

    \paragraph{Zur Handhabung dieses Pakets.}
    In diesem Paket sind Vorlagen für verschiedene Dokumenttypen enthalten, die
    sie als Ausgangspunkt für ihre Arbeit verwenden können.
    Es gibt jeweils Vorlagen für deutsche und englische Arbeiten.
    \begin{itemize}
        \item \verb+template_thesis_de.tex+, \verb+template_thesis_en.tex+:
            Vorlage für Bachelorarbeit bzw. Masterarbeit
        \item \verb+template_seminar_de.tex+, \verb+template_seminar_en.tex+:
            Vorlage für Seminarausarbeitungen und Praktikumsberichte
    \end{itemize}
    Der Quelltext zu diesem Leitfaden ist ebenfalls im Paket enthalten.
    Diesen können Sie als praktisches Beispiel dafür verwenden, wie diese
    Dokumentenklasse angewandt wird.

    \paragraph{Inhalt der Zusammenfassung.}
    Schreiben Sie hier eine Zusammenfassung der Arbeit, vergleichbar mit dem Abstract auf wissenschaftlichen Papers.
    Sie dient dem Leser dazu, einen groben Überblick über die Inhalte zu gewinnen (Problemstellung, verwendeter Lösungsansatz, ggf.\ experimentelle Ergebnisse, gewonnene Erkentnisse).
    Der Umfang soll ca.\ eine halbe Seite betragen.
    Für Seminararbeiten ist diese Zusammenfassung nicht erforderlich.
    
    \emph{Achtung:} Bei Arbeiten auf Englisch fordern die
    Prüfungsordnungen, dass es eine deutsche Zusammenfassung gibt.
    Schreiben Sie in diesem Fall eine englische \emph{und} eine deutsche Zusammenfassung (mit dem gleichen Inhalt).
    Die passenden \LaTeX-Befehle dafür finden Sie in den englischsprachigen
    Vorlagen.

    \paragraph{WARNUNG:} 
    Die vorliegende Version des Leitfadens ist eine \textcolor{red}{Vorabversion}, die noch nicht vollständig ist.
    Sie bezieht sich größtenteils auf die Ausarbeitung von Bachelor- und Masterarbeiten; Seminararbeiten unterscheiden sich davon etwas in Aufbau und Inhalt.

%    \vspace{3em}
%    \textbf{TODOs für die Titelseite:}
%    \todo{Soll der Name des Lehrstuhls für englische Arbeiten übersetzt werden?}
\end{abstract}

\thesistableofcontents

\chapter{Einleitung}
\textbf{Thomas Schaffroth} \\
Nachdem der in diesen Tagen gängige Begriff der \textit{Artificial Intelligence} (AI) auf der Dartmouth Conference 1956 erstmals geprägt wurde, bezeichnet dieser bis heute das entsprechende Forschungs- und Entwicklungsfeld. Was damals mit dem computergestützten Lösen algebraischer Probleme oder der Demonstration geometrischer Theoreme begann, brachte im Laufe der Jahre einen umfangreichen, sich schnell entwickelndem Bereich der Informatik hervor. Nicht zuletzt stellten Innovationen, wie die Bayesschen Lernverfahren 1960, die Einführung des „Parallel Computings“ 1980 oder die Erfindung des Backpropagation-Algorithmus für Neuronale Netze 1984, wesentliche Meilensteine in dieser Entwicklung dar. \\
Durch aufkommende Neuerungen zu Beginn der 2000er Jahre, wie der Herausforderung des Big Data sowie der Entfaltung des Internets und der mobilen Kommunikation wurden seitdem große Fortschritte in Feldern, wie der Computer Vision, Intelligenten Agenten und Mustererkennung erzielt. Es sind unter anderem ebendiese Weiterentwicklungen, die den Weg für moderne Herausforderungen, wie Spracherkennung, selbstfahrende Fahrzeuge und Natural Language Processing (NLP), geebnet haben \cite{Perez2018}.
Besonders das Themengebiet des NLP nimmt einen wichtigen Stellenwert in vielen Software-Anwendungen unseres täglichen Lebens ein. Einige der herausragendsten Beispiele sind E-Mail Plattformen, wie Microsoft Outlook (Spam-Klassifikation, Auto-Complete, etc.), sprachbasierte Assistenzsysteme, wie Apple Siri und Amazon Alexa oder Services für maschinelle Übersetzung, wie Google Translate \cite{Vajjala2020}. \\
Die zunehmende Bedeutung von NLP lässt sich neben seiner Rolle in unserem Alltag auch in der Forschung beobachten. Die ACL Anthology (AA) ist ein digitales Repository, dass zehntausende Veröffentlichungen von NLP-Papern aus der Familie der ACL- sowie anderer NLP-Konferenzen beinhaltet. Lag die Anzahl der Veröffentlichungen im Jahr 2000 noch bei 1050 wurden 2018 bereits 4173 publizierte Paper verzeichnet \cite{Mohammad2019}. \\
In diesem Praktikum wird im Folgenden das Natural Language Processing an sich mit wissenschaftlichen Arbeiten, die zu diesem Thema veröffentlicht wurden, zusammengeführt.
Ziel dieser Arbeit ist die Bearbeitung der Fragestellung, ob die Erkennung der Herkunft von Autoren wissenschaftlicher Publikationen aufgrund textueller Eigenschaften ihrer Veröffentlichungen möglich ist, sowie welche Methoden bessere oder schlechtere Ergebnisse liefern. Der hierfür zu Verfügung stehende Datensatz beinhaltet 18000 Paper verschiedener NLP- und AI-Konferenzen in PDF- und Text-Form. \\
Das Praktikum gliedert sich in zwei Bestandteile. Zunächst müssen die Paper einer grundsätzlichen Datenbereinigung unterzogen werden. Informationen, von denen direkt oder indirekt auf die Herkunft der Autoren geschlossen werden kann, müssen dabei aus den Arbeiten entfernt werden. Beispiele hierfür sind Autoren-Namen, E-Mail-Adressen, Ländernamen, Institutionen oder Referenzen im Text. Der Grund dafür ist, das für den zweiten Teil der Arbeit sichergestellt werden muss, dass die Klassifikation der Paper aufgrund allgemeiner Texteigenschaften, wie der Syntax und dem verwendeten Vokabular stattfindet. \\
Informationen, die sich auf die Herkunft des Autors beziehen, würden für eine Verfälschung des Klassifikationsergebnisses sorgen.
Der folgende Schritt beschäftigt sich mit der Feature-Modellierung, dem Training und der Evaluierung im Kontext verschiedener Techniken der AI und NLP auf dem genannten Korpus in Bezug auf die Klassifikations-Aufgabe. Ziel soll sein, einen Klassifikator zu trainieren, der für eine möglichst große Zahl der Paper das Herkunftsland des jeweiligen Autors korrekt vorhersagt. 
%Grundlegendes Wissen über die Funktionsweise, typische Architekturen und dem Lernprozess neuronaler Netze sowie klassische Vorverarbeitungsschritte im NLP-Bereich werden in Folgenden vorausgesetzt und nicht explizit behandelt.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Methodik}
\textbf{Konstantin Herud} \\

\section{Task 1: Dataset Preparation}

Da verschiedene Informationen das Herkunftsland von Autoren entweder direkt oder indirekt preisgeben, müssen diese aus der Datenbasis entfernt werden. Die Zielsetzung sieht hierbei vor
Namen der Autoren,
E-Mails,
Institutionen und Firmen,
Herkunftsländer,
Förderungen und Danksagungen,
Persönliche Daten
sowie Referenzen
zu entfernen.
Referenzen sind beispielsweise indirekte Hinweise, weil Arbeitsgruppen der gleichen Herkunft und somit dem selben Fachgebiet die selbe Literatur referenzieren. \\
Da diese Aufgabe essentieller Bestandteil der Validität späterer Ergebnisse ist, wurde eine mehrstufige Daten-Pipeline entwickelt, um alle Informationen bestmöglich aus den Texten zu entfernen. Jeder folgende Paragraph entspricht einer der Stufen.

\paragraph{Textextraktion} Im ersten Schritt muss der rohe Text der Dokumente eingelesen werden. Die Daten liegen sowohl in PDF- als auch in Textform vor, wobei letztere durch das Befehlszeilenprogramm \textit{pdftotext} erstellt wurde und somit bereits Potenzial für Fehler bietet. Ursprünglich wurde deshalb ein Ansatz mit dem Werkzeug \textit{pdfminer} verfolgt, um den Text der Dokumente selbstständig zu extrahieren und dabei direkt Strukturinformationen ausnutzen zu können (z.\,B. um Referenz-Blöcke zu erkennen). Bei einigen Dokumenten entstanden jedoch Fehler mit pdfminer, weshalb dieser Ansatz verworfen wurde. So werden in der Pipeline zunächst die Textdateien sowie zugehörige Metadaten eingelesen \footnote{\url{https://github.com/marekrei/ml_nlp_paper_data}}. Da verschiedene spätere Schritte außerdem Informationen über die Zeichenposition des Abstract-Starts brauchen, wird dieser zudem bereits mit einem regulären Ausdruck, unterstützt durch verschiedene Fehlermaßnahmen, bestimmt. So wird beispielsweise überprüft, ob der Titel des Dokuments das Wort Abstract enthält, um zu frühe Treffer des regulären Ausdrucks zu überspringen. Falls der Start nicht gefunden werden kann oder eine bestimmte Obergrenze überschreitet, wird ein mittelwert-basierter Defaultwert verwendet.
\paragraph{Dokumentkopf} Als Dokumentkopf definieren wir jeglichen Text (in Rohform), der vor dem Abstract erscheint. Die hohe Dichte unterschiedlicher zu entfernender Informationen, gepaart mit Artefakten und Strukturfehlern, machen diese Stufe zu einer der größten Herausforderungen der Aufgabe. So kommen beziehen sich vor allem Namen, E-Mails, Herkunftsländer, Institutionen und persönliche Daten auf diesen Schritt. Initial wurde dafür die Idee verfolgt, \textit{Named-Entity-Recognition} mittels \textit{spaCy} \cite{Honnibal2017} einzusetzen, um all diese Informationen (außer E-Mails, welche leicht durch reguläre Ausdrücke zu erkennen sind) zu entfernen. SpaCy bietet dafür verschiedene vortrainierte Modelle, welche für diese Arbeit mit im Text durch reguläre Ausdrücke erkannten Informationen nachtrainiert wurden. Da dieser Ansatz Entitäten nicht nur zu unverlässlich erkannte (insbesondere keine Phrasen, die über mehrere Tokens reichen), sondern auch zu schlecht klassifzierte wurde ein weiterer Ansatz entwickelt. So wurde ein \textit{LSTM}-basiertes \cite{Hochreiter1997}, neuronales Netzwerk implementiert, um jede Zeile (im Textdokument, also keine ganzen Sätze) des Vor-Abstract-Texts in die Klassen Autor, E-Mail, Private Informationen, Institution oder Anderes zu unterteilen. Länder werden in einem späteren Schritt entfernt.  Zum Training wurden 250 Dokumente zeilenweise mit einer Kommandozeilenanwendung annotiert. Dabei wurden zufällig Dokumente aus dem Korpus gezogen, allerdings wurde sichergestellt, dass jede Konferenz vorkommt. Zudem sind verschiedene Fehlerfälle enthalten, bei denen der Start des Abstracts nicht korrekt erkannt wurde und somit späterer Text enthalten ist. Das Modell erreicht auf weiteren 50 Dokumenten einen Makro-F1-Wert von etwa 96\%. Die Architektur verfolgt einen ähnlichen Ansatz wie \autoref{eq:embedding}--\ref{eq:zd}, nutzt jedoch nach $X_{zd}$ ein zweites LSTM zur zeilenweisen Klassifikation.
\paragraph{Referenzen} Referenzen unterteilen sich in zwei Typen: Verweise innerhalb des Texts und ausführlich Referenzblöcke. Erstere sind aufgrund ihrer einheitlichen Struktur leicht mittels regulären Ausdrücken zu ermitteln. Dafür wurden zahlreiche zufällige Dokumente betrachtet und drei Strukturtypen identifiziert, für die eigene reguläre Ausdrücke entwickelt wurden: Klammer-Ausdrücke, die Jahreszahlen im 20. und 21. Jahrhundert enthalten, Phrasen mit ``et. al.'' und Aufzählungen (z.\,B. verbunden durch ``und'', ``,'' oder ``\&'' etc.), gefolgt von Jahreszahlen. Referenzblöcke zu identifizieren stellt sich jedoch als zweitschwerste Aufgabe heraus. Ein initialer Ansatz sah vor, die zuvor im Text gefundenen Referenzen zusammen mit verschiedenen Schlüsselwörtern dazu zu nutzen, Zeilen-Blöcke als Referenzen zu erkennen. Da durch die Konvertierung von zweispaltigen, mehrseitigen Dokumenten in eindimensionalen Text jedoch häufig Strukturfehler auftreten, sind Teile der Referenzblöcke oftmals stark über die Dokumente verteilt, weshalb dieser initiale Ansatz zu keinen befriedigenden Ergebnissen führte. Deshalb wurde ebenfalls ein neuronales Netz, mit einer Architektur äquivalent zur vorherigen, trainiert, um zeilenweise zu annotieren, ob es sich um Referenzen handelt. Dafür wurden etwa 50 Dokumente manuell annotiert, wobei neben zufällig gewählten Werken insbesondere darauf geachtet wurde, Fehlerfälle zu verwenden. Das Netz verarbeitet den Text iterativ in Stücken von 80 Zeilen (um Padding und Grafikspeicher-Anforderungen möglichst gering zu halten, wurde kein kompletter \textit{End-to-End} Ansatz gewählt). Um dennoch von Informationen über die Position dieser 80 Zeilen im Dokument zu profitieren, wird auf die $d$-dimensionalen ($1, ..., i, ..., d$), internen Zustände des Netzwerks ein Positionssignal $PE$ addiert (vgl. \cite{Vaswani2017}, \autoref{eq:pe1} \& \ref{eq:pe2}).
\begin{align}
PE(zeile, 2i) &= \sin\left(\frac{zeile}{10000^{2i / d}}\right) \label{eq:pe1} \\ \label{eq:pe2}
PE(zeile, 2i+1) &= \cos\left(\frac{zeile}{10000^{2i / d}}\right)
\end{align}
Das Modell erreicht auf 20\% separaten, zufälligen Daten einen Makro-F1-Wert von beinahe 99,7\%.
Sowohl das Dokumentkopf- als auch das Referenzblocknetz sind mit ihren Gewichten aufgrund der hoch-rekurrenten Struktur unter einem Megabyte groß.
\paragraph{Gutachter} Generell wurden wenige Gutachter innerhalb der Dokumente festgestellt. Diese geringe Menge folgt jedoch einem festen Schema, welches mittels eines regulären Ausdrucks ermittelt wird.
\paragraph{Danksagungen} Danksagungen sind meist wenige Zeilen in geschlossenen Blöcken und somit leichter zu ermitteln als Referenzblöcke. Vorerst stand im Raum, ebenfalls ein neuronales Netz einzusetzen, allerdings genügen reguläre Ausdrücke. So wird nach dem letzten Vorkommen der gängigen Überschrift ``Acknowledgment'' gesucht (wobei unterschiedliche Schreibweisen zu beachten sind, i.\,e. Acknowledge?ments?). Das Ende der Danksagung wird durch eine Liste mit Schlüsselwörtern, kombiniert mit den zuvor gefunden Referenzen sowie einem regulären Ausdruck für Referenzen (da Danksagungen diesen häufig vorausgehen), ermittelt. Kann dennoch kein Ende gefunden werden, wird der Text bis zum Ende des Paragraphen gewählt.
\paragraph{E-Mails} Der Großteil aller E-Mails wurde in diesem Schritt bereits durch die Kopfzielen-Stufe entfernt (insbesondere schwer identifizierbare Fälle, deren gezielt irreführende Schreibweise z.\,B. Spam verhindern soll). Der Rest aller E-Mails wird in diesem Schritt mit zwei weiteren regulären Ausdrücken entfernt.
\paragraph{Herkunftsländer} Länder und Nationalitäten werden mit dem Python-Modul \textit{geotext} \footnote{\url{https://github.com/elyase/geotext}} ermittelt. Das Modul arbeitet mit regulären Ausdrücken und der geographischen Datenbank \textit{GeoNames}, die jegliche Länder und über elf Millionen Ortsnamen enthält.
\paragraph{Fußnoten} Fußnoten sind unscharf definiert und aufgrund der fehlenden visuellen Struktur sehr schwer im rohen Text zu identifizieren. Um diese Informationen dennoch zu entfernen, wird der Text zunächst mittels spaCy in Sätze unterteilt. Anschließend werden alle Sätze mit einer Liste von Schlüsselwörtern, wie ``sponsored'' oder ``internship'', sowie bereits ermittelten Autor- und Organisationsnamen untersucht. Wird ein Schlüsselwort gefunden, wird der entsprechende Satz entfernt.
\paragraph{Sprache} Zuletzt wird nicht-englische Sprache entfernt. Diese Aufgabe stellt ebenfalls eine Herausforderung dar, da viele Rohtext-Fragmente aufgrund kryptischer Formeln oder anderen Artefakten von gängigen Sprachidentifikationssystemen fälschlicherweise als nicht-englisch erkannt werden. Dennoch wird die spaCy-Erweiterung \textit{CLD} eingesetzt, welche die \textit{Compact Language Detection 2} Bibliothek \footnote{\url{https://github.com/CLD2Owners/cld2}} anbindet. Diese ermöglicht die Klassifikation von rohem  Text in über 80 verschiedene Sprachen mittels Zeichen-N-Grammen und einem Naiven-Bayes-Modell. Dieser Ansatz führt zu einer akzeptablen Sensitivität, allerdings zu einer schlechten Genauigkeit. Um letztere zu verbessern, wird lediglich eine geringe Teilmenge relevanter Sprachen der 80+ möglichen berücksichtigt.
\\
\\
Die einzelnen Stufen der Pipeline lassen sich nicht nur jeweils über verschiedene Prozesse verteilen und vervielfältigen, die Pipeline lässt sich auch als Ganzes parallelisieren. Zusätzlich ermöglicht die Anwendung Hardware-Beschleunigung für die neuronalen Netzwerke. Das Wissen über zu entfernende Informationen wird akkumuliert und letztlich in eine CSV-Datei geschrieben, außerdem wird der inkrementell bereinigte sowie der ursprüngliche Text zwischen Stufen weitergegeben und abschließend in eine Text-Datei geschrieben.

\section{Task 2: Learning to Discriminate}

Um die bereinigten Dokumente hinsichtlich des Herkunftslands ihrer Autoren zu klassifizieren, wurden zwei Ansätze verfolgt: Zum einen Klassifikation mittels verschiedenen \textit{stylometrischen} Merkmalen, zum anderen ein \textit{End-to-End-Deep-Learning}-Modell.

\subsection{Stylometrische Klassifikation}

Stylometrie befasst sich mit der Untersuchung von Sprachstil in der Regel zur Bestimmung des Autors \cite{Spillner1974}. Diesem Abschnitt liegt die Annahme zugrunde, dass sich die selben Mittel auch zur Bestimmung des Herkunftlands übertragen lassen. Statistische und rechnerische Zuweisung der Urheberschaft hat generell eine lange Geschichte, die bis ins 19. Jahrhundert reicht \cite{Stamatatos2009}. Dementsprechend existieren viele Methoden, die von einfachen Ideen, wie Mendenhalls Kurven zu Wortlängen \cite{Mendenhall1887}, über Kilgarriffs $\chi^2$-Tests zu Vokabularähnlichkeit \cite{Kilgarriff2001} bis zu fortgeschritteneren Ideen, wie Burrows Delta Statistik \cite{Burrows2002}, reicht. In dieser Arbeit wurden verschiedene linguistische Merkmale extrahiert.
\begin{itemize}
\item \textbf{Lexikalische Diversität}: Für verschiedene Maße zum Reichtum des Wortschatzes einzelner Dokumente wurde das Python-Modul \textit{lexicalrichness} eingesetzt \footnote{\url{https://github.com/LSYS/lexicalrichness}}. Insgesamt wurden die Maße Type-Token-Ration (TTR) \cite{Templin1957}, Guirauds Index \cite{Guiraud1954},  Corrected-TTR \cite{Carroll1964}, Herdans Maß \cite{Herdan1964}, Somers Maß \cite{Somers1966}, Dugasts Index \cite{Dugast1978}, Maas Index \cite{Maas1972}, Mean-Segmental-TTR \cite{Torruella2013}, Moving-Average-TTR \cite{Covington2010}, Measure of Textual Lexical Diversity  \cite{McCarthy2010} und die Hypergeometrische-Verteilungs-Diversität \cite{McCarthy2007} ermittelt.
\item \textbf{Wort- und Satzlängen}: Für Wort- und Satzlängen werden Histogramme der Dokumente gebildet, die mittels der Anzahl an Wörtern beziehungsweise Sätzen insgesamt normalisiert wird. Wortlängen werden in Zeichen gemessen und auf 50 beschränkt, Satzlängen mittels ihrer Anzahl an Wörtern bis 100 (\autoref{app:word-sentence-lengths} zeigt solche Histogramme für alle Dokumente kombiniert).
\item \textbf{Interpunktion}: Häufigkeiten von genutzten Interpunktionszeichen (jegliche in Python durch \textit{string.punctuation} definierte Zeichen), normalisiert über die gesamte Zeichenanzahl im Dokument.
\item \textbf{Funktionswörter}: Häufigkeiten von insbesondere Stopwörtern und weiteren Kandidaten (insgesamt 277 Funktionswörter \footnote{\url{https://semanticsimilarity.wordpress.com/function-word-lists}}).
\item \textbf{N-Gramme}: Den wichtigsten Teil bilden die N-Gramm-Histogramme. Dafür werden jeweils die 200 häufigsten 2- und 3-Gramme über die Lemmata der Wörter, über OntoNotes5-POS-Tags und über universelle POS-Tags mittels spaCy ermittelt (später in dieser Reihenfolge mit \textit{Token}, \textit{Tag} und \textit{POS} abgekürzt) \cite{RalphWeischedel2013}.
\end{itemize}
%
Für jedes Dokument werden alle Merkmale extrahiert, über den gesamten Korpus standardisiert und anschließend mit einem einfachen neuronalen Netzwerk klassifiziert. Das Netzwerk besteht aus einer beliebigen Anzahl Blöcken, die jeweils aus einer verdeckten Schicht, Batch-Normalisierung \cite{Ioffe2015}, SELU-Aktivierung \cite{Klambauer2017} und anschließend Dropout \cite{Srivastava2014} bestehen. Aufgrund der einfachen und schnell zu trainierenden Architektur werden die Hyperparameter (wie Anzahl Blöcke, Anzahl Neuronen, Dropout etc.) mit einer umfangreichen Zufallssuche ermittelt. Neben einem Netzwerk mit der Kombination aller Merkmale existieren separate Netzwerke für alle Merkmalstypen alleinstehend, mit der Intention diese vortrainierten Subnetzwerke später mit dem \textit{End-To-End}-Modell zu kombinieren.
%
\subsection{End-to-End Klassifikation}
%
Als \textit{End-to-End} werden Modelle bezeichnet, die keine vorherigen oder manuellen Schritte, wie Merkmalsextraktion, benötigen, sondern ausschließlich anhand der rohen, bereinigten Texte klassifizieren. Dafür ist zunächst eine Methode zur Textrepräsentation notwendig. Gängige frühe Verfahren zur Repräsentation ganzer Wörter sind latente \textit{Word2Vec}- oder \textit{GloVe}-Vektoren \cite{Mikolov2013,Pennington2014}. Diese sind jedoch aus zahlreichen Gründen problematisch, insbesondere werden die Wörter dabei in der Regel auf Lemmata, Wortstamm oder Ähnliches normalisiert, um die Anzahl benötigter Vektoren zu reduzieren. Zudem existieren für falsche Schreibweisen, seltene oder zusammengesetzte Wörter und andere Eigenheiten keine individuellen Vektoren. All diese Informationen spielen aber potentiell eine wichtige Rolle zur Klassifizierung. \textit{FastText} löst diese Probleme teilweise, indem Wörter als Summen von Zeichen-n-Gramm-Vektoren dargestellt werden und somit auch unbekannte Sequenzen repräsentiert werden können \cite{Bojanowski2016}. Der gegenwärtige Standard und auch in dieser Arbeit gewählte Ansatz ist jedoch eine \textit{Byte Pair Encoding}-Subwortdarstellung \cite{Sennrich2015}. Hierbei werden ausgehend von einer Repräsentation auf Zeichenebene inkrementell die häufigsten Symbolfolgen zu einzelnen Subwörtern kombiniert, wodurch letztlich häufige Wörter als eigene Vokabeln existieren, unbekannte Wörter jedoch auch durch Subwortsequenzen ausgedrückt werden können. Dies ermöglicht trotz geringer Vokabulargröße Texte kompakt zu repräsentieren. Aufgrund der eigenen wissenschaftlichen Semantik der Konferenzen und der spezifischen Syntax der Textdateien mit ihren Strukturfehlern und Artefakten, werden keine vortrainierten Embeddings verwendet, sondern während dem Trainingsprozess von Grund auf erlernt. Zudem wurde probiert, die Embeddings eigenständig vorzutrainieren, indem das spätere, anschließend weiterverwendete LSTM (\autoref{eq:embedding} \& \ref{eq:lstm}) mittels \textit{Masked Lanugage Modelling} trainiert wird. Die Idee dabei ist, einzelne Wörter zu maskieren, um anschließend mit dem unmaskierten Kontext vorherzusagen, welche Wörter an den jeweiligen Stellen standen, wodurch das Modell Syntax und teilweise Semantik erlernt und die Embeddings entsprechend anpasst. Dabei zeigte sich zwar keine Verbesserung der späteren Ergebnisse, allerdings eine erhebliche Beschleunigung des Trainingsfortschritts, was eine effiziente Hyperparametersuche erlaubt. \\
Um die zentrale, für diese Arbeit entwickelte End-to-End-Netzarchitektur zu beschreiben werden zunächst einige Indizes definiert.
%
\begin{itemize}
\itemsep-.5em
\item $z$: Zeilen im Dokument
\item $w$: (Sub-)Worte pro Zeile (Padding kürzerer Zeilen)
\item $d$: Dimension des Modells (z.\,B. 150)
\item $e$: Embedding-Dimension
\item $c$: Anzahl Klassen
\end{itemize}
%
Die Netzarchitektur lässt sich dann durch \autoref{eq:embedding} -- \ref{eq:classification} beschreiben (die Batchdimension wird hierbei ausgelassen).
%
\begin{align}
X_{zwe} &= \text{Embedding}(X_{zw}) \label{eq:embedding} \\
X_{zwd} &= \text{LSTM}_w(X_{zwe}) \label{eq:lstm} \\
A_{zw} &= \text{softmax}_w\left(\sum_d X_{zwd} W_{d}^1\right) \\
X_{zd} &= \sum_w X_{zwd} A_{zw} \label{eq:zd} \\
A_{z} &= \text{softmax}_z\left(\sum_d X_{zd} W_{d}^2\right) \label{eq:az} \\
X_{d} &= \sum_z X_{zd} A_z \label{eq:d} \\
Y_c &= X_d W_{dc}^3 \label{eq:classification}
\end{align}
%
So läuft zunächst eine LSTM-Schicht parallel und individuell über jede Zeile im Dokument. Für diesen Schritt wird die Zeilen- als Batchaxe behandelt (d.\,h. beide Dimensionen werden zusammengeführt), wodurch alle Zeilen in einem einzigen Vorwärtsschritt des Netzwerks verarbeitet werden können, was die Architektur extrem schnell macht. Das Netzwerk lernt anschließend eine Gewichtung $W_d^1$ (in Form eines einzelnen Neurons oder Subnetzwerks mit einzelnem Ausgabeneuron, daher eindimensional), wie wichtig jeder verdeckte Zustand der LSTM-Schicht zu jedem Subwort ist. Anschließend werden die mit der \textit{Softmax}-Funktion normalisierten Anteile für jedes Subwort innerhalb einer Zeile bestimmt, um damit die LSTM-Zustände zu gewichten und anschließend zu einem Vektor pro Zeile $X_{zd}$ aufzuaddieren. Das Netzwerk lernt dann eine zweite, äquivalente Gewichtung $W_d^2$, um damit alle Zeilen gemäß ihrer Relevanz zu gewichten und zu einem finalen Vektor, der das gesamte Dokument repräsentiert (siehe \autoref{app:hidden-state-projection}), zu summieren. Eine letzte lineare Schicht ergibt dann die Klassen-Logits $Y_c$.
Eine zweite LSTM-Schicht zwischen \autoref{eq:zd} und \ref{eq:az}, um potentiell Abhängigkeiten zwischen Zeilen besser zu erfassen, führte empirisch zu keiner Verbesserung. Dokumente satz- statt zeilenweise zu verarbeiten ergibt vergleichbare Ergebnisse. Durch zufällige Suche wurde $e = 100$ und $d = 150$ empirisch ermittelt. \\
Um extremes Padding zu vermeiden, wird mit einer Batch-Größe von eins trainiert (andernfalls müssten alle Daten im Batch auf die Anzahl des Dokuments mit den meisten Zeilen angepasst werden). Die Subwortaxe wird auf die längste Zeile im Dokument, höchstens jedoch auf 50 Subwörter mittels Padding angepasst (was 99,9\% aller Zeilen einschließt). Um sowohl Speicher- als auch Zeitkomplexität zu reduzieren, wird \textit{Automatic Mixed Precision} \footnote{\url{https://github.com/NVIDIA/apex}}, also auf Float16 statt Float32 basierende Parameter, eingesetzt.
Zudem wurde mit Normalisierung und Dropout nach \autoref{eq:zd} und \ref{eq:d} experimentiert, jedoch ohne resultierende Verbesserungen. \\
Da sich die Häufigkeiten der Dokumente zu jedem Herkunftsland extrem unterscheiden (\autoref{tab:label-frequencies}), wurden verschiedene Maßnahmen für das Training untersucht.
%
\begin{table}[h!]
\begin{tabular}{@{}lllllllllllll@{}}
%\toprule
Land       & USA  & CN   & UK   & DE  & JP  & CA  & FR  & IL  & AU  & CH  & SG  & IN  \\ \midrule
Häufigkeit & 8699 & 2119 & 1042 & 770 & 557 & 542 & 470 & 278 & 277 & 256 & 254 & 200 \\ %\bottomrule
\end{tabular}
\caption{Häufigkeiten der Herkunftsländer im Korpus}
\label{tab:label-frequencies}
\end{table}
%
Sowohl Auslassen von Datensätzen größerer Klassen (\textit{Downsampling}) als auch Vervielfachen von Dokumenten seltener Länder (\textit{Upsampling}) zeigten keinen Erfolg. Erstes verschlechterte die Ergebnisse maßgeblich, letzteres führte hauptsächlich zu einer längeren Trainingsdauer. Stattdessen wurde die Loss-Funktion (Kreuzentropie) gewichtet (\autoref{eq:loss-weights}). Der Wert von $\alpha$ wurde dabei durch Ausprobieren aller Werte von 0.0--1.0 in Zehntelschritten ermittelt.
%
\begin{equation}
W_{C_i}^L = \left(\frac{|C_{\max}|}{|C_i|}\right)^\alpha,\;\;\;\alpha = 0.6 \label{eq:loss-weights}
\end{equation}
%
\newpage
\noindent Außerdem wurden verschiedene andere Architekturen ausprobiert. Generell sind Transformer der vorherrschende Architekturstandard. Da deren Komplexität jedoch quadratisch mit der Anzahl an Eingabewörtern skaliert und die Parameteranzahl von vortrainierten Modellen meist in den Gigabyte-Bereich reicht, eignen sich entsprechende Modelle nur bedingt. Zudem ist der Domänentransfer problematisch, da sich die bereinigten Textdateien mit oftmals fehlenden (oder schlichtweg nicht vorhandenen, z.\,B. Titel, Überschriften etc.) Satzgrenzen und zahlreichen Artefakten stark von den strukturierten Daten, auf denen die Transformer vortrainiert wurden, unterscheiden.
\begin{itemize}
\item \textbf{CNN}: Hierbei wurden \autoref{eq:lstm}--\ref{eq:zd} durch ein Satz-CNN ersetzt \cite{Kim2014}. Anstatt den Text eindimensional in das CNN zu geben, wurde die zweidimensionale Form $z \times w$ beibehalten, was die Zeitkomplexität von O($zw$) maßgeblich auf O($w$) reduziert (dabei gehen jedoch potentiell Abhängigkeiten zwischen Zeilen verloren).
\item \textbf{RoBERTa}: Dieses 125 Millionen Parameter starke, vortrainierte Sprachmodell basiert auf der Transformer-Architektur und kann Sequenzlängen bis 512 verarbeiten \cite{Liu2019}. Der Text wird deshalb in entsprechende Stücke aufgeteilt, welche separat mit dem Modell verarbeitet werden. Anschließend läuft eine LSTM-Schicht über die gepoolte Ausgabe dieser Stücke, um das Dokument letztlich zu klassifizieren.
\item \textbf{Longformer}: Dieses ebenfalls vortrainierte Modell verbessert die Transformer- Architektur, sodass Speicher- und Zeitkomplexität linear mit der Sequenzlänge skalieren \cite{Beltagy2020}. Dadurch wird es möglich, gleichzeitig 4096 Subwörter zu verarbeiten, was durchschnittlich 53\% einzelner Dokumente abdeckt. So werden die ersten 4096 der Subtokens mit dem Modell und einer einfachen linearen Schicht klassifiziert. 
\end{itemize}
%
%O($t$) statt O($zt$)

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}

\chapter{Ergebnisse}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.8\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/attention-visualized.pdf}
         \subcaption{Bereinigter Text}
         \label{fig:visualized-cleaned-text}
     \end{subfigure}
     \begin{subfigure}[b]{0.8\textwidth}
         \centering
         \vspace{.5em}
         \includegraphics[width=\textwidth]{img/attention-visualized-uncensored.pdf}
         \subcaption{Originaltext}
         \label{fig:visualized-original-text}
     \end{subfigure}
     \caption{Visualisierung der gelernten Gewichtung einzelner Subtokens zur Klassifikation des Herkunftslandes mit einem Ausschnitt des bereinigtem und ursprünglichem Text.}
     \label{fig:visualized-texts}
\end{figure}

%\begin{align}
%A_\text{global} &= A_{zt} \cdot A_{z} \\
%A_\text{global,norm} &= \frac{A_\text{global} - \min A_\text{global}}{\max A_\text{global} - \min A_\text{global}}
%\end{align}

\begin{table}[]
\centering
%\scalebox{1.3}{
\begin{tabular}{@{}clrrrrrrrrrrrr@{}}
\multicolumn{1}{l}{}       & \multicolumn{13}{c}{Voraussage}                                   \\ \cmidrule(l){2-14} 
\multicolumn{1}{l}{}       &     & AU & CA & CN  & FR & DE & IN & IL & JP & SG & CH & UK & USA \\ \cmidrule(l){2-14} 
\multirow{11}{*}{\rotatebox[origin=c]{90}{Wahrheit}} & AU  & 6  &    & 8   &    & 1  &    &    &    &    &    & 7  & 6   \\
                           & CA  & 1  & 8  & 4   &    &    &    &    & 2  &    & 1  & 7  & 32  \\
                           & CN  & 1  & 1  & 188 &    & 1  &    &    & 2  & 1  &    & 1  & 17  \\
                           & FR  &    &    & 1   & 36 & 3  &    &    & 1  &    &    & 2  & 4   \\
                           & DE  &   & 2  & 2   & 2  & 47 &    &    & 1  &    &    & 12 & 11  \\
                           & IN  &    &    & 1   &    &    & 6  &    &    &    &    & 2  & 11  \\
                           & IL  &    & 1  &     & 1  &    &    & 14 & 2  &    &    &    & 10  \\
                           & JP  &    &    & 2   & 1  &    &    &    & 45 &    & 1  & 1  & 6   \\
                           & SG  &    & 1  & 6   &    &    &    &    &    & 9  &    & 2  & 8   \\
                           & CH  &    &    &    & 3   & 7  &    &    &    &    & 4  & 4  & 8   \\
                           & UK  & 3  & 5  & 1   & 6  & 1  & 2  &    & 1  &    & 2  & 66 & 18  \\
                           & USA & 3  & 13 & 38  & 17 & 6  & 1  & 2  & 8  & 2  & 3  & 32 & 746 \\ \cmidrule(l){2-14} 
\end{tabular}
%}
\caption{Konfusionsmatrix der Klassen Australien (AU), Canada (CA), China (CN), Frankreich(FR), Deutschland (DE), Indien (IN), Israel (IL), Japan (JP), Singapur (SG), Schweitz (CH), Großbritannien (UK), Amerika (USA). Leere Zellen implizieren Nullen.}
\end{table}

\chapter{Zusammenfassung}
asdasd



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thesisbibliography
\bibliography{report}

\appendix

\chapter{Wort- und Satzlängenhistogramme}
\label{app:word-sentence-lengths}

\begin{figure}[h!]
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/histogram-word-lengths.pdf}
         \subcaption{Wortlängen}
         \label{fig:hist-mentions}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{img/histogram-sentence-lengths.pdf}
         \subcaption{Satzlängen}
         \label{fig:hist-clusters}
     \end{subfigure}
     \caption{Histogramme über alle Dokumente hinsichtlich durchschnittlicher Zeichenlänge von Wörtern und Wortlänge von Sätzen. Artefakte und fehlerhafte Strukturinformationen verzerren die Ergebnisse.}
     \label{fig:droc-histograms}
\end{figure}

\chapter{PCA-Projektion des internen Modellzustands}
\label{app:hidden-state-projection}

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{img/hidden-state-projection.png}
\caption{Dreidimensionale \textit{Principal Component Analysis}-Projektion des internen Modellzustands $X_d$ (siehe \autoref{eq:d}) der Klassen USA (blau), China (rot) und UK (rosa). Erstellt mittels \url{https://projector.tensorflow.org/}.}
\end{figure}

\chapter{Klassifikation einzelner Zeilen}
\label{app:hidden-state-projection}

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{img/line-pca.png}
\caption{Dreidimensionale \textit{Principal Component Analysis}-Projektion des internen Modellzustands jeder Zeile $X_{zd}$ eines zufälligen Dokuments (siehe \autoref{eq:zd}). Über die Kosinus-Distanz zu Schwerpunkten der Dokumentvektoren $X_d$ kann jeder Zeile eines der Länder USA (blau), China (rot) oder UK (rosa) zugeordnet werden.}
\end{figure}

\textbf{Beispiele für Zeilen mit minimaler Distanz}
\begin{itemize}
\item \textbf{USA}:%
\vspace{-.5em}
\begin{figure}[h!]
\centering
\includegraphics[height=1.2em]{img/line-usa.pdf}
\end{figure}
\vspace{-1.5em}
\item \textbf{China}:%
\vspace{-.5em}
\begin{figure}[h!]
\centering
\includegraphics[height=1.2em]{img/line-china.pdf}
\end{figure}
\vspace{-1.5em}
\item \textbf{UK}:%
\vspace{-.5em}
\begin{figure}[h!]
\centering
\includegraphics[height=1.2em]{img/line-uk.pdf}
\end{figure}
\end{itemize}

%\clearpage
%\pdfbookmark[0]{ToDo-Liste}{todos}
%\listoftodos

\end{document}
